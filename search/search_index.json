{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Interactive Clustering","text":"<p>Python package used to apply NLP interactive clustering methods.</p>"},{"location":"#quick-description","title":"Quick description","text":"<p>Interactive clustering is a method intended to assist in the design of a training data set.</p> <p>This iterative process begins with an unlabeled dataset, and it uses a sequence of two substeps :</p> <ol> <li> <p>the user defines constraints on data sampled by the computer ;</p> </li> <li> <p>the computer performs data partitioning using a constrained clustering algorithm.</p> </li> </ol> <p>Thus, at each step of the process :</p> <ul> <li> <p>the user corrects the clustering of the previous steps using constraints, and</p> </li> <li> <p>the computer offers a corrected and more relevant data partitioning for the next step.</p> </li> </ul> <p> Simplified diagram of how Interactive Clustering works. </p> <p> Example of iterations of Interactive Clustering. </p> <p>The process use severals objects :</p> <ul> <li> <p>a constraints manager : its role is to manage the constraints annotated by the user and to feed back the information deduced (such as the transitivity between constraints or the situation of inconsistency) ;</p> </li> <li> <p>a constraints sampler : its role is to select the most relevant data during the annotation of constraints by the user ;</p> </li> <li> <p>a constrained clustering algorithm : its role is to partition the data while respecting the constraints provided by the user.</p> </li> </ul> <p>NB :</p> <ul> <li> <p>This python library does not contain integration into a graphic interface.</p> </li> <li> <p>For more details, read the Documentation and the articles in the References section.</p> </li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Main documentation</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Interactive Clustering requires Python 3.8 or above.</p> <p>To install with <code>pip</code>:</p> <pre><code># install package\npython3 -m pip install cognitivefactory-interactive-clustering\n\n# install spacy language model dependencies (the one you want, with version \"3.4.x\")\npython3 -m spacy download fr_core_news_md-3.4.0 --direct\n</code></pre> <p>To install with <code>pipx</code>:</p> <pre><code># install pipx\npython3 -m pip install --user pipx\n\n# install package\npipx install --python python3 cognitivefactory-interactive-clustering\n\n# install spacy language model dependencies (the one you want, with version \"3.4.x\")\npython3 -m spacy download fr_core_news_md-3.4.0 --direct\n</code></pre> <p>NB : Other spaCy language models can be downloaded here : spaCy - Models &amp; Languages. Use spacy version <code>\"3.4.x\"</code>.</p>"},{"location":"#development","title":"Development","text":"<p>To work on this project or contribute to it, please read:</p> <ul> <li>the Copier PDM template documentation ;</li> <li>the Contributing page for environment setup and development help ;</li> <li>the Code of Conduct page for contribution rules.</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li> <p>Interactive Clustering:</p> <ul> <li>PhD report: <code>Schild, E. (2024, in press). De l'Importance de Valoriser l'Expertise Humaine dans l'Annotation : Application \u00e0 la Mod\u00e9lisation de Textes en Intentions \u00e0 l'aide d'un Clustering Interactif. Universit\u00e9 de Lorraine.</code> ;</li> <li>First presentation: <code>Schild, E., Durantin, G., Lamirel, J.C., &amp; Miconi, F. (2021). Conception it\u00e9rative et semi-supervis\u00e9e d'assistants conversationnels par regroupement interactif des questions. In EGC 2021 - 21\u00e8mes Journ\u00e9es Francophones Extraction et Gestion des Connaissances. Edition RNTI. &lt;hal-03133007&gt;.</code></li> <li>Theoretical study: <code>Schild, E., Durantin, G., Lamirel, J., &amp; Miconi, F. (2022). Iterative and Semi-Supervised Design of Chatbots Using Interactive Clustering. International Journal of Data Warehousing and Mining (IJDWM), 18(2), 1-19. http://doi.org/10.4018/IJDWM.298007. &lt;hal-03648041&gt;.</code></li> <li>Methodological discussion: <code>Schild, E., Durantin, G., &amp; Lamirel, J.C. (2021). Concevoir un assistant conversationnel de mani\u00e8re it\u00e9rative et semi-supervis\u00e9e avec le clustering interactif. In Atelier - Fouille de Textes - Text Mine 2021 - En conjonction avec EGC 2021. &lt;hal-03133060&gt;.</code></li> </ul> </li> <li> <p>Constraints and Constrained Clustering:</p> <ul> <li>Constraints in clustering: <code>Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110.</code></li> <li>Survey on Constrained Clustering: <code>Lampert, T., T.-B.-H. Dao, B. Lafabregue, N. Serrette, G. Forestier, B. Cremilleux, C. Vrain, et P. Gancarski (2018). Constrained distance based clustering for time-series : a comparative and experimental study. Data Mining and Knowledge Discovery 32(6), 1663\u20131707.</code></li> <li>Affinity Propagation:<ul> <li>Affinity Propagation Clustering: <code>Frey, B. J., &amp; Dueck, D. (2007). Clustering by Passing Messages Between Data Points. In Science (Vol. 315, Issue 5814, pp. 972\u2013976). American Association for the Advancement of Science (AAAS). https://doi.org/10.1126/science.1136800</code></li> <li>Constrained Affinity Propagation Clustering: <code>Givoni, I., &amp; Frey, B. J. (2009). Semi-Supervised Affinity Propagation with Instance-Level Constraints. Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, PMLR 5:161-168</code></li> </ul> </li> <li>DBScan:<ul> <li>DBScan Clustering: <code>Ester, Martin &amp; Kr\u00f6ger, Peer &amp; Sander, Joerg &amp; Xu, Xiaowei. (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. KDD. 96. 226-231</code>.</li> <li>Constrained DBScan Clustering: <code>Ruiz, Carlos &amp; Spiliopoulou, Myra &amp; Menasalvas, Ernestina. (2007). C-DBSCAN: Density-Based Clustering with Constraints. 216-223. 10.1007/978-3-540-72530-5_25.</code></li> </ul> </li> <li>KMeans Clustering:<ul> <li>KMeans Clustering: <code>MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297.</code></li> <li>Constrained 'COP' KMeans Clustering: <code>Wagstaff, K., C. Cardie, S. Rogers, et S. Schroedl (2001). Constrained K-means Clustering with Background Knowledge. International Conference on Machine Learning</code></li> <li>Constrained 'MPC' KMeans Clustering: <code>Khan, Md. A., Tamim, I., Ahmed, E., &amp; Awal, M. A. (2012). Multiple Parameter Based Clustering (MPC): Prospective Analysis for Effective Clustering in Wireless Sensor Network (WSN) Using K-Means Algorithm. In Wireless Sensor Network (Vol. 04, Issue 01, pp. 18\u201324). Scientific Research Publishing, Inc. https://doi.org/10.4236/wsn.2012.41003</code></li> </ul> </li> <li>Hierarchical Clustering:<ul> <li>Hierarchical Clustering: <code>Murtagh, F. et P. Contreras (2012). Algorithms for hierarchical clustering : An overview. Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery 2, 86\u201397.</code></li> <li>Constrained Hierarchical Clustering: <code>Davidson, I. et S. S. Ravi (2005). Agglomerative Hierarchical Clustering with Constraints : Theoretical and Empirical Results. Springer, Berlin, Heidelberg 3721, 12.</code></li> </ul> </li> <li>Spectral Clustering:<ul> <li>Spectral Clustering: <code>Ng, A. Y., M. I. Jordan, et Y.Weiss (2002). On Spectral Clustering: Analysis and an algorithm. In T. G. Dietterich, S. Becker, et Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press.</code></li> <li>Constrained 'SPEC' Spectral Clustering: <code>Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566.</code></li> </ul> </li> </ul> </li> <li> <p>Preprocessing and Vectorization:</p> <ul> <li>spaCy: <code>Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.</code><ul> <li>spaCy language models: <code>https://spacy.io/usage/models</code></li> </ul> </li> <li>NLTK: <code>Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc.</code><ul> <li>NLTK 'SnowballStemmer': <code>https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball</code></li> </ul> </li> <li>Scikit-learn: <code>Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830.</code><ul> <li>Scikit-learn 'TfidfVectorizer': <code>https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"#other-links","title":"Other links","text":"<ul> <li>A web application designed for NLP data annotation using Interactive Clustering methodology: <code>Schild, E. (2021). cognitivefactory/interactive-clustering-gui. Zenodo. https://doi.org/10.5281/zenodo.4775270</code></li> </ul> <p> Welcome page of Interactive Clustering Web Application. </p> <ul> <li>Several comparative studies of Interactive Clustering methodology on NLP datasets: <code>Schild, E. (2021). cognitivefactory/interactive-clustering-comparative-study. Zenodo. https://doi.org/10.5281/zenodo.5648255</code></li> </ul> <p> Organizational diagram of the different Comparative Studies of Interactive Clustering. </p>"},{"location":"#how-to-cite","title":"How to cite","text":"<p><code>Schild, E. (2021). cognitivefactory/interactive-clustering. Zenodo. https://doi.org/10.5281/zenodo.4775251.</code></p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#100-2023-11-16","title":"1.0.0 - 2023-11-16","text":"<p>Compare with 0.6.1</p>"},{"location":"changelog/#build","title":"Build","text":"<ul> <li>prepare production release (2b6ef71 by Erwan Schild).</li> </ul>"},{"location":"changelog/#061-2023-11-16","title":"0.6.1 - 2023-11-16","text":"<p>Compare with 0.6.0</p>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>update mistakes in documentation (9c83be3 by Erwan Schild).</li> </ul>"},{"location":"changelog/#code-refactoring","title":"Code Refactoring","text":"<ul> <li>make format (11a9223 by Erwan Schild).</li> <li>remove experiments from python package (see sdia2022 branch) (f004258 by Erwan Schild).</li> </ul>"},{"location":"changelog/#060-2023-11-15","title":"0.6.0 - 2023-11-15","text":"<p>Compare with 0.5.4</p>"},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>add typing-extensions version restriction to avoid spacy download error (cfa2a4e by Erwan Schild).</li> <li>Save nb of clusters in affinity propagation (f4d2ce7 by David Nicolazo).</li> <li>MPCK-means empty clusters management (29e9875 by Your Name).</li> <li>Hide warning as it might happen in the wrapped function (e34af54 by David Nicolazo).</li> <li>Don't declare Cannot-link matrixes in absolute_must_links mode (2478e24 by David Nicolazo).</li> </ul>"},{"location":"changelog/#build_1","title":"Build","text":"<ul> <li>minor update in duties.py (62189f7 by Erwan Schild).</li> <li>update .gitignore with pdm fix (ed657d5 by Erwan Schild).</li> <li>update dependencies (dd09442 by SCHILD Erwan).</li> </ul>"},{"location":"changelog/#code-refactoring_1","title":"Code Refactoring","text":"<ul> <li>refactoring in progress (4970fef by SCHILD Erwan).</li> <li>fix documentation (b8b6550 by SCHILD Erwan).</li> <li>update dependencies (c19b250 by SCHILD Erwan).</li> <li>updatz dev dependencies after merge (c88561b by SCHILD Erwan).</li> <li>adapt docs before merge (d1214b8 by SCHILD Erwan).</li> <li>adapt code before merge (5aed987 by SCHILD Erwan).</li> <li>make format (c9f3eb4 by SCHILD Erwan).</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>add Affinity Propagantion, C-DBScan and MPC Kmeans with FutureWarning for no production use (a0a46e8 by Erwan Schild).</li> <li>Affinity propagation implementation (0df9458 by Your Name).</li> <li>C-DBScan and MPCKmeans implementations (c148fad by Your Name).</li> <li>AffinityPropagation preference tweaks (724903f by David Nicolazo).</li> <li>Add absolute_must_links mode to Constrained AP (461048a by David Nicolazo).</li> <li>Add constrained Affinity Propagation (1249161 by David Nicolazo).</li> </ul>"},{"location":"changelog/#054-2022-10-06","title":"0.5.4 - 2022-10-06","text":"<p>Compare with 0.5.3</p>"},{"location":"changelog/#code-refactoring_2","title":"Code Refactoring","text":"<ul> <li>fix F541 in duties.py (79a14b8 by SCHILD Erwan).</li> <li>update copier template to 0.10.2 (fe282d1 by SCHILD Erwan).</li> </ul>"},{"location":"changelog/#053-2022-08-25","title":"0.5.3 - 2022-08-25","text":"<p>Compare with 0.5.2</p>"},{"location":"changelog/#build_2","title":"Build","text":"<ul> <li>add .gitignore (34a71c9 by SCHILD Erwan).</li> <li>update pyproject.toml with url dependencies (d063637 by SCHILD Erwan).</li> <li>refactor scripts and configs (1f56aeb by SCHILD Erwan).</li> </ul>"},{"location":"changelog/#052-2022-08-22","title":"0.5.2 - 2022-08-22","text":"<p>Compare with 0.5.1</p>"},{"location":"changelog/#code-refactoring_3","title":"Code Refactoring","text":"<ul> <li>update copier-pdm template (bd93764 by SCHILD Erwan).</li> </ul>"},{"location":"changelog/#051-2022-02-16","title":"0.5.1 - 2022-02-16","text":"<p>Compare with 0.5.0</p>"},{"location":"changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>update constraints manager serialization (6111542 by Erwan Schild).</li> </ul>"},{"location":"changelog/#build_3","title":"Build","text":"<ul> <li>remove previous dependency fix for python 3.10 (6509506 by Erwan Schild).</li> <li>update dependencies for python 3.10 (07e8cad by Erwan Schild).</li> <li>update dependencies (4cc63af by Erwan Schild).</li> </ul>"},{"location":"changelog/#code-refactoring_4","title":"Code Refactoring","text":"<ul> <li>speed up constraints transitivity (5e255ef by Erwan Schild).</li> <li>update spacy usage (2d05289 by Erwan Schild).</li> </ul>"},{"location":"changelog/#050-2022-02-15","title":"0.5.0 - 2022-02-15","text":"<p>Compare with 0.4.2</p>"},{"location":"changelog/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>force nb_cluster to be smaller than dataset size (9a5a3f0 by Erwan Schild).</li> <li>correct kmeans centroid computation for deleted data ids (b0cd1a0 by Erwan Schild).</li> </ul>"},{"location":"changelog/#build_4","title":"Build","text":"<ul> <li>remove python 3.11 from ci (71768f0 by Erwan Schild).</li> <li>update to python 3.7 (981fd0f by Erwan Schild).</li> <li>update .gitignore (76d8ff8 by Erwan Schild).</li> </ul>"},{"location":"changelog/#code-refactoring_5","title":"Code Refactoring","text":"<ul> <li>make format (f363998 by Erwan Schild).</li> </ul>"},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>add constraints manager serialization (c2e13e2 by Erwan Schild).</li> </ul>"},{"location":"changelog/#042-2021-10-27","title":"0.4.2 - 2021-10-27","text":"<p>Compare with 0.4.1</p>"},{"location":"changelog/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>force csr_matrix for spacy vectors in order to perform vstack (be7b75c by Erwan Schild).</li> </ul>"},{"location":"changelog/#build_5","title":"Build","text":"<ul> <li>remove 'regex' dependency correction (e448515 by Erwan Schild).</li> </ul>"},{"location":"changelog/#code-refactoring_6","title":"Code Refactoring","text":"<ul> <li>refactor code and force sparse matrix (63e94a2 by Erwan Schild).</li> <li>speed up spectral clustering (711cf4d by Erwan Schild).</li> </ul>"},{"location":"changelog/#041-2021-10-19","title":"0.4.1 - 2021-10-19","text":"<p>Compare with 0.4.0</p>"},{"location":"changelog/#code-refactoring_7","title":"Code Refactoring","text":"<ul> <li>speed up clustering and refactor code (93ada28 by Erwan Schild).</li> </ul>"},{"location":"changelog/#040-2021-10-12","title":"0.4.0 - 2021-10-12","text":"<p>Compare with 0.3.0</p>"},{"location":"changelog/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>correct networkx dependency requirement (4ec4587 by Erwan SCHILD).</li> <li>correct networkx import in sampling (521a4ff by Erwan Schild).</li> <li>speed up computation of sampling.clusters_based.sampling for distance restrictions (5ab6821 by Erwan Schild).</li> <li>speed up computation of constraints.binary.get_min_and_max_number_of_clusters (1e50f7c by Erwan Schild).</li> </ul>"},{"location":"changelog/#code-refactoring_8","title":"Code Refactoring","text":"<ul> <li>update template with copier update (e0a7c77 by Erwan Schild).</li> <li>fix black dependenciy installation (eef88c5 by Erwan Schild).</li> <li>delete old random sampler (6cd0a06 by Erwan Schild).</li> </ul>"},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>implementation of getter of data IDs involved in a constraint conflict (6eace0d by Erwan Schild).</li> </ul>"},{"location":"changelog/#030-2021-10-04","title":"0.3.0 - 2021-10-04","text":"<p>Compare with 0.2.1</p>"},{"location":"changelog/#021-2021-09-20","title":"0.2.1 - 2021-09-20","text":"<p>Compare with 0.2.0</p>"},{"location":"changelog/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>correct constraints transitivity inconsistencies in BinaryConstraintsManager.add_constraint (98f162e by Erwan Schild).</li> </ul>"},{"location":"changelog/#build_6","title":"Build","text":"<ul> <li>fix cvxopt 1.2.7 dependency error for linux (a2f5429 by Erwan Schild).</li> <li>update flake config (696fb97 by Erwan Schild).</li> </ul>"},{"location":"changelog/#code-refactoring_9","title":"Code Refactoring","text":"<ul> <li>fix code quality errors (02c03ee by Erwan Schild).</li> <li>update exception message (2003a1e by Erwan Schild).</li> </ul>"},{"location":"changelog/#020-2021-09-01","title":"0.2.0 - 2021-09-01","text":"<p>Compare with 0.1.3</p>"},{"location":"changelog/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>change constraints storage from sorted lists to sets (47d3528 by Erwan Schild).</li> </ul>"},{"location":"changelog/#build_7","title":"Build","text":"<ul> <li>update python cross-versions dependencies (dc4ac32 by Erwan Schild).</li> <li>fix python test dependencies (8b4106f by Erwan Schild).</li> <li>fix python dependencies (4e61a57 by Erwan Schild).</li> <li>update python dependencies (8cd3e49 by Erwan Schild).</li> <li>correct build steps (68091d4 by Erwan Schild).</li> <li>update project from poetry to pdm (3133391 by Erwan Schild).</li> <li>update .gitignore with migration from poetry to pdm (31d2374 by Erwan Schild).</li> <li>change template informations (7314555 by Erwan Schild).</li> <li>prepare migration from poetry to pdm (89a214e by Erwan Schild).</li> </ul>"},{"location":"changelog/#code-refactoring_10","title":"Code Refactoring","text":"<ul> <li>delete utils.checking (a9a1f50 by Erwan Schild).</li> <li>remove checks and force usage of constraints_manager (4cdb0bb by Erwan Schild).</li> <li>improve sampling speed (9d6ed5c by Erwan Schild).</li> <li>add py.typed file (25c7be3 by Erwan Schild).</li> </ul>"},{"location":"changelog/#013-2021-05-20","title":"0.1.3 - 2021-05-20","text":"<p>Compare with 0.1.2</p>"},{"location":"changelog/#012-2021-05-19","title":"0.1.2 - 2021-05-19","text":"<p>Compare with 0.1.1</p>"},{"location":"changelog/#build_8","title":"Build","text":"<ul> <li>remove spacy language model as direct dependencies (95093a6 by Erwan SCHILD).</li> <li>remove local spacy language model dependencies (779f737 by Erwan SCHILD).</li> <li>install fr_core_news_sm as a local dependency (564fa5c by Erwan SCHILD).</li> <li>add .gitattributes (b35a1e2 by Erwan SCHILD).</li> </ul>"},{"location":"changelog/#code-refactoring_11","title":"Code Refactoring","text":"<ul> <li>correct format and tests (e3245f3 by Erwan SCHILD).</li> </ul>"},{"location":"changelog/#011-2021-05-18","title":"0.1.1 - 2021-05-18","text":"<p>Compare with 0.1.0</p>"},{"location":"changelog/#build_9","title":"Build","text":"<ul> <li>correct install source (c84bb4c by Erwan Schild).</li> <li>correct package import (4121954 by Erwan Schild).</li> </ul>"},{"location":"changelog/#010-2021-05-17","title":"0.1.0 - 2021-05-17","text":"<p>Compare with first commit</p>"},{"location":"changelog/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>fix encoding error on fr_core_news_sm-2.3.0/meta.json ? (98acb42 by R1D1).</li> <li>correct installation sources (de4c727 by R1D1).</li> </ul>"},{"location":"changelog/#build_10","title":"Build","text":"<ul> <li>update import information (fde123e by R1D1).</li> <li>fix installation source (a9d3423 by R1D1).</li> <li>change ci configuration (0d8befd by R1D1).</li> <li>init repository (b248af7 by Erwan SCHILD).</li> </ul>"},{"location":"changelog/#code-refactoring_12","title":"Code Refactoring","text":"<ul> <li>order import and update documentation (70e8780 by R1D1).</li> <li>remove local fr_core_news_sm model (1f9da8f by R1D1).</li> <li>test fr_core_news_sm installation like a pip package (b249159 by R1D1).</li> </ul>"},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li>implement Interactive Clustering (d678d87 by Erwan SCHILD).</li> </ul>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li> <p>Using welcoming and inclusive language</p> </li> <li> <p>Being respectful of differing viewpoints and experiences</p> </li> <li> <p>Gracefully accepting constructive criticism</p> </li> <li> <p>Focusing on what is best for the community</p> </li> <li> <p>Showing empathy towards other community members</p> </li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li> <p>The use of sexualized language or imagery and unwelcome sexual attention or advances</p> </li> <li> <p>Trolling, insulting/derogatory comments, and personal or political attacks</p> </li> <li> <p>Public or private harassment</p> </li> <li> <p>Publishing others' private information, such as a physical or electronic address, without explicit permission</p> </li> <li> <p>Other conduct which could reasonably be considered inappropriate in a professional setting</p> </li> </ul>"},{"location":"code_of_conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at erwan.schild@e-i.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#environment-setup","title":"Environment setup","text":"<p>Nothing easier! Follow the instructions below.</p> <p>Note</p> <p>We STRONGLY recommend using a Linux distribution for Python development (Windows sometimes leads to obscure compatibility errors...)</p> <ol> <li> <p>Install <code>Git</code> to version and track our software changes.</p> <ul> <li> <p>On Windows, use the official installer: <code>Git-for-Windows</code>.</p> </li> <li> <p>On Linux, simply use your package manager.</p> </li> </ul> <p>Note</p> <p><code>Git-for-Windows</code> doesn't provide the command <code>make</code>. In following step, use <code>pdm</code> instead.</p> </li> <li> <p>Install <code>Python</code> as programming language for this projet.</p> <ul> <li> <p>On Windows, use the official installer: Python Releases for Windows.</p> </li> <li> <p>On Linux, simply use your package manager.</p> </li> </ul> <p>Note</p> <p>You can also use use <code>pyenv</code>.</p> <pre><code># install pyenv\ngit clone https://github.com/pyenv/pyenv ~/.pyenv\n\n# setup pyenv (you should also put these three lines in .bashrc or similar)\nexport PATH=\"${HOME}/.pyenv/bin:${PATH}\"\nexport PYENV_ROOT=\"${HOME}/.pyenv\"\neval \"$(pyenv init -)\"\n\n# install Python 3.8\npyenv install 3.8\n\n# make it available globally\npyenv global system 3.8\n</code></pre> </li> <li> <p>Fork and clone the repository:</p> <pre><code>git clone https://github.com/cognitivefactory/interactive-clustering/\ncd interactive-clustering\n</code></pre> </li> <li> <p>Install the dependencies of the projet with:</p> <pre><code>cd interactive-clustering\nmake setup # on Linux\npdm install # on Windows\n</code></pre> <p>Note</p> <p>If it fails for some reason (especially on Windows), you'll need to install <code>pipx</code> and <code>pdm</code> manually.</p> <p>You can install them with:</p> <pre><code>python3 -m pip install --user pipx\npipx install pdm\n</code></pre> <p>Now you can try running <code>make setup</code> again, or simply <code>pdm install</code>.</p> </li> </ol> <p>Your project is now ready and dependencies are installed.</p>"},{"location":"contributing/#available-template-tasks","title":"Available template tasks","text":"<p>This project uses duty to run tasks. A Makefile is also provided. To run a task, use <code>make TASK</code> on Linux and <code>pdm run duty TASK</code> on Windows.</p> <p>To show the available template task:</p> <pre><code>make help # on Linux\npdm run duty --list # on Windows\n</code></pre> <p>The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following:</p> <ol> <li><code>export PYTHON_VERSIONS=</code>: this will run the task    with only the current Python version</li> <li>run the task directly with <code>pdm run duty TASK</code></li> </ol> <p>The Makefile detects if a virtual environment is activated, so <code>make</code>/<code>pdm</code> will work the same with the virtualenv activated or not.</p>"},{"location":"contributing/#development-journey","title":"Development journey","text":"<p>As usual:</p> <ol> <li>create a new branch: <code>git checkout -b feature-or-bugfix-name</code></li> <li>edit the code and/or the documentation</li> </ol> <p>If you updated the documentation or the project dependencies:</p> <ol> <li>run <code>make docs-regen</code></li> <li>run <code>make docs-serve</code>, go to http://localhost:8000 and check that everything looks good</li> </ol> <p>Before committing:</p> <ol> <li>run <code>make format</code> to auto-format the code</li> <li>run <code>make check</code> to check everything (fix any warning)</li> <li>run <code>make test</code> to run the tests (fix any issue)</li> <li>follow our commit message convention</li> </ol> <p>If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review.</p> <p>Don't bother updating the changelog, we will take care of this.</p>"},{"location":"contributing/#commit-message-convention","title":"Commit message convention","text":"<p>Commits messages must follow the Angular style:</p> <pre><code>&lt;type&gt;[(scope)]: Subject\n\n[Body]\n</code></pre> <p>Scope and body are optional. Type can be:</p> <ul> <li><code>build</code>: About packaging, building wheels, etc.</li> <li><code>chore</code>: About packaging or repo/files management.</li> <li><code>ci</code>: About Continuous Integration.</li> <li><code>docs</code>: About documentation.</li> <li><code>feat</code>: New feature.</li> <li><code>fix</code>: Bug fix.</li> <li><code>perf</code>: About performance.</li> <li><code>refactor</code>: Changes which are not features nor bug fixes.</li> <li><code>style</code>: A change in code style/format.</li> <li><code>tests</code>: About tests.</li> </ul> <p>Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end:</p> <pre><code>Body.\n\nReferences: #10, #11.\nFixes #15.\n</code></pre>"},{"location":"contributing/#pull-requests-guidelines","title":"Pull requests guidelines","text":"<p>Link to any related issue in the Pull Request message.</p> <p>During review, we recommend using fixups:</p> <pre><code># SHA is the SHA of the commit you want to fix\ngit commit --fixup=SHA\n</code></pre> <p>Once all the changes are approved, you can squash your commits:</p> <pre><code>git rebase -i --autosquash master\n</code></pre> <p>And force-push:</p> <pre><code>git push -f\n</code></pre> <p>If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.</p>"},{"location":"credits/","title":"Credits","text":"<p><p>These projects were used to build <code>cognitivefactory-interactive-clustering</code>. Thank you!</p> <p><code>python</code> | <code>pdm</code> | <code>copier-pdm</code></p> </p>"},{"location":"credits/#exec-1--runtime-dependencies","title":"Runtime dependencies","text":"Project Summary Version (accepted) Version (last resolved) License <code>blis</code> The Blis BLAS-like linear algebra library, as a self-contained C-extension. <code>&lt;0.8.0,&gt;=0.7.8</code> <code>0.7.11</code> BSD <code>catalogue</code> Super lightweight function registries for your library <code>&lt;2.1.0,&gt;=2.0.6</code> <code>2.0.10</code> MIT <code>certifi</code> Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2023.7.22</code> MPL-2.0 <code>charset-normalizer</code> The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&lt;4,&gt;=2</code> <code>3.3.2</code> MIT <code>click</code> Composable command line interface toolkit <code>&lt;9.0.0,&gt;=7.1.1</code> <code>8.1.7</code> BSD-3-Clause <code>colorama</code> Cross-platform colored terminal text. <code>; platform_system == \"Windows\"</code> <code>0.4.6</code> BSD License <code>confection</code> The sweetest config system for Python <code>&lt;1.0.0,&gt;=0.0.1</code> <code>0.1.3</code> MIT <code>cymem</code> Manage calls to calloc/free through Cython <code>&lt;2.1.0,&gt;=2.0.2</code> <code>2.0.8</code> MIT <code>idna</code> Internationalized Domain Names in Applications (IDNA) <code>&lt;4,&gt;=2.5</code> <code>3.4</code> BSD License <code>jinja2</code> A very fast and expressive template engine. <code>3.1.2</code> BSD-3-Clause <code>joblib</code> Lightweight pipelining with Python functions <code>&gt;=1.1.1</code> <code>1.3.2</code> BSD 3-Clause <code>langcodes</code> Tools for labeling human languages with IETF language tags <code>&lt;4.0.0,&gt;=3.2.0</code> <code>3.3.0</code> MIT <code>markupsafe</code> Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0</code> <code>2.1.3</code> BSD-3-Clause <code>murmurhash</code> Cython bindings for MurmurHash <code>&lt;1.1.0,&gt;=0.28.0</code> <code>1.0.10</code> MIT <code>networkx</code> Python package for creating and manipulating graphs and networks <code>&gt;=2.6</code> <code>3.1</code> BSD License <code>numpy</code> Fundamental package for array computing in Python <code>&gt;=1.23.5</code> <code>1.24.4</code> BSD-3-Clause <code>packaging</code> Core utilities for Python packages <code>&gt;=20.0</code> <code>23.2</code> BSD License <code>pathy</code> pathlib.Path subclasses for local and cloud bucket storage <code>&gt;=0.3.5</code> <code>0.10.3</code> Apache 2.0 <code>preshed</code> Cython hash table that trusts the keys are pre-hashed <code>&lt;3.1.0,&gt;=3.0.2</code> <code>3.0.9</code> MIT <code>pydantic</code> Data validation and settings management using python type hints <code>!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4</code> <code>1.10.13</code> MIT <code>requests</code> Python HTTP for Humans. <code>&lt;3.0.0,&gt;=2.13.0</code> <code>2.31.0</code> Apache 2.0 <code>scikit-learn</code> A set of python modules for machine learning and data mining <code>&gt;=0.24.1</code> <code>1.3.2</code> new BSD <code>scipy</code> Fundamental algorithms for scientific computing in Python <code>&gt;=1.7.3</code> <code>1.9.3</code> BSD License <code>setuptools</code> Easily download, build, install, upgrade, and uninstall Python packages <code>&gt;=65.5.1</code> <code>68.2.2</code> MIT License <code>smart-open</code> Utils for streaming large files (S3, HDFS, GCS, Azure Blob Storage, gzip, bz2...) <code>&lt;7.0.0,&gt;=5.2.1</code> <code>6.4.0</code> MIT <code>spacy</code> Industrial-strength Natural Language Processing (NLP) in Python <code>&gt;=3.4, &lt;3.5</code> <code>3.4.4</code> MIT <code>spacy-legacy</code> Legacy registered functions for spaCy backwards compatibility <code>&lt;3.1.0,&gt;=3.0.10</code> <code>3.0.12</code> MIT <code>spacy-loggers</code> Logging utilities for SpaCy <code>&lt;2.0.0,&gt;=1.0.0</code> <code>1.0.5</code> MIT <code>srsly</code> Modern high-performance serialization utilities for Python <code>&lt;3.0.0,&gt;=2.4.3</code> <code>2.4.8</code> MIT <code>thinc</code> A refreshing functional take on deep learning, compatible with your favorite libraries <code>&lt;8.2.0,&gt;=8.1.0</code> <code>8.1.12</code> MIT <code>threadpoolctl</code> threadpoolctl <code>&gt;=2.0.0</code> <code>3.2.0</code> BSD-3-Clause <code>tqdm</code> Fast, Extensible Progress Meter <code>&lt;5.0.0,&gt;=4.38.0</code> <code>4.66.1</code> MPL-2.0 AND MIT <code>typer</code> Typer, build great CLIs. Easy to code. Based on Python type hints. <code>&lt;0.8.0,&gt;=0.3.0</code> <code>0.7.0</code> MIT License <code>typing-extensions</code> Backported and Experimental Type Hints for Python 3.7+ <code>&gt;=4.2.0</code> <code>4.5.0</code> Python Software Foundation License <code>urllib3</code> HTTP library with thread-safe connection pooling, file post, and more. <code>&lt;3,&gt;=1.21.1</code> <code>1.26.18</code> MIT <code>wasabi</code> A lightweight console printing and formatting toolkit <code>&lt;1.1.0,&gt;=0.9.1</code> <code>0.10.1</code> MIT"},{"location":"credits/#exec-1--development-dependencies","title":"Development dependencies","text":"Project Summary Version (accepted) Version (last resolved) License <code>ansimarkup</code> Produce colored terminal text with an xml-like markup <code>~=1.4</code> <code>1.5.0</code> Revised BSD License <code>astor</code> Read/rewrite/write Python ASTs <code>&gt;=0.8</code> <code>0.8.1</code> BSD-3-Clause <code>attrs</code> Classes Without Boilerplate <code>&gt;=19.2.0</code> <code>23.1.0</code> MIT License <code>autoflake</code> Removes unused imports and unused variables <code>&gt;=1.4</code> <code>1.7.8</code> MIT <code>babel</code> Internationalization utilities <code>~=2.10</code> <code>2.13.1</code> BSD-3-Clause <code>bandit</code> Security oriented static analyser for python code. <code>&gt;=1.7.3</code> <code>1.7.5</code> Apache-2.0 license <code>black</code> The uncompromising code formatter. <code>&gt;=21.10b0</code> <code>23.11.0</code> MIT <code>blis</code> The Blis BLAS-like linear algebra library, as a self-contained C-extension. <code>&lt;0.8.0,&gt;=0.7.8</code> <code>0.7.11</code> BSD <code>catalogue</code> Super lightweight function registries for your library <code>&lt;2.1.0,&gt;=2.0.6</code> <code>2.0.10</code> MIT <code>certifi</code> Python package for providing Mozilla's CA Bundle. <code>&gt;=2022.12.7</code> <code>2023.7.22</code> MPL-2.0 <code>charset-normalizer</code> The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&lt;4,&gt;=2</code> <code>3.3.2</code> MIT <code>click</code> Composable command line interface toolkit <code>&gt;=8.0.0</code> <code>8.1.7</code> BSD-3-Clause <code>colorama</code> Cross-platform colored terminal text. <code>; platform_system == \"Windows\"</code> <code>0.4.6</code> BSD License <code>confection</code> The sweetest config system for Python <code>&lt;1.0.0,&gt;=0.0.1</code> <code>0.1.3</code> MIT <code>coverage</code> Code coverage measurement for Python <code>[toml]&gt;=5.2.1</code> <code>7.3.2</code> Apache-2.0 <code>cymem</code> Manage calls to calloc/free through Cython <code>&lt;2.1.0,&gt;=2.0.2</code> <code>2.0.8</code> MIT <code>darglint</code> A utility for ensuring Google-style docstrings stay up to date with the source code. <code>&gt;=1.8</code> <code>1.8.1</code> MIT <code>dparse</code> A parser for Python dependency files <code>&gt;=0.6.2</code> <code>0.6.3</code> MIT license <code>duty</code> A simple task runner. <code>&gt;=0.7</code> <code>1.1.0</code> ISC <code>en-core-web-md</code> English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer. <code>@ https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.0/en_core_web_md-3.4.0.tar.gz</code> <code>3.4.0</code> MIT <code>exceptiongroup</code> Backport of PEP 654 (exception groups) <code>&gt;=1.0.0rc8; python_version &lt; \"3.11\"</code> <code>1.1.3</code> MIT License <code>execnet</code> execnet: rapid multi-Python deployment <code>&gt;=1.1</code> <code>2.0.2</code> MIT License <code>failprint</code> Run a command, print its output only if it fails. <code>!=1.0.0,&gt;=0.11</code> <code>1.0.2</code> ISC <code>flake8</code> the modular source code checker: pep8 pyflakes and co <code>&gt;=4.0</code> <code>5.0.4</code> MIT <code>flake8-bandit</code> Automated security testing with bandit and flake8. <code>&gt;=2.1</code> <code>4.1.1</code> MIT <code>flake8-black</code> flake8 plugin to call black as a code style validator <code>&gt;=0.2</code> <code>0.3.6</code> MIT <code>flake8-bugbear</code> A plugin for flake8 finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle. <code>&gt;=21.9</code> <code>23.3.12</code> MIT <code>flake8-builtins</code> Check for python builtins being used as variables or parameters <code>&gt;=1.5</code> <code>2.2.0</code> GNU General Public License v2 (GPLv2) <code>flake8-comprehensions</code> A flake8 plugin to help you write better list/set/dict comprehensions. <code>&gt;=3.7</code> <code>3.14.0</code> MIT <code>flake8-docstrings</code> Extension for flake8 which uses pydocstyle to check docstrings <code>&gt;=1.6</code> <code>1.7.0</code> MIT <code>flake8-plugin-utils</code> The package provides base classes and utils for flake8 plugin writing <code>&lt;2.0.0,&gt;=1.3.2</code> <code>1.3.3</code> MIT <code>flake8-polyfill</code> Polyfill package for Flake8 plugins <code>&gt;=1.0.2</code> <code>1.0.2</code> MIT <code>flake8-pytest-style</code> A flake8 plugin checking common style issues or inconsistencies with pytest-based tests. <code>&gt;=1.5</code> <code>1.7.2</code> MIT <code>flake8-string-format</code> string format checker, plugin for flake8 <code>&gt;=0.3</code> <code>0.3.0</code> MIT License <code>flake8-tidy-imports</code> A flake8 plugin that helps you write tidier imports. <code>&gt;=4.5</code> <code>4.10.0</code> MIT <code>flake8-variables-names</code> A flake8 extension that helps to make more readable variables names <code>&gt;=0.0</code> <code>0.0.6</code> MIT <code>fr-core-news-md</code> French pipeline optimized for CPU. Components: tok2vec, morphologizer, parser, senter, ner, attribute_ruler, lemmatizer. <code>@ https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.4.0/fr_core_news_md-3.4.0.tar.gz</code> <code>3.4.0</code> LGPL-LR <code>ghp-import</code> Copy your docs directly to the gh-pages branch. <code>&gt;=1.0</code> <code>2.1.0</code> Apache Software License <code>git-changelog</code> Automatic Changelog generator using Jinja2 templates. <code>&gt;=0.4,&lt;1.0</code> <code>0.6.0</code> ISC <code>gitdb</code> Git Object Database <code>&lt;5,&gt;=4.0.1</code> <code>4.0.11</code> BSD License <code>gitpython</code> GitPython is a Python library used to interact with Git repositories <code>&gt;=3.1.30</code> <code>3.1.40</code> BSD <code>griffe</code> Signatures for entire Python programs. Extract the structure, the frame, the skeleton of your project, to generate API documentation or find breaking changes in your API. <code>&gt;=0.37</code> <code>0.38.0</code> ISC <code>idna</code> Internationalized Domain Names in Applications (IDNA) <code>&lt;4,&gt;=2.5</code> <code>3.4</code> BSD License <code>importlib-metadata</code> Read metadata from Python packages <code>&gt;=4.3; python_version &lt; \"3.10\"</code> <code>6.8.0</code> Apache Software License <code>iniconfig</code> brain-dead simple config-ini parsing <code>2.0.0</code> MIT License <code>isort</code> A Python utility / library to sort Python imports. <code>&gt;=5.10</code> <code>5.12.0</code> MIT <code>jinja2</code> A very fast and expressive template engine. <code>&lt;4,&gt;=2.11</code> <code>3.1.2</code> BSD-3-Clause <code>langcodes</code> Tools for labeling human languages with IETF language tags <code>&lt;4.0.0,&gt;=3.2.0</code> <code>3.3.0</code> MIT <code>markdown</code> Python implementation of John Gruber's Markdown. <code>&lt;4.0.0,&gt;=3.3.3</code> <code>3.5.1</code> BSD License <code>markdown-callouts</code> Markdown extension: a classier syntax for admonitions <code>&gt;=0.2</code> <code>0.3.0</code> MIT <code>markdown-exec</code> Utilities to execute code blocks in Markdown files. <code>&gt;=0.5</code> <code>1.7.0</code> ISC <code>markdown-it-py</code> Python port of markdown-it. Markdown parsing, done right! <code>&gt;=2.2.0</code> <code>3.0.0</code> MIT License <code>markupsafe</code> Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0</code> <code>2.1.3</code> BSD-3-Clause <code>mccabe</code> McCabe checker, plugin for flake8 <code>&lt;0.8.0,&gt;=0.7.0</code> <code>0.7.0</code> Expat license <code>mdurl</code> Markdown URL utilities <code>~=0.1</code> <code>0.1.2</code> MIT License <code>mergedeep</code> A deep merge function for \ud83d\udc0d. <code>&gt;=1.3.4</code> <code>1.3.4</code> MIT License <code>mkdocs</code> Project documentation with Markdown. <code>&gt;=1.3</code> <code>1.5.3</code> BSD License <code>mkdocs-coverage</code> MkDocs plugin to integrate your coverage HTML report into your site. <code>&gt;=0.2</code> <code>1.0.0</code> ISC <code>mkdocs-gen-files</code> MkDocs plugin to programmatically generate documentation pages during the build <code>&gt;=0.3</code> <code>0.5.0</code> MIT License <code>mkdocs-literate-nav</code> MkDocs plugin to specify the navigation in Markdown instead of YAML <code>&gt;=0.4</code> <code>0.6.1</code> MIT License <code>mkdocs-material</code> Documentation that simply works <code>&gt;=7.3</code> <code>9.4.8</code> MIT License <code>mkdocs-material-extensions</code> Extension pack for Python Markdown and MkDocs Material. <code>~=1.3</code> <code>1.3</code> MIT License <code>mkdocs-section-index</code> MkDocs plugin to allow clickable sections that lead to an index page <code>&gt;=0.3</code> <code>0.3.8</code> MIT License <code>mkdocstrings</code> Automatic documentation from sources, for MkDocs. <code>[python]&gt;=0.18</code> <code>0.24.0</code> ISC <code>mkdocstrings-python</code> A Python handler for mkdocstrings. <code>&gt;=0.5.2</code> <code>1.7.4</code> ISC <code>murmurhash</code> Cython bindings for MurmurHash <code>&lt;1.1.0,&gt;=0.28.0</code> <code>1.0.10</code> MIT <code>mypy</code> Optional static typing for Python <code>&gt;=0.910</code> <code>1.7.0</code> MIT <code>mypy-extensions</code> Type system extensions for programs checked with the mypy type checker. <code>&gt;=0.4.3</code> <code>1.0.0</code> MIT License <code>numpy</code> Fundamental package for array computing in Python <code>&gt;=1.15.0</code> <code>1.24.4</code> BSD-3-Clause <code>packaging</code> Core utilities for Python packages <code>&gt;=22.0</code> <code>23.2</code> BSD License <code>paginate</code> Divides large result sets into pages for easier browsing <code>~=0.5</code> <code>0.5.6</code> MIT <code>pathspec</code> Utility library for gitignore style pattern matching of file paths. <code>&gt;=0.9.0</code> <code>0.11.2</code> Mozilla Public License 2.0 (MPL 2.0) <code>pathy</code> pathlib.Path subclasses for local and cloud bucket storage <code>&gt;=0.3.5</code> <code>0.10.3</code> Apache 2.0 <code>pbr</code> Python Build Reasonableness <code>!=2.1.0,&gt;=2.0.0</code> <code>6.0.0</code> Apache Software License <code>pep8-naming</code> Check PEP-8 naming conventions, plugin for flake8 <code>&gt;=0.12</code> <code>0.13.3</code> Expat license <code>platformdirs</code> A small Python package for determining appropriate platform-specific dirs, e.g. a \"user data dir\". <code>&gt;=2</code> <code>4.0.0</code> MIT License <code>pluggy</code> plugin and hook calling mechanisms for python <code>&lt;2.0,&gt;=0.12</code> <code>1.3.0</code> MIT <code>preshed</code> Cython hash table that trusts the keys are pre-hashed <code>&lt;3.1.0,&gt;=3.0.2</code> <code>3.0.9</code> MIT <code>ptyprocess</code> Run a subprocess in a pseudo terminal <code>~=0.6; sys_platform != \"win32\"</code> <code>0.7.0</code> ? <code>pycodestyle</code> Python style guide checker <code>&lt;2.10.0,&gt;=2.9.0</code> <code>2.9.1</code> Expat license <code>pydantic</code> Data validation and settings management using python type hints <code>!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4</code> <code>1.10.13</code> MIT <code>pydocstyle</code> Python docstring style checker <code>&gt;=2.1</code> <code>6.3.0</code> MIT <code>pyflakes</code> passive checker of Python programs <code>&lt;3,&gt;=1.1.0</code> <code>2.5.0</code> MIT <code>pygments</code> Pygments is a syntax highlighting package written in Python. <code>~=2.16</code> <code>2.16.1</code> BSD-2-Clause <code>pymdown-extensions</code> Extension pack for Python Markdown. <code>&gt;=9</code> <code>10.4</code> MIT License <code>pytest</code> pytest: simple powerful testing with Python <code>&gt;=6.2</code> <code>7.4.3</code> MIT <code>pytest-cov</code> Pytest plugin for measuring coverage. <code>&gt;=3.0</code> <code>4.1.0</code> MIT <code>pytest-randomly</code> Pytest plugin to randomly order tests and control random.seed. <code>&gt;=3.10</code> <code>3.15.0</code> MIT <code>pytest-xdist</code> pytest xdist plugin for distributed testing, most importantly across multiple CPUs <code>&gt;=2.4</code> <code>3.4.0</code> MIT <code>python-dateutil</code> Extensions to the standard Python datetime module <code>&gt;=2.8.1</code> <code>2.8.2</code> Dual License <code>pytz</code> World timezone definitions, modern and historical <code>&gt;=2015.7; python_version &lt; \"3.9\"</code> <code>2023.3.post1</code> MIT <code>pyyaml</code> YAML parser and emitter for Python <code>&gt;=5.1</code> <code>6.0.1</code> MIT <code>pyyaml-env-tag</code> A custom YAML tag for referencing environment variables in YAML files. <code>&gt;=0.1</code> <code>0.1</code> MIT License <code>regex</code> Alternative regular expression module, to replace re. <code>&gt;=2022.4</code> <code>2023.10.3</code> Apache Software License <code>requests</code> Python HTTP for Humans. <code>~=2.26</code> <code>2.31.0</code> Apache 2.0 <code>rich</code> Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal <code>13.7.0</code> MIT <code>ruamel-yaml</code> ruamel.yaml is a YAML parser/emitter that supports roundtrip preservation of comments, seq/map flow style, and map key order <code>&gt;=0.17.21</code> <code>0.18.5</code> MIT license <code>ruamel-yaml-clib</code> C version of reader, parser and emitter for ruamel.yaml derived from libyaml <code>&gt;=0.2.7; platform_python_implementation == \"CPython\" and python_version &lt; \"3.13\"</code> <code>0.2.8</code> MIT <code>safety</code> Checks installed dependencies for known vulnerabilities and licenses. <code>&gt;=2</code> <code>2.3.4</code> MIT license <code>semver</code> Python helper for Semantic Versioning (http://semver.org/) <code>~=2.13</code> <code>2.13.0</code> BSD <code>setuptools</code> Easily download, build, install, upgrade, and uninstall Python packages <code>&gt;=19.3</code> <code>68.2.2</code> MIT License <code>six</code> Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.16.0</code> MIT <code>smart-open</code> Utils for streaming large files (S3, HDFS, GCS, Azure Blob Storage, gzip, bz2...) <code>&lt;7.0.0,&gt;=5.2.1</code> <code>6.4.0</code> MIT <code>smmap</code> A pure Python implementation of a sliding window memory map manager <code>&lt;6,&gt;=3.0.1</code> <code>5.0.1</code> BSD <code>snowballstemmer</code> This package provides 29 stemmers for 28 languages generated from Snowball algorithms. <code>&gt;=2.2.0</code> <code>2.2.0</code> BSD-3-Clause <code>spacy</code> Industrial-strength Natural Language Processing (NLP) in Python <code>&lt;3.5.0,&gt;=3.4.0</code> <code>3.4.4</code> MIT <code>spacy-legacy</code> Legacy registered functions for spaCy backwards compatibility <code>&lt;3.1.0,&gt;=3.0.10</code> <code>3.0.12</code> MIT <code>spacy-loggers</code> Logging utilities for SpaCy <code>&lt;2.0.0,&gt;=1.0.0</code> <code>1.0.5</code> MIT <code>srsly</code> Modern high-performance serialization utilities for Python <code>&lt;3.0.0,&gt;=2.4.3</code> <code>2.4.8</code> MIT <code>stevedore</code> Manage dynamic plugins for Python applications <code>&gt;=1.20.0</code> <code>5.1.0</code> Apache Software License <code>thinc</code> A refreshing functional take on deep learning, compatible with your favorite libraries <code>&lt;8.2.0,&gt;=8.1.0</code> <code>8.1.12</code> MIT <code>toml</code> Python Library for Tom's Obvious, Minimal Language <code>&gt;=0.10</code> <code>0.10.2</code> MIT <code>tomli</code> A lil' TOML parser <code>&gt;=2.0.1; python_version &lt; \"3.11\"</code> <code>2.0.1</code> MIT License <code>tqdm</code> Fast, Extensible Progress Meter <code>&lt;5.0.0,&gt;=4.38.0</code> <code>4.66.1</code> MPL-2.0 AND MIT <code>typer</code> Typer, build great CLIs. Easy to code. Based on Python type hints. <code>&lt;0.8.0,&gt;=0.3.0</code> <code>0.7.0</code> MIT License <code>types-markdown</code> Typing stubs for Markdown <code>&gt;=3.3</code> <code>3.5.0.3</code> Apache-2.0 license <code>types-toml</code> Typing stubs for toml <code>&gt;=0.10</code> <code>0.10.8.7</code> Apache-2.0 license <code>typing-extensions</code> Backported and Experimental Type Hints for Python 3.7+ <code>&lt;4.6</code> <code>4.5.0</code> Python Software Foundation License <code>urllib3</code> HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.26,&lt;2</code> <code>1.26.18</code> MIT <code>wasabi</code> A lightweight console printing and formatting toolkit <code>&lt;1.1.0,&gt;=0.9.1</code> <code>0.10.1</code> MIT <code>watchdog</code> Filesystem events monitoring <code>&gt;=2.0</code> <code>3.0.0</code> Apache License 2.0 <code>wps-light</code> The strictest and most opinionated python linter ever (lighter fork). <code>&gt;=0.15</code> <code>0.16.1</code> MIT <code>zipp</code> Backport of pathlib-compatible object wrapper for zip files <code>&gt;=0.5</code> <code>3.17.0</code> MIT License"},{"location":"license/","title":"CeCILL-C FREE SOFTWARE LICENSE AGREEMENT","text":""},{"location":"license/#notice","title":"Notice","text":"<p>This Agreement is a Free Software license agreement that is the result of discussions between its authors in order to ensure compliance with the two main principles guiding its drafting:</p> <ul> <li> <p>firstly, compliance with the principles governing the distribution of Free Software: access to source code, broad rights granted to users,</p> </li> <li> <p>secondly, the election of a governing law, French law, with which it is conformant, both as regards the law of torts and intellectual property law, and the protection that it offers to both authors and holders of the economic rights over software.</p> </li> </ul> <p>The authors of the CeCILL-C license are:</p> <ul> <li> <p>Commissariat \u00e0 l'Energie Atomique - CEA, a public scientific, technical and industrial research establishment, having its principal place of business at 25 rue Leblanc, immeuble Le Ponant D, 75015 Paris, France.</p> </li> <li> <p>Centre National de la Recherche Scientifique - CNRS, a public scientific and technological establishment, having its principal place of business at 3 rue Michel-Ange, 75794 Paris cedex 16, France.</p> </li> <li> <p>Institut National de Recherche en Informatique et en Automatique - INRIA, a public scientific and technological establishment, having its principal place of business at Domaine de Voluceau, Rocquencourt, BP 105, 78153 Le Chesnay cedex, France.</p> </li> </ul> <p> CeCILL stands for Ce(a) C(nrs) I(nria) L(ogiciel) L(ibre)</p>"},{"location":"license/#preamble","title":"Preamble","text":"<p>The purpose of this Free Software license agreement is to grant users the right to modify and re-use the software governed by this license.</p> <p>The exercising of this right is conditional upon the obligation to make available to the community the modifications made to the source code of the software so as to contribute to its evolution.</p> <p>In consideration of access to the source code and the rights to copy, modify and redistribute granted by the license, users are provided only with a limited warranty and the software's author, the holder of the economic rights, and the successive licensors only have limited liability.</p> <p>In this respect, the risks associated with loading, using, modifying and/or developing or reproducing the software by the user are brought to the user's attention, given its Free Software status, which may make it complicated to use, with the result that its use is reserved for developers and experienced professionals having in-depth computer knowledge. Users are therefore encouraged to load and test the suitability of the software as regards their requirements in conditions enabling the security of their systems and/or data to be ensured and, more generally, to use and operate it in the same conditions of security. This Agreement may be freely reproduced and published, provided it is not altered, and that no provisions are either added or removed herefrom.</p> <p>This Agreement may apply to any or all software for which the holder of the economic rights decides to submit the use thereof to its provisions.</p>"},{"location":"license/#article-1-definitions","title":"Article 1 - DEFINITIONS","text":"<p>For the purpose of this Agreement, when the following expressions commence with a capital letter, they shall have the following meaning:</p> <p>Agreement: means this license agreement, and its possible subsequent versions and annexes.</p> <p>Software: means the software in its Object Code and/or Source Code form and, where applicable, its documentation, \"as is\" when the Licensee accepts the Agreement.</p> <p>Initial Software: means the Software in its Source Code and possibly its Object Code form and, where applicable, its documentation, \"as is\" when it is first distributed under the terms and conditions of the Agreement.</p> <p>Modified Software: means the Software modified by at least one Integrated Contribution.</p> <p>Source Code: means all the Software's instructions and program lines to which access is required so as to modify the Software.</p> <p>Object Code: means the binary files originating from the compilation of the Source Code.</p> <p>Holder: means the holder(s) of the economic rights over the Initial Software.</p> <p>Licensee: means the Software user(s) having accepted the Agreement.</p> <p>Contributor: means a Licensee having made at least one Integrated Contribution.</p> <p>Licensor: means the Holder, or any other individual or legal entity, who distributes the Software under the Agreement.</p> <p>Integrated Contribution: means any or all modifications, corrections, translations, adaptations and/or new functions integrated into the Source Code by any or all Contributors.</p> <p>Related Module: means a set of sources files including their documentation that, without modification to the Source Code, enables supplementary functions or services in addition to those offered by the Software.</p> <p>Derivative Software: means any combination of the Software, modified or not, and of a Related Module.</p> <p>Parties: mean both the Licensee and the Licensor.</p> <p>These expressions may be used both in singular and plural form.</p>"},{"location":"license/#article-2-purpose","title":"Article 2 - PURPOSE","text":"<p>The purpose of the Agreement is the grant by the Licensor to the Licensee of a non-exclusive, transferable and worldwide license for the Software as set forth in Article 5 hereinafter for the whole term of the protection granted by the rights over said Software.</p>"},{"location":"license/#article-3-acceptance","title":"Article 3 - ACCEPTANCE","text":"<p> 3.1 The Licensee shall be deemed as having accepted the terms and conditions of this Agreement upon the occurrence of the first of the following events:</p> <ul> <li> <p>(i) loading the Software by any or all means, notably, by downloading from a remote server, or by loading from a physical medium;</p> </li> <li> <p>(ii) the first time the Licensee exercises any of the rights granted hereunder.</p> </li> </ul> <p> 3.2 One copy of the Agreement, containing a notice relating to the characteristics of the Software, to the limited warranty, and to the fact that its use is restricted to experienced users has been provided to the Licensee prior to its acceptance as set forth in Article 3.1 hereinabove, and the Licensee hereby acknowledges that it has read and understood it.</p>"},{"location":"license/#article-4-effective-date-and-term","title":"Article 4 - EFFECTIVE DATE AND TERM","text":""},{"location":"license/#41-effective-date","title":"4.1 EFFECTIVE DATE","text":"<p>The Agreement shall become effective on the date when it is accepted by the Licensee as set forth in Article 3.1.</p>"},{"location":"license/#42-term","title":"4.2 TERM","text":"<p>The Agreement shall remain in force for the entire legal term of protection of the economic rights over the Software.</p>"},{"location":"license/#article-5-scope-of-rights-granted","title":"Article 5 - SCOPE OF RIGHTS GRANTED","text":"<p>The Licensor hereby grants to the Licensee, who accepts, the following rights over the Software for any or all use, and for the term of the Agreement, on the basis of the terms and conditions set forth hereinafter.</p> <p>Besides, if the Licensor owns or comes to own one or more patents protecting all or part of the functions of the Software or of its components, the Licensor undertakes not to enforce the rights granted by these patents against successive Licensees using, exploiting or modifying the Software. If these patents are transferred, the Licensor undertakes to have the transferees subscribe to the obligations set forth in this paragraph.</p>"},{"location":"license/#51-right-of-use","title":"5.1 RIGHT OF USE","text":"<p>The Licensee is authorized to use the Software, without any limitation as to its fields of application, with it being hereinafter specified that this comprises:</p> <ol> <li> <p>permanent or temporary reproduction of all or part of the Software by any or all means and in any or all form.</p> </li> <li> <p>loading, displaying, running, or storing the Software on any or all medium.</p> </li> <li> <p>entitlement to observe, study or test its operation so as to determine the ideas and principles behind any or all constituent elements of said Software. This shall apply when the Licensee carries out any or all loading, displaying, running, transmission or storage operation as regards the Software, that it is entitled to carry out hereunder.</p> </li> </ol>"},{"location":"license/#52-right-of-modification","title":"5.2 RIGHT OF MODIFICATION","text":"<p>The right of modification includes the right to translate, adapt, arrange, or make any or all modifications to the Software, and the right to reproduce the resulting software. It includes, in particular, the right to create a Derivative Software.</p> <p>The Licensee is authorized to make any or all modification to the Software provided that it includes an explicit notice that it is the author of said modification and indicates the date of the creation thereof.</p>"},{"location":"license/#53-right-of-distribution","title":"5.3 RIGHT OF DISTRIBUTION","text":"<p>In particular, the right of distribution includes the right to publish, transmit and communicate the Software to the general public on any or all medium, and by any or all means, and the right to market, either in consideration of a fee, or free of charge, one or more copies of the Software by any means.</p> <p>The Licensee is further authorized to distribute copies of the modified or unmodified Software to third parties according to the terms and conditions set forth hereinafter.</p>"},{"location":"license/#531-distribution-of-software-without-modification","title":"5.3.1 DISTRIBUTION OF SOFTWARE WITHOUT MODIFICATION","text":"<p>The Licensee is authorized to distribute true copies of the Software in Source Code or Object Code form, provided that said distribution complies with all the provisions of the Agreement and is accompanied by:</p> <ol> <li> <p>a copy of the Agreement,</p> </li> <li> <p>a notice relating to the limitation of both the Licensor's warranty and liability as set forth in Article 8 and Article 9,</p> </li> </ol> <p>and that, in the event that only the Object Code of the Software is redistributed, the Licensee allows effective access to the full Source Code of the Software at a minimum during the entire period of its distribution of the Software, it being understood that the additional cost of acquiring the Source Code shall not exceed the cost of transferring the data.</p>"},{"location":"license/#532-distribution-of-modified-software","title":"5.3.2 DISTRIBUTION OF MODIFIED SOFTWARE","text":"<p>When the Licensee makes an Integrated Contribution to the Software, the terms and conditions for the distribution of the resulting Modified Software become subject to all the provisions of this Agreement.</p> <p>The Licensee is authorized to distribute the Modified Software, in source code or object code form, provided that said distribution complies with all the provisions of the Agreement and is accompanied by:</p> <ol> <li> <p>a copy of the Agreement,</p> </li> <li> <p>a notice relating to the limitation of both the Licensor's warranty and liability as set forth in Article 8 and Article 9,</p> </li> </ol> <p>and that, in the event that only the object code of the Modified Software is redistributed, the Licensee allows effective access to the full source code of the Modified Software at a minimum during the entire period of its distribution of the Modified Software, it being understood that the additional cost of acquiring the source code shall not exceed the cost of transferring the data.</p>"},{"location":"license/#533-distribution-of-derivative-software","title":"5.3.3 DISTRIBUTION OF DERIVATIVE SOFTWARE","text":"<p>When the Licensee creates Derivative Software, this Derivative Software may be distributed under a license agreement other than this Agreement, subject to compliance with the requirement to include a notice concerning the rights over the Software as defined in Article 6.4. In the event the creation of the Derivative Software required modification of the Source Code, the Licensee undertakes that:</p> <ol> <li> <p>the resulting Modified Software will be governed by this Agreement,</p> </li> <li> <p>the Integrated Contributions in the resulting Modified Software will be clearly identified and documented,</p> </li> <li> <p>the Licensee will allow effective access to the source code of the Modified Software, at a minimum during the entire period of distribution of the Derivative Software, such that such modifications may be carried over in a subsequent version of the Software; it being understood that the additional cost of purchasing the source code of the Modified Software shall not exceed the cost of transferring the data.</p> </li> </ol>"},{"location":"license/#534-compatibility-with-the-cecill-license","title":"5.3.4 COMPATIBILITY WITH THE CeCILL LICENSE","text":"<p>When a Modified Software contains an Integrated Contribution subject to the CeCILL license agreement, or when a Derivative Software contains a Related Module subject to the CeCILL license agreement, the provisions set forth in the third item of Article 6.4 are optional.</p>"},{"location":"license/#article-6-intellectual-property","title":"Article 6 - INTELLECTUAL PROPERTY","text":""},{"location":"license/#61-over-the-initial-software","title":"6.1 OVER THE INITIAL SOFTWARE","text":"<p>The Holder owns the economic rights over the Initial Software. Any or all use of the Initial Software is subject to compliance with the terms and conditions under which the Holder has elected to distribute its work and no one shall be entitled to modify the terms and conditions for the distribution of said Initial Software.</p> <p>The Holder undertakes that the Initial Software will remain ruled at least by this Agreement, for the duration set forth in Article 4.2.</p>"},{"location":"license/#62-over-the-integrated-contributions","title":"6.2 OVER THE INTEGRATED CONTRIBUTIONS","text":"<p>The Licensee who develops an Integrated Contribution is the owner of the intellectual property rights over this Contribution as defined by applicable law.</p>"},{"location":"license/#63-over-the-related-modules","title":"6.3 OVER THE RELATED MODULES","text":"<p>The Licensee who develops a Related Module is the owner of the intellectual property rights over this Related Module as defined by applicable law and is free to choose the type of agreement that shall govern its distribution under the conditions defined in Article 5.3.3.</p>"},{"location":"license/#64-notice-of-rights","title":"6.4 NOTICE OF RIGHTS","text":"<p>The Licensee expressly undertakes:</p> <ol> <li> <p>not to remove, or modify, in any manner, the intellectual property notices attached to the Software;</p> </li> <li> <p>to reproduce said notices, in an identical manner, in the copies of the Software modified or not;</p> </li> <li> <p>to ensure that use of the Software, its intellectual property notices and the fact that it is governed by the Agreement is indicated in a text that is easily accessible, specifically from the interface of any Derivative Software.</p> </li> </ol> <p>The Licensee undertakes not to directly or indirectly infringe the intellectual property rights of the Holder and/or Contributors on the Software and to take, where applicable, vis-\u00e0-vis its staff, any and all measures required to ensure respect of said intellectual property rights of the Holder and/or Contributors.</p>"},{"location":"license/#article-7-related-services","title":"Article 7 - RELATED SERVICES","text":"<p> 7.1 Under no circumstances shall the Agreement oblige the Licensor to provide technical assistance or maintenance services for the Software.</p> <p>However, the Licensor is entitled to offer this type of services. The terms and conditions of such technical assistance, and/or such maintenance, shall be set forth in a separate instrument. Only the Licensor offering said maintenance and/or technical assistance services shall incur liability therefor.</p> <p> 7.2 Similarly, any Licensor is entitled to offer to its licensees, under its sole responsibility, a warranty, that shall only be binding upon itself, for the redistribution of the Software and/or the Modified Software, under terms and conditions that it is free to decide. Said warranty, and the financial terms and conditions of its application, shall be subject of a separate instrument executed between the Licensor and the Licensee.</p>"},{"location":"license/#article-8-liability","title":"Article 8 - LIABILITY","text":"<p> 8.1 Subject to the provisions of Article 8.2, the Licensee shall be entitled to claim compensation for any direct loss it may have suffered from the Software as a result of a fault on the part of the relevant Licensor, subject to providing evidence thereof.</p> <p> 8.2 The Licensor's liability is limited to the commitments made under this Agreement and shall not be incurred as a result of in particular:</p> <ul> <li> <p>(i) loss due the Licensee's total or partial failure to fulfill its obligations,</p> </li> <li> <p>(ii) direct or consequential loss that is suffered by the Licensee due to the use or performance of the Software, and</p> </li> <li> <p>(iii) more generally, any consequential loss.</p> </li> </ul> <p>In particular the Parties expressly agree that any or all pecuniary or business loss (i.e. loss of data, loss of profits, operating loss, loss of customers or orders, opportunity cost, any disturbance to business activities) or any or all legal proceedings instituted against the Licensee by a third party, shall constitute consequential loss and shall not provide entitlement to any or all compensation from the Licensor.</p>"},{"location":"license/#article-9-warranty","title":"Article 9 - WARRANTY","text":"<p> 9.1 The Licensee acknowledges that the scientific and technical state-of-the-art when the Software was distributed did not enable all possible uses to be tested and verified, nor for the presence of possible defects to be detected. In this respect, the Licensee's attention has been drawn to the risks associated with loading, using, modifying and/or developing and reproducing the Software which are reserved for experienced users.</p> <p>The Licensee shall be responsible for verifying, by any or all means, the suitability of the product for its requirements, its good working order, and for ensuring that it shall not cause damage to either persons or properties.</p> <p> 9.2 The Licensor hereby represents, in good faith, that it is entitled to grant all the rights over the Software (including in particular the rights set forth in Article 5).</p> <p> 9.3 The Licensee acknowledges that the Software is supplied \"as is\" by the Licensor without any other express or tacit warranty, other than that provided for in Article 9.2 and, in particular, without any warranty as to its commercial value, its secured, safe, innovative or relevant nature.</p> <p>Specifically, the Licensor does not warrant that the Software is free from any error, that it will operate without interruption, that it will be compatible with the Licensee's own equipment and software configuration, nor that it will meet the Licensee's requirements.</p> <p> 9.4 The Licensor does not either expressly or tacitly warrant that the Software does not infringe any third party intellectual property right relating to a patent, software or any other property right. Therefore, the Licensor disclaims any and all liability towards the Licensee arising out of any or all proceedings for infringement that may be instituted in respect of the use, modification and redistribution of the Software. Nevertheless, should such proceedings be instituted against the Licensee, the Licensor shall provide it with technical and legal assistance for its defense. Such technical and legal assistance shall be decided on a case-by-case basis between the relevant Licensor and the Licensee pursuant to a memorandum of understanding. The Licensor disclaims any and all liability as regards the Licensee's use of the name of the Software. No warranty is given as regards the existence of prior rights over the name of the Software or as regards the existence of a trademark.</p>"},{"location":"license/#article-10-termination","title":"Article 10 - TERMINATION","text":"<p> 10.1 In the event of a breach by the Licensee of its obligations hereunder, the Licensor may automatically terminate this Agreement thirty (30) days after notice has been sent to the Licensee and has remained ineffective.</p> <p> 10.2 A Licensee whose Agreement is terminated shall no longer be authorized to use, modify or distribute the Software. However, any licenses that it may have granted prior to termination of the Agreement shall remain valid subject to their having been granted in compliance with the terms and conditions hereof.</p>"},{"location":"license/#article-11-miscellaneous","title":"Article 11 - MISCELLANEOUS","text":"<p> 11.1 EXCUSABLE EVENTS Neither Party shall be liable for any or all delay, or failure to perform the Agreement, that may be attributable to an event of force majeure, an act of God or an outside cause, such as defective functioning or interruptions of the electricity or telecommunications networks, network paralysis following a virus attack, intervention by government authorities, natural disasters, water damage, earthquakes, fire, explosions, strikes and labor unrest, war, etc.</p> <p> 11.2 Any failure by either Party, on one or more occasions, to invoke one or more of the provisions hereof, shall under no circumstances be interpreted as being a waiver by the interested Party of its right to invoke said provision(s) subsequently.</p> <p> 11.3 The Agreement cancels and replaces any or all previous agreements, whether written or oral, between the Parties and having the same purpose, and constitutes the entirety of the agreement between said Parties concerning said purpose. No supplement or modification to the terms and conditions hereof shall be effective as between the Parties unless it is made in writing and signed by their duly authorized representatives.</p> <p> 11.4 In the event that one or more of the provisions hereof were to conflict with a current or future applicable act or legislative text, said act or legislative text shall prevail, and the Parties shall make the necessary amendments so as to comply with said act or legislative text. All other provisions shall remain effective. Similarly, invalidity of a provision of the Agreement, for any reason whatsoever, shall not cause the Agreement as a whole to be invalid.</p> <p> 11.5 LANGUAGE The Agreement is drafted in both French and English and both versions are deemed authentic.</p>"},{"location":"license/#article-12-new-versions-of-the-agreement","title":"Article 12 - NEW VERSIONS OF THE AGREEMENT","text":"<p> 12.1 Any person is authorized to duplicate and distribute copies of this Agreement.</p> <p> 12.2 So as to ensure coherence, the wording of this Agreement is protected and may only be modified by the authors of the License, who reserve the right to periodically publish updates or new versions of the Agreement, each with a separate number. These subsequent versions may address new issues encountered by Free Software.</p> <p> 12.3 Any Software distributed under a given version of the Agreement may only be subsequently distributed under the same version of the Agreement or a subsequent version.</p>"},{"location":"license/#article-13-governing-law-and-jurisdiction","title":"Article 13 - GOVERNING LAW AND JURISDICTION","text":"<p> 13.1 The Agreement is governed by French law. The Parties agree to endeavor to seek an amicable solution to any disagreements or disputes that may arise during the performance of the Agreement.</p> <p> 13.2 Failing an amicable solution within two (2) months as from their occurrence, and unless emergency proceedings are necessary, the disagreements or disputes shall be referred to the Paris Courts having jurisdiction, by the more diligent Party.</p> <p>Version 1.0 dated 2006-09-05.</p>"},{"location":"usage/","title":"Usage","text":"<p>Import dependencies. <pre><code>from cognitivefactory.interactive_clustering.utils.preprocessing import preprocess\nfrom cognitivefactory.interactive_clustering.utils.vectorization import vectorize\nfrom cognitivefactory.interactive_clustering.constraints.factory import managing_factory\nfrom cognitivefactory.interactive_clustering.clustering.factory import clustering_factory\nfrom cognitivefactory.interactive_clustering.sampling.factory import sampling_factory\n</code></pre></p>"},{"location":"usage/#initialization-step-iteration-0","title":"Initialization step (iteration <code>0</code>)","text":"<p>Get data. <pre><code># Define dictionary of texts.\ndict_of_texts = {\n    \"0\": \"This is my first question.\",\n    \"1\": \"This is my second item.\",\n    \"2\": \"This is my third query.\",\n    \"3\": \"This is my fourth issue.\",\n    # ...\n    \"N\": \"This is my last request.\",\n}\n</code></pre></p> <p>Preprocess data. <pre><code># Preprocess data.\ndict_of_preprocess_texts = preprocess(\n    dict_of_texts=dict_of_texts,\n    spacy_language_model=\"fr_core_news_md\",\n)  # Apply simple preprocessing. Spacy language model has to be installed. Other parameters are available.\n</code></pre></p> <p>Vectorize data. <pre><code># Vectorize data.\ndict_of_vectors = vectorize(\n    dict_of_texts=dict_of_preprocess_texts,\n    vectorizer_type=\"tfidf\",\n)  # Apply TF-IDF vectorization. Other parameters are available.\n</code></pre></p> <p>Initialize constraints manager. <pre><code># Create an instance of binary constraints manager.\nconstraints_manager = managing_factory(\n    manager=\"binary\",\n    list_of_data_IDs = list(dict_of_texts.keys()),\n)\n</code></pre></p> <p>Apply first clustering without constraints. <pre><code># Create an instance of constrained COP-kmeans clustering.\nclustering_model = clustering_factory(\n    algorithm=\"kmeans\",\n    random_seed=1,\n)  # Other clustering algorithms are available.\n\n# Run clustering.\nclustering_result = clustering_model.cluster(\n    constraints_manager=constraints_manager,\n    nb_clusters=2,\n    vectors=dict_of_vectors,\n)\n</code></pre></p>"},{"location":"usage/#iteration-step-iteration-n","title":"Iteration step (iteration <code>N</code>)","text":"<p>Check if all possible constraints are annotated. <pre><code># Check if all constraints are already annotated.\nis_finish = constraints_manager.check_completude_of_constraints()\n\n# Print result\nif is_finish:\n    print(\"All possible constraints are annotated. No more iteration can be run.\")\n    # break\n</code></pre></p> <p>Sampling constraints to annotate. <pre><code># Create an instance of random sampler.\nsampler = sampling_factory(\n    algorithm=\"random\",\n    random_seed=None,\n)  # Other algorithms are available.\n\n# Sample constraints to annotated.\nselection = sampler.sample(\n    constraints_manager=constraints_manager,\n    nb_to_select=3,\n    #clustering_result=clustering_result,  # Results from iteration `N-1`.\n    #vectors=dict_of_vectors,\n)\n</code></pre></p> <p>Annotate constraints (manual operation). <pre><code># TODO: Use a graphical interface for interactive clustering.\n# WIP: Project `interactive-clustering-gui`.\n\nlist_of_annotation = []  # List of triplets with format `(data_ID1, data_ID2, annotation_type)` where `annotation_type` can be \"MUST_LINK\" or \"CANNOT_LINK\".\n</code></pre></p> <p>Update constraints manager. <pre><code>for annotation in list_of_annotation:\n\n    # Get the annotation\n    data_ID1, data_ID2, constraint_type = annotation\n\n    # Add constraints\n    try:\n        constraints_manager.add_constraint(\n            data_ID1=data_ID1,\n            data_ID2=data_ID2,\n            constraint_type=constraint_type\n        )\n    except ValueError as err:\n        print(err)  # An error can occur if parameters are incorrect or if annotation is incompatible with previous annotation.\n</code></pre></p> <p>Determine the range of possible cluster number. <pre><code># Get min and max range of clusters based on constraints.\nmin_n, max_n = constraints_manager.get_min_and_max_number_of_clusters()\n\n# Choose the number of cluster.\nnb_clusters = int( (min_n + max_n) / 2 ) # or manual selection.\n</code></pre></p> <p>Run constrained clustering. <pre><code># Create an instance of constrained COP-kmeans clustering.\nclustering_model = clustering_factory(\n    algorithm=\"kmeans\",\n    random_seed=1,\n)  # Other clustering algorithms are available.\n\n# Run clustering.\nclustering_result = clustering_model.cluster(\n    constraints_manager=constraints_manager,  # Annotation since iteration `0`.\n    nb_clusters=nb_clusters,\n    vectors=dict_of_vectors,\n)  # Clustering results are corrected since the previous iteration.\n</code></pre></p> <p>Analyze cluster (not implemented here). <pre><code># TODO: Evaluate completness, homogeneity, v-measure, rand index (basic, adjusted), mutual information (basic, normalized, mutual), ...\n# TODO: Plot clustering.\n</code></pre></p>"},{"location":"docs/figures/","title":"Index","text":"<p>This figures are used in integration of main repository <code>README.md</code> in repository documentation with mkdocs (<code>pdm run duty docs</code>).</p> <p>They are duplicates of <code>docs/figures/</code>, used in main repository <code>README.md</code> on GitHub.</p>"},{"location":"figures/","title":"Index","text":"<p>This figures are used in main repository <code>README.md</code> on GitHub.</p> <p>Duplicate <code>docs/figures/</code> in <code>docs/docs/figures/</code> in order to make available these figures for integration of main repository <code>README.md</code> in repository documentation with mkdocs (<code>pdm run duty docs</code>).</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cognitivefactory<ul> <li>interactive_clustering<ul> <li>clustering<ul> <li>abstract</li> <li>affinity_propagation</li> <li>dbscan</li> <li>factory</li> <li>hierarchical</li> <li>kmeans</li> <li>mpckmeans</li> <li>spectral</li> </ul> </li> <li>constraints<ul> <li>abstract</li> <li>binary</li> <li>factory</li> </ul> </li> <li>sampling<ul> <li>abstract</li> <li>clusters_based</li> <li>factory</li> </ul> </li> <li>utils<ul> <li>frequency</li> <li>preprocessing</li> <li>vectorization</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/","title":"interactive_clustering","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering</li> <li>Description:  Python package used to apply NLP interactive clustering methods.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul> <p>Three modules are available:</p> <ul> <li><code>constraints</code>: it provides a constraints manager, that stores annotated constraints on data and gives some feedback on information deduced (such as the transitivity between constraints or the situation of inconsistency). See interactive_clustering/constraints documentation ;</li> <li><code>sampling</code>: it provides several constraints sampling algorithm, that selecte relevant contraints to annotate by an expert. See interactive_clustering/sampling documentation ;</li> <li><code>clustering</code>: it provides several constrained clustering algorithms, that partition the data according to annotated constraints. See interactive_clustering/clustering documentation ;</li> <li><code>utils</code>: it provides several basic functionnalities, like data preprocessing and data vectorization. See interactive_clustering/utils documentation.</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/","title":"clustering","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.clustering</li> <li>Description:  Constrained clustering module of the Interactive Clustering package.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul> <p>This module provides several constrained clustering algorithms, that partition the data according to annotated constraints :</p> <ul> <li><code>abstract</code>: an abstract class that defines constrained clustering algorithms functionnalities. See interactive_clustering/clustering/abstract documentation ;</li> <li><code>factory</code>: a factory to easily instantiate constrained clustering algorithm object. See interactive_clustering/clustering/factory documentation ;</li> <li><code>kmeans</code>: a constrained clustering algorithm implementation that uses COP-KMeans. See interactive_clustering/clustering/kmeans documentation ;</li> <li><code>hierarchical</code>: a constrained clustering algorithm implementation that uses constrained hierarchical clustering. See interactive_clustering/clustering/hierarchical documentation ;</li> <li><code>spectral</code>: a constrained clustering algorithm implementation that uses constrained spectral clustering. See interactive_clustering/clustering/spectral documentation ;</li> <li><code>affinity_propagation</code>: a constrained clustering algorithm implementation that uses constrained affinity propagation clustering (not production ready !). See interactive_clustering/clustering/affinity_propagation documentation ;</li> <li><code>dbscan</code>: a constrained clustering algorithm implementation that uses C-DBScan (not production ready !). See interactive_clustering/clustering/dbscan documentation ;</li> <li><code>mpckmeans</code>: a constrained clustering algorithm implementation that uses MPC-KMeans (not production ready !). See interactive_clustering/clustering/mpckmeans documentation.</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/abstract/","title":"abstract","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.clustering.abstract</li> <li>Description:  The abstract class used to define constrained clustering algorithms.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/abstract/#cognitivefactory.interactive_clustering.clustering.abstract.AbstractConstrainedClustering","title":"<code>AbstractConstrainedClustering</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class that is used to define constrained clustering algorithms. The main inherited method is <code>cluster</code>.</p> References <ul> <li>Survey on Constrained Clustering : <code>Lampert, T., T.-B.-H. Dao, B. Lafabregue, N. Serrette, G. Forestier, B. Cremilleux, C. Vrain, et P. Gancarski (2018). Constrained distance based clustering for time-series : a comparative and experimental study. Data Mining and Knowledge Discovery 32(6), 1663\u20131707.</code></li> </ul> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\abstract.py</code> <pre><code>class AbstractConstrainedClustering(ABC):\n    \"\"\"\n    Abstract class that is used to define constrained clustering algorithms.\n    The main inherited method is `cluster`.\n\n    References:\n        - Survey on Constrained Clustering : `Lampert, T., T.-B.-H. Dao, B. Lafabregue, N. Serrette, G. Forestier, B. Cremilleux, C. Vrain, et P. Gancarski (2018). Constrained distance based clustering for time-series : a comparative and experimental study. Data Mining and Knowledge Discovery 32(6), 1663\u20131707.`\n    \"\"\"\n\n    # ==============================================================================\n    # ABSTRACT METHOD - CLUSTER\n    # ==============================================================================\n    @abstractmethod\n    def cluster(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        vectors: Dict[str, csr_matrix],\n        nb_clusters: Optional[int],\n        verbose: bool = False,\n        **kargs,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to cluster data.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n            vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n            nb_clusters (Optional[int]): The number of clusters to compute. Can be `None` if this parameters is estimated or if the algorithm doesn't need it.\n            verbose (bool, optional): Enable verbose output. Defaults to `False`.\n            **kargs (dict): Other parameters that can be used in the clustering.\n\n        Raises:\n            ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/abstract/#cognitivefactory.interactive_clustering.clustering.abstract.AbstractConstrainedClustering.cluster","title":"<code>cluster(constraints_manager, vectors, nb_clusters, verbose=False, **kargs)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to cluster data.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs that will force clustering to respect some conditions during computation.</p> required <code>vectors</code> <code>Dict[str, csr_matrix]</code> <p>The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data.</p> required <code>nb_clusters</code> <code>Optional[int]</code> <p>The number of clusters to compute. Can be <code>None</code> if this parameters is estimated or if the algorithm doesn't need it.</p> required <code>verbose</code> <code>bool</code> <p>Enable verbose output. Defaults to <code>False</code>.</p> <code>False</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the clustering.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>vectors</code> and <code>constraints_manager</code> are incompatible, or if some parameters are incorrectly set.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\abstract.py</code> <pre><code>@abstractmethod\ndef cluster(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    vectors: Dict[str, csr_matrix],\n    nb_clusters: Optional[int],\n    verbose: bool = False,\n    **kargs,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to cluster data.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n        vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n        nb_clusters (Optional[int]): The number of clusters to compute. Can be `None` if this parameters is estimated or if the algorithm doesn't need it.\n        verbose (bool, optional): Enable verbose output. Defaults to `False`.\n        **kargs (dict): Other parameters that can be used in the clustering.\n\n    Raises:\n        ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n    Returns:\n        Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/abstract/#cognitivefactory.interactive_clustering.clustering.abstract.rename_clusters_by_order","title":"<code>rename_clusters_by_order(clusters)</code>","text":"<p>Rename cluster ID to be ordered by data IDs.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>Dict[str, int]</code> <p>The dictionary of clusters.</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str, int]: The sorted dictionary of clusters.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\abstract.py</code> <pre><code>def rename_clusters_by_order(\n    clusters: Dict[str, int],\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Rename cluster ID to be ordered by data IDs.\n\n    Args:\n        clusters (Dict[str, int]): The dictionary of clusters.\n\n    Returns:\n        Dict[str, int]: The sorted dictionary of clusters.\n    \"\"\"\n\n    # Get `list_of_data_IDs`.\n    list_of_data_IDs = sorted(clusters.keys())\n\n    # Define a map to be able to rename cluster IDs.\n    mapping_of_old_ID_to_new_ID: Dict[int, int] = {}\n    new_ID: int = 0\n    for data_ID in list_of_data_IDs:  # , cluster_ID in clusters.items():\n        if clusters[data_ID] not in mapping_of_old_ID_to_new_ID.keys():\n            mapping_of_old_ID_to_new_ID[clusters[data_ID]] = new_ID\n            new_ID += 1\n\n    # Rename cluster IDs.\n    new_clusters = {\n        data_ID_to_assign: mapping_of_old_ID_to_new_ID[clusters[data_ID_to_assign]]\n        for data_ID_to_assign in list_of_data_IDs\n    }\n\n    # Return the new ordered clusters\n    return new_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/affinity_propagation/","title":"affinity_propagation","text":"<ul> <li>Name:         interactive-clustering/src/clustering/affinity_propagation.py</li> <li>Description:  Implementation of constrained Affinity Propagation clustering algorithm.</li> <li>Author:       David NICOLAZO, Esther LENOTRE, Marc TRUTT</li> <li>Created:      02/03/2022</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/affinity_propagation/#cognitivefactory.interactive_clustering.clustering.affinity_propagation.AffinityPropagationConstrainedClustering","title":"<code>AffinityPropagationConstrainedClustering</code>","text":"<p>             Bases: <code>AbstractConstrainedClustering</code></p> <p>This class will implements the Affinity Propagation constrained clustering. It inherits from <code>AbstractConstrainedClustering</code>.</p> References <ul> <li>Affinity Propagation Clustering: <code>Frey, B. J., &amp; Dueck, D. (2007). Clustering by Passing Messages Between Data Points. In Science (Vol. 315, Issue 5814, pp. 972\u2013976). American Association for the Advancement of Science (AAAS). https://doi.org/10.1126/science.1136800</code></li> <li>Constrained Affinity Propagation Clustering: <code>Givoni, I., &amp; Frey, B. J. (2009). Semi-Supervised Affinity Propagation with Instance-Level Constraints. Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, PMLR 5:161-168</code></li> </ul> Example <pre><code># Import.\nfrom scipy.sparse import csr_matrix\nfrom cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\nfrom cognitivefactory.interactive_clustering.clustering.affinity_propagation import AffinityPropagationConstrainedClustering\n\n# Create an instance of affinity propagation clustering.\nclustering_model = AffinityPropagationConstrainedClustering(\n    random_seed=1,\n)\n\n# Define vectors.\n# NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\nvectors = {\n    \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n    \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n    \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n    \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n    \"4\": csr_matrix([0.60, 0.17, 0.16, 0.07]),\n    \"5\": csr_matrix([0.60, 0.16, 0.17, 0.07]),\n    \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n    \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n    \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n}\n\n# Define constraints manager.\nconstraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"3\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"7\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"7\", constraint_type=\"CANNOT_LINK\")\n\n# Run clustering.\ndict_of_predicted_clusters = clustering_model.cluster(\n    constraints_manager=constraints_manager,\n    vectors=vectors,\n    ####nb_clusters=None,\n)\n\n# Print results.\nprint(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 1, \"5\": 1, \"6\": 2, \"7\": 2, \"8\": 2,})  # TODO:\nprint(\"Computed results\", \":\", dict_of_predicted_clusters)\n</code></pre> <p>Warns:</p> Type Description <code>FutureWarning</code> <p><code>clustering.affinity_propagation.AffinityPropagationConstrainedClustering</code> is still in development and is not fully tested : it is not ready for production use.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\affinity_propagation.py</code> <pre><code>class AffinityPropagationConstrainedClustering(AbstractConstrainedClustering):\n    \"\"\"\n    This class will implements the Affinity Propagation constrained clustering.\n    It inherits from `AbstractConstrainedClustering`.\n\n    References:\n        - Affinity Propagation Clustering: `Frey, B. J., &amp; Dueck, D. (2007). Clustering by Passing Messages Between Data Points. In Science (Vol. 315, Issue 5814, pp. 972\u2013976). American Association for the Advancement of Science (AAAS). https://doi.org/10.1126/science.1136800`\n        - Constrained Affinity Propagation Clustering: `Givoni, I., &amp; Frey, B. J. (2009). Semi-Supervised Affinity Propagation with Instance-Level Constraints. Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, PMLR 5:161-168`\n\n    Example:\n        ```python\n        # Import.\n        from scipy.sparse import csr_matrix\n        from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\n        from cognitivefactory.interactive_clustering.clustering.affinity_propagation import AffinityPropagationConstrainedClustering\n\n        # Create an instance of affinity propagation clustering.\n        clustering_model = AffinityPropagationConstrainedClustering(\n            random_seed=1,\n        )\n\n        # Define vectors.\n        # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\n        vectors = {\n            \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n            \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n            \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n            \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n            \"4\": csr_matrix([0.60, 0.17, 0.16, 0.07]),\n            \"5\": csr_matrix([0.60, 0.16, 0.17, 0.07]),\n            \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n            \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n            \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n        }\n\n        # Define constraints manager.\n        constraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"3\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"7\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"7\", constraint_type=\"CANNOT_LINK\")\n\n        # Run clustering.\n        dict_of_predicted_clusters = clustering_model.cluster(\n            constraints_manager=constraints_manager,\n            vectors=vectors,\n            ####nb_clusters=None,\n        )\n\n        # Print results.\n        print(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 1, \"5\": 1, \"6\": 2, \"7\": 2, \"8\": 2,})  # TODO:\n        print(\"Computed results\", \":\", dict_of_predicted_clusters)\n        ```\n\n    Warns:\n        FutureWarning: `clustering.affinity_propagation.AffinityPropagationConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n    \"\"\"\n\n    def __init__(\n        self,\n        max_iteration: int = 150,\n        convergence_iteration: int = 10,\n        random_seed: Optional[int] = None,\n        absolute_must_links: bool = True,\n        **kargs,\n    ) -&gt; None:\n        \"\"\"\n        The constructor for the Affinity Propagation constrained clustering.\n\n        Args:\n            max_iteration (int, optional): The maximum number of iteration for convergence. Defaults to `150`.\n            convergence_iteration (int, optional): The number of iterations with no change to consider a convergence. Default to `15`.\n            absolute_must_links (bool, optional): the option to strictly respect `\"MUST_LINK\"` type constraints. Defaults to ``True`.\n            random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`.\n            **kargs (dict): Other parameters that can be used in the instantiation.\n\n        Warns:\n            FutureWarning: `clustering.affinity_propagation.AffinityPropagationConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set.\n        \"\"\"\n\n        # Deprecation warnings\n        warnings.warn(\n            \"`clustering.affinity_propagation.AffinityPropagationConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\",\n            FutureWarning,  # DeprecationWarning\n            stacklevel=2,\n        )\n\n        # Store 'self.max_iteration`.\n        if max_iteration &lt; 1:\n            raise ValueError(\"The `max_iteration` must be greater than or equal to 1.\")\n        self.max_iteration: int = max_iteration\n\n        # Store 'self.convergence_iteration`.\n        if convergence_iteration &lt; 1:\n            raise ValueError(\"The `convergence_iteration` must be greater than or equal to 1.\")\n        self.convergence_iteration: int = convergence_iteration\n\n        # Store 'self.absolute_must_links`.\n        self.absolute_must_links: bool = absolute_must_links\n\n        # Store 'self.random_seed`.\n        self.random_seed: Optional[int] = random_seed\n\n        # Store `self.kargs` for kmeans clustering.\n        self.kargs = kargs\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n\n    # ==============================================================================\n    # MAIN - CLUSTER DATA\n    # ==============================================================================\n\n    def cluster(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        vectors: Dict[str, csr_matrix],\n        nb_clusters: Optional[int] = None,\n        verbose: bool = False,\n        **kargs,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        The main method used to cluster data with the KMeans model.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n            vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n            nb_clusters (Optional[int]): The number of clusters to compute. Here `None`.\n            verbose (bool, optional): Enable verbose output. Defaults to `False`.\n            **kargs (dict): Other parameters that can be used in the clustering.\n\n        Raises:\n            ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        ###\n        ### GET PARAMETERS\n        ###\n\n        # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n        if not isinstance(constraints_manager, AbstractConstraintsManager):\n            raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n        self.constraints_manager: AbstractConstraintsManager = constraints_manager\n        self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n        # Store `self.vectors`.\n        if not isinstance(vectors, dict):\n            raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n        self.vectors: Dict[str, csr_matrix] = vectors\n\n        # Store `self.nb_clusters`.\n        if nb_clusters is not None:\n            raise ValueError(\"The `nb_clusters` should be 'None' for Affinity Propagataion clustering.\")\n        self.nb_clusters: Optional[int] = None\n\n        ###\n        ### RUN AFFINITY PROPAGATION CONSTRAINED CLUSTERING\n        ###\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters = None\n\n        # Correspondances ID -&gt; index\n        data_ID_to_idx: Dict[str, int] = {v: i for i, v in enumerate(self.list_of_data_IDs)}\n        n_sample: int = len(self.list_of_data_IDs)\n\n        # Compute similarity between data points.\n        S: csr_matrix = -pairwise_distances(vstack(self.vectors[data_ID] for data_ID in self.list_of_data_IDs))\n\n        # Get connected components (closures of MUST_LINK contraints).\n        must_link_closures: List[List[str]] = self.constraints_manager.get_connected_components()\n        must_links: List[List[int]] = [[data_ID_to_idx[ID] for ID in closure] for closure in must_link_closures]\n\n        # Get annotated CANNOT_LINK contraints.\n        cannot_links: List[Tuple[int, int]] = []\n        for data_ID_i1, data_ID_j1 in combinations(range(n_sample), 2):\n            constraint = self.constraints_manager.get_added_constraint(\n                self.list_of_data_IDs[data_ID_i1], self.list_of_data_IDs[data_ID_j1]\n            )\n            if constraint and constraint[0] == \"CANNOT_LINK\":\n                cannot_links.append((data_ID_i1, data_ID_j1))\n\n        # Run constrained affinity propagation.\n        cluster_labels: List[int] = _affinity_propagation_constrained(\n            S,\n            must_links=must_links,\n            cannot_links=cannot_links,\n            absolute_must_links=self.absolute_must_links,\n            max_iteration=self.max_iteration,\n            convergence_iteration=self.convergence_iteration,\n            random_seed=self.random_seed,\n            verbose=verbose,\n        )\n\n        # Rename cluster IDs by order.\n        self.dict_of_predicted_clusters = rename_clusters_by_order(\n            {self.list_of_data_IDs[i]: l for i, l in enumerate(cluster_labels)}\n        )\n\n        return self.dict_of_predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/affinity_propagation/#cognitivefactory.interactive_clustering.clustering.affinity_propagation.AffinityPropagationConstrainedClustering.__init__","title":"<code>__init__(max_iteration=150, convergence_iteration=10, random_seed=None, absolute_must_links=True, **kargs)</code>","text":"<p>The constructor for the Affinity Propagation constrained clustering.</p> <p>Parameters:</p> Name Type Description Default <code>max_iteration</code> <code>int</code> <p>The maximum number of iteration for convergence. Defaults to <code>150</code>.</p> <code>150</code> <code>convergence_iteration</code> <code>int</code> <p>The number of iterations with no change to consider a convergence. Default to <code>15</code>.</p> <code>10</code> <code>absolute_must_links</code> <code>bool</code> <p>the option to strictly respect <code>\"MUST_LINK\"</code> type constraints. Defaults to <code>`True</code>.</p> <code>True</code> <code>random_seed</code> <code>Optional[int]</code> <p>The random seed to use to redo the same clustering. Defaults to <code>None</code>.</p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Warns:</p> Type Description <code>FutureWarning</code> <p><code>clustering.affinity_propagation.AffinityPropagationConstrainedClustering</code> is still in development and is not fully tested : it is not ready for production use.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\affinity_propagation.py</code> <pre><code>def __init__(\n    self,\n    max_iteration: int = 150,\n    convergence_iteration: int = 10,\n    random_seed: Optional[int] = None,\n    absolute_must_links: bool = True,\n    **kargs,\n) -&gt; None:\n    \"\"\"\n    The constructor for the Affinity Propagation constrained clustering.\n\n    Args:\n        max_iteration (int, optional): The maximum number of iteration for convergence. Defaults to `150`.\n        convergence_iteration (int, optional): The number of iterations with no change to consider a convergence. Default to `15`.\n        absolute_must_links (bool, optional): the option to strictly respect `\"MUST_LINK\"` type constraints. Defaults to ``True`.\n        random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Warns:\n        FutureWarning: `clustering.affinity_propagation.AffinityPropagationConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set.\n    \"\"\"\n\n    # Deprecation warnings\n    warnings.warn(\n        \"`clustering.affinity_propagation.AffinityPropagationConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\",\n        FutureWarning,  # DeprecationWarning\n        stacklevel=2,\n    )\n\n    # Store 'self.max_iteration`.\n    if max_iteration &lt; 1:\n        raise ValueError(\"The `max_iteration` must be greater than or equal to 1.\")\n    self.max_iteration: int = max_iteration\n\n    # Store 'self.convergence_iteration`.\n    if convergence_iteration &lt; 1:\n        raise ValueError(\"The `convergence_iteration` must be greater than or equal to 1.\")\n    self.convergence_iteration: int = convergence_iteration\n\n    # Store 'self.absolute_must_links`.\n    self.absolute_must_links: bool = absolute_must_links\n\n    # Store 'self.random_seed`.\n    self.random_seed: Optional[int] = random_seed\n\n    # Store `self.kargs` for kmeans clustering.\n    self.kargs = kargs\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/affinity_propagation/#cognitivefactory.interactive_clustering.clustering.affinity_propagation.AffinityPropagationConstrainedClustering.cluster","title":"<code>cluster(constraints_manager, vectors, nb_clusters=None, verbose=False, **kargs)</code>","text":"<p>The main method used to cluster data with the KMeans model.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs that will force clustering to respect some conditions during computation.</p> required <code>vectors</code> <code>Dict[str, csr_matrix]</code> <p>The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data.</p> required <code>nb_clusters</code> <code>Optional[int]</code> <p>The number of clusters to compute. Here <code>None</code>.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose output. Defaults to <code>False</code>.</p> <code>False</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the clustering.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>vectors</code> and <code>constraints_manager</code> are incompatible, or if some parameters are incorrectly set.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\affinity_propagation.py</code> <pre><code>def cluster(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    vectors: Dict[str, csr_matrix],\n    nb_clusters: Optional[int] = None,\n    verbose: bool = False,\n    **kargs,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    The main method used to cluster data with the KMeans model.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n        vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n        nb_clusters (Optional[int]): The number of clusters to compute. Here `None`.\n        verbose (bool, optional): Enable verbose output. Defaults to `False`.\n        **kargs (dict): Other parameters that can be used in the clustering.\n\n    Raises:\n        ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n    Returns:\n        Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n\n    ###\n    ### GET PARAMETERS\n    ###\n\n    # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n    if not isinstance(constraints_manager, AbstractConstraintsManager):\n        raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n    self.constraints_manager: AbstractConstraintsManager = constraints_manager\n    self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n    # Store `self.vectors`.\n    if not isinstance(vectors, dict):\n        raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n    self.vectors: Dict[str, csr_matrix] = vectors\n\n    # Store `self.nb_clusters`.\n    if nb_clusters is not None:\n        raise ValueError(\"The `nb_clusters` should be 'None' for Affinity Propagataion clustering.\")\n    self.nb_clusters: Optional[int] = None\n\n    ###\n    ### RUN AFFINITY PROPAGATION CONSTRAINED CLUSTERING\n    ###\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters = None\n\n    # Correspondances ID -&gt; index\n    data_ID_to_idx: Dict[str, int] = {v: i for i, v in enumerate(self.list_of_data_IDs)}\n    n_sample: int = len(self.list_of_data_IDs)\n\n    # Compute similarity between data points.\n    S: csr_matrix = -pairwise_distances(vstack(self.vectors[data_ID] for data_ID in self.list_of_data_IDs))\n\n    # Get connected components (closures of MUST_LINK contraints).\n    must_link_closures: List[List[str]] = self.constraints_manager.get_connected_components()\n    must_links: List[List[int]] = [[data_ID_to_idx[ID] for ID in closure] for closure in must_link_closures]\n\n    # Get annotated CANNOT_LINK contraints.\n    cannot_links: List[Tuple[int, int]] = []\n    for data_ID_i1, data_ID_j1 in combinations(range(n_sample), 2):\n        constraint = self.constraints_manager.get_added_constraint(\n            self.list_of_data_IDs[data_ID_i1], self.list_of_data_IDs[data_ID_j1]\n        )\n        if constraint and constraint[0] == \"CANNOT_LINK\":\n            cannot_links.append((data_ID_i1, data_ID_j1))\n\n    # Run constrained affinity propagation.\n    cluster_labels: List[int] = _affinity_propagation_constrained(\n        S,\n        must_links=must_links,\n        cannot_links=cannot_links,\n        absolute_must_links=self.absolute_must_links,\n        max_iteration=self.max_iteration,\n        convergence_iteration=self.convergence_iteration,\n        random_seed=self.random_seed,\n        verbose=verbose,\n    )\n\n    # Rename cluster IDs by order.\n    self.dict_of_predicted_clusters = rename_clusters_by_order(\n        {self.list_of_data_IDs[i]: l for i, l in enumerate(cluster_labels)}\n    )\n\n    return self.dict_of_predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/dbscan/","title":"dbscan","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.clustering.dbscan</li> <li>Description:  Implementation of constrained DBScan clustering algorithms.</li> <li>Author:       Marc TRUTT, Esther LENOTRE, David NICOLAZO</li> <li>Created:      08/05/2022</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/dbscan/#cognitivefactory.interactive_clustering.clustering.dbscan.DBScanConstrainedClustering","title":"<code>DBScanConstrainedClustering</code>","text":"<p>             Bases: <code>AbstractConstrainedClustering</code></p> <p>This class implements the DBScan constrained clustering. It inherits from <code>AbstractConstrainedClustering</code>.</p> References <ul> <li>DBScan Clustering: <code>Ester, Martin &amp; Kr\u00f6ger, Peer &amp; Sander, Joerg &amp; Xu, Xiaowei. (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. KDD. 96. 226-231</code>.</li> <li>Constrained DBScan Clustering: <code>Ruiz, Carlos &amp; Spiliopoulou, Myra &amp; Menasalvas, Ernestina. (2007). C-DBSCAN: Density-Based Clustering with Constraints. 216-223. 10.1007/978-3-540-72530-5_25.</code></li> </ul> Example <pre><code># Import.\nfrom scipy.sparse import csr_matrix\nfrom cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\nfrom cognitivefactory.interactive_clustering.clustering.dbscan import DBScanConstrainedClustering\n\n# Create an instance of CDBscan clustering.\nclustering_model = DBScanConstrainedClustering(\n    eps=0.02,\n    min_samples=3,\n)\n\n# Define vectors.\n# NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\nvectors = {\n    \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n    \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n    \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n    \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n    \"4\": csr_matrix([0.50, 0.22, 0.21, 0.07]),\n    \"5\": csr_matrix([0.50, 0.21, 0.22, 0.07]),\n    \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n    \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n    \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n}\n\n# Define constraints manager.\nconstraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"7\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n\n# Run clustering.\ndict_of_predicted_clusters = clustering_model.cluster(\n    constraints_manager=constraints_manager,\n    vectors=vectors,\n    #### nb_clusters=None,\n)\n\n# Print results.\nprint(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 1, \"3\": 1, \"4\": 2, \"5\": 2, \"6\": 0, \"7\": 0, \"8\": 0,})\nprint(\"Computed results\", \":\", dict_of_predicted_clusters)\n</code></pre> <p>Warns:</p> Type Description <code>FutureWarning</code> <p><code>clustering.dbscan.DBScanConstrainedClustering</code> is still in development and is not fully tested : it is not ready for production use.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\dbscan.py</code> <pre><code>class DBScanConstrainedClustering(AbstractConstrainedClustering):\n    \"\"\"\n    This class implements the DBScan constrained clustering.\n    It inherits from `AbstractConstrainedClustering`.\n\n    References:\n        - DBScan Clustering: `Ester, Martin &amp; Kr\u00f6ger, Peer &amp; Sander, Joerg &amp; Xu, Xiaowei. (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. KDD. 96. 226-231`.\n        - Constrained DBScan Clustering: `Ruiz, Carlos &amp; Spiliopoulou, Myra &amp; Menasalvas, Ernestina. (2007). C-DBSCAN: Density-Based Clustering with Constraints. 216-223. 10.1007/978-3-540-72530-5_25.`\n\n    Example:\n        ```python\n        # Import.\n        from scipy.sparse import csr_matrix\n        from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\n        from cognitivefactory.interactive_clustering.clustering.dbscan import DBScanConstrainedClustering\n\n        # Create an instance of CDBscan clustering.\n        clustering_model = DBScanConstrainedClustering(\n            eps=0.02,\n            min_samples=3,\n        )\n\n        # Define vectors.\n        # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\n        vectors = {\n            \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n            \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n            \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n            \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n            \"4\": csr_matrix([0.50, 0.22, 0.21, 0.07]),\n            \"5\": csr_matrix([0.50, 0.21, 0.22, 0.07]),\n            \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n            \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n            \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n        }\n\n        # Define constraints manager.\n        constraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"7\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n\n        # Run clustering.\n        dict_of_predicted_clusters = clustering_model.cluster(\n            constraints_manager=constraints_manager,\n            vectors=vectors,\n            #### nb_clusters=None,\n        )\n\n        # Print results.\n        print(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 1, \"3\": 1, \"4\": 2, \"5\": 2, \"6\": 0, \"7\": 0, \"8\": 0,})\n        print(\"Computed results\", \":\", dict_of_predicted_clusters)\n        ```\n\n    Warns:\n        FutureWarning: `clustering.dbscan.DBScanConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n    \"\"\"\n\n    # ==============================================================================\n    # INITIALIZATION\n    # ==============================================================================\n    def __init__(\n        self,\n        eps: float = 0.5,\n        min_samples: int = 5,\n        random_seed: Optional[int] = None,\n        **kargs,\n    ) -&gt; None:\n        \"\"\"\n        The constructor for DBScan Constrainted Clustering class.\n\n        Args:\n            eps (float): The maximus radius of a neighborhood around its center. Defaults to `0.5`.\n            min_samples (int): The minimum number of points in a neighborhood to consider a center as a core point. Defaults to `5`.\n            random_seed (Optional[int]): The random seed to use to redo the same clustering. Defaults to `None`.\n            **kargs (dict): Other parameters that can be used in the instantiation.\n\n        Warns:\n            FutureWarning: `clustering.dbscan.DBScanConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set.\n        \"\"\"\n\n        # Deprecation warnings\n        warnings.warn(\n            \"`clustering.dbscan.DBScanConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\",\n            FutureWarning,  # DeprecationWarning\n            stacklevel=2,\n        )\n\n        # Store 'self.eps`.\n        if eps &lt;= 0:\n            raise ValueError(\"The `eps` must be greater than 0.\")\n        self.eps: float = eps\n\n        # Store 'self.min_samples`.\n        if min_samples &lt;= 0:\n            raise ValueError(\"The `min_samples` must be greater than or equal to 1.\")\n        self.min_samples: int = min_samples\n\n        # Store `self.random_seed`.\n        self.random_seed: Optional[int] = random_seed\n\n        # Store `self.kargs` for kmeans clustering.\n        self.kargs = kargs\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n\n        # Initialize number of clusters attributes.\n        self.number_of_single_noise_point_clusters: int = 0\n        self.number_of_regular_clusters: int = 0\n        self.number_of_clusters: int = 0\n\n    # ==============================================================================\n    # MAIN - CLUSTER DATA\n    # ==============================================================================\n    def cluster(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        vectors: Dict[str, csr_matrix],\n        nb_clusters: Optional[int] = None,\n        verbose: bool = False,\n        **kargs,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        The main method used to cluster data with the DBScan model.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n            vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n            nb_clusters (Optional[int]): The number of clusters to compute. Here `None`.\n            verbose (bool, optional): Enable verbose output. Defaults to `False`.\n            **kargs (dict): Other parameters that can be used in the clustering.\n\n        Raises:\n            ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        ###\n        ### GET PARAMETERS\n        ###\n\n        # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n        if not isinstance(constraints_manager, AbstractConstraintsManager):\n            raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n        self.constraints_manager: AbstractConstraintsManager = constraints_manager\n        self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n        # Store `self.vectors`.\n        if not isinstance(vectors, dict):\n            raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n        self.vectors: Dict[str, csr_matrix] = vectors\n\n        # Store `self.nb_clusters`.\n        if nb_clusters is not None:\n            raise ValueError(\"The `nb_clusters` should be 'None' for DBScan clustering.\")\n        self.nb_clusters: Optional[int] = None\n\n        ###\n        ### COMPUTE DISTANCE\n        ###\n\n        # Compute pairwise distances.\n        matrix_of_pairwise_distances: csr_matrix = pairwise_distances(\n            X=vstack(self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()),\n            metric=\"euclidean\",  # TODO get different pairwise_distances config in **kargs\n        )\n\n        # Format pairwise distances in a dictionary and store `self.dict_of_pairwise_distances`.\n        self.dict_of_pairwise_distances: Dict[str, Dict[str, float]] = {\n            vector_ID1: {\n                vector_ID2: float(matrix_of_pairwise_distances[i1, i2])\n                for i2, vector_ID2 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n            }\n            for i1, vector_ID1 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n        }\n\n        ###\n        ### INITIALIZE VARIABLES\n        ###\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters = {}\n\n        # To assign \"CORE\", \"SINGLE_CORE\" or \"NOISE\" labels to the points\n        self.dict_of_data_IDs_labels: Dict[str, str] = {data_ID: \"UNLABELED\" for data_ID in self.list_of_data_IDs}\n\n        # To store the lists of points of each computed local cluster\n        self.dict_of_local_clusters: Dict[str, List[str]] = {}\n\n        # To store the lists of points of each computed core local cluster\n        self.dict_of_core_local_clusters: Dict[str, List[str]] = {data_ID: [] for data_ID in self.list_of_data_IDs}\n\n        ###\n        ### CREATE LOCAL CLUSTERS\n        ###\n\n        for possible_core_ID in self.list_of_data_IDs:\n            if self.dict_of_data_IDs_labels[possible_core_ID] != \"SINGLE_CORE\":\n                # Points involved in a Cannot-link constraint are not associated to other points in this step\n                list_of_possible_neighbors: List[str] = [\n                    neighbor_ID\n                    for neighbor_ID in self.list_of_data_IDs\n                    if self.dict_of_data_IDs_labels[neighbor_ID] != \"SINGLE_CORE\"\n                ]\n\n                # Compute distances to other possible neighbors\n                distances_to_possible_neighbors: Dict[str, float] = {\n                    neighbor_ID: self.dict_of_pairwise_distances[possible_core_ID][neighbor_ID]\n                    for neighbor_ID in list_of_possible_neighbors\n                }\n\n                # Keep only points within the radius of eps as neighbors\n                list_of_neighbors_ID: List[str] = [\n                    neighbor_ID\n                    for neighbor_ID in list_of_possible_neighbors\n                    if distances_to_possible_neighbors[neighbor_ID] &lt;= self.eps\n                ]\n\n                # Get the lists of not compatible data_IDs for deciding if the points are separated in different clusters\n                not_compatible_cluster_IDs: List[List[str]] = [\n                    [\n                        data_ID_i\n                        for data_ID_i in list_of_neighbors_ID\n                        if (\n                            self.constraints_manager.get_inferred_constraint(\n                                data_ID1=data_ID_j,\n                                data_ID2=data_ID_i,\n                            )\n                            == \"CANNOT_LINK\"\n                        )\n                    ]\n                    for data_ID_j in list_of_neighbors_ID\n                ]\n\n                # Check if there is a Cannot-link constraint between points in the neighborhood\n                no_conflict = True\n                for neighborhood_not_compatible_IDs in not_compatible_cluster_IDs:\n                    if neighborhood_not_compatible_IDs:\n                        no_conflict = False\n                        break\n\n                if len(list_of_neighbors_ID) &lt; self.min_samples:\n                    self.dict_of_data_IDs_labels[possible_core_ID] = \"NOISE\"\n\n                elif no_conflict is False:\n                    for neighbor_ID in list_of_neighbors_ID:\n                        # Each point of the neighborhood will be a single core point cluster\n                        # and won't be involved in other clusters in this step\n\n                        self.dict_of_data_IDs_labels[neighbor_ID] = \"SINGLE_CORE\"\n                        self.dict_of_local_clusters[neighbor_ID] = [neighbor_ID]\n\n                else:\n                    self.dict_of_data_IDs_labels[possible_core_ID] = \"CORE\"\n                    self.dict_of_local_clusters[possible_core_ID] = list_of_neighbors_ID\n\n        ###\n        ### MERGE LOCAL CLUSTERS UNDER MUST-LINK CONSTRAINTS\n        ###\n\n        # Get the lists of data_IDs for which each point is in a Must-link constraint\n        compatible_cluster_IDs: Dict[str, List[str]] = {\n            data_ID_j: [\n                data_ID_i\n                for data_ID_i in self.list_of_data_IDs\n                if (\n                    self.constraints_manager.get_inferred_constraint(\n                        data_ID1=data_ID_j,\n                        data_ID2=data_ID_i,\n                    )\n                    == \"MUST_LINK\"\n                )\n            ]\n            for data_ID_j in self.list_of_data_IDs\n        }\n\n        # Get the lists of local clusters where each point is in\n        clusters_of_data_IDs: Dict[str, List[str]] = {\n            data_ID_j: [\n                cluster_id\n                for cluster_id in self.dict_of_local_clusters.keys()\n                if (data_ID_j in self.dict_of_local_clusters[cluster_id])\n            ]\n            for data_ID_j in self.list_of_data_IDs\n        }\n\n        # Initialize a variable in order to analyze a point Must-link constraints only once\n        list_of_analyzed_IDs: List[str] = []\n\n        # Initialize a variable in order not to take one point into account in several core local clusters\n        dict_of_assigned_local_cluster: Dict[str, str] = {data_ID: \"NONE\" for data_ID in self.list_of_data_IDs}\n\n        for data_ID_i in self.list_of_data_IDs:\n            if data_ID_i not in list_of_analyzed_IDs:\n                if compatible_cluster_IDs[data_ID_i]:\n                    # Choose a coherent ID of core local cluster corresponding to a local cluster ID of data_ID_i\n\n                    # Initialize ID of the potential local cluster of data_ID_i and list of involved points\n                    local_cluster_i_points: List[str] = []\n\n                    if self.dict_of_data_IDs_labels[data_ID_i] == \"NOISE\":\n                        data_ID_i_cluster = data_ID_i\n                        local_cluster_i_points = [data_ID_i]\n\n                    elif data_ID_i in self.dict_of_local_clusters.keys():\n                        data_ID_i_cluster = data_ID_i\n                        local_cluster_i_points = self.dict_of_local_clusters[data_ID_i]\n\n                    else:\n                        # Choose a local cluster ID where data_ID_i is in,\n                        # and preferably a local cluster ID that is not already in a core local cluster\n\n                        data_ID_i_cluster = clusters_of_data_IDs[data_ID_i][0]\n                        for cluster_i_id in clusters_of_data_IDs[data_ID_i]:\n                            if dict_of_assigned_local_cluster[data_ID_i] == \"NONE\":\n                                data_ID_i_cluster = cluster_i_id\n                                break\n                        local_cluster_i_points = self.dict_of_local_clusters[data_ID_i_cluster]\n\n                    for data_ID_j in compatible_cluster_IDs[data_ID_i]:\n                        if self.dict_of_data_IDs_labels[data_ID_j] == \"NOISE\":\n                            # Merge all the available points of the clusters involved in a Must-link constraint\n\n                            list_of_core_cluster_points = []\n                            for data_ID_k in local_cluster_i_points:\n                                if dict_of_assigned_local_cluster[data_ID_k] == \"NONE\":\n                                    list_of_core_cluster_points.append(data_ID_k)\n                                    dict_of_assigned_local_cluster[data_ID_k] = data_ID_i_cluster\n\n                            self.dict_of_core_local_clusters[data_ID_i_cluster] = list(\n                                set(\n                                    self.dict_of_core_local_clusters[data_ID_i_cluster]\n                                    + list_of_core_cluster_points\n                                    + [data_ID_i, data_ID_j]\n                                )\n                            )\n                        else:\n                            # Initialize ID of the potential local cluster of data_ID_j and the list of involved points\n                            local_cluster_j_points = []\n\n                            if data_ID_j in self.dict_of_local_clusters.keys():\n                                local_cluster_j_points = [data_ID_j]\n\n                            else:\n                                # Choose a local cluster ID where data_ID_j is in,\n                                # and preferably a local cluster ID that is not already in a core local cluster\n\n                                data_ID_j_cluster = clusters_of_data_IDs[data_ID_j][0]\n                                for cluster_j_id in clusters_of_data_IDs[data_ID_j]:\n                                    if dict_of_assigned_local_cluster[data_ID_j] == \"NONE\":\n                                        data_ID_j_cluster = cluster_j_id\n                                        break\n                                local_cluster_j_points = self.dict_of_local_clusters[data_ID_j_cluster]\n\n                            # Merge all the available points of the clusters involved in a Must-link constraint\n\n                            list_of_core_cluster_points = []\n                            for data_ID_l in list(set(local_cluster_i_points + local_cluster_j_points)):\n                                if dict_of_assigned_local_cluster[data_ID_l] == \"NONE\":\n                                    list_of_core_cluster_points.append(data_ID_l)\n                                    dict_of_assigned_local_cluster[data_ID_l] = data_ID_i_cluster\n\n                            self.dict_of_core_local_clusters[data_ID_i_cluster] = list(\n                                set(\n                                    self.dict_of_core_local_clusters[data_ID_i_cluster]\n                                    + list_of_core_cluster_points\n                                    + [data_ID_i, data_ID_j]\n                                )\n                            )\n\n                # Mark the current point as analyzed in order not to have it in two clusters\n                list_of_analyzed_IDs.append(data_ID_i)\n\n        # Clean the `dict_of_core_local_clusters` variable\n        for data_ID in self.list_of_data_IDs:\n            if not self.dict_of_core_local_clusters[data_ID]:\n                # Clean by deleting non-existing core local clusters entries\n                self.dict_of_core_local_clusters.pop(data_ID)\n            elif dict_of_assigned_local_cluster[data_ID] != data_ID:\n                # Clean by deleting core local clusters entries corresponding to another already created core cluster\n                self.dict_of_core_local_clusters.pop(data_ID)\n\n        # Clean the `dict_of_core_local_clusters` variable by removing single-point clusters\n        # because don't make sense in a Must-link constraint\n        for potential_single_data_ID in self.list_of_data_IDs:\n            if (\n                potential_single_data_ID in self.dict_of_core_local_clusters.keys()\n                and len(self.dict_of_core_local_clusters[potential_single_data_ID]) &lt; 2\n            ):\n                self.dict_of_core_local_clusters.pop(potential_single_data_ID)\n\n        ###\n        ### MERGE LOCAL CLUSTERS UNDER CANNOT-LINK CONSTRAINTS\n        ###\n\n        for core_cluster_ID in self.dict_of_core_local_clusters.keys():\n            merging = True\n\n            while merging and self.dict_of_local_clusters:\n                # While there is no conflict and there is still local clusters\n\n                distances_to_local_clusters: Dict[str, float] = {}\n\n                # Compute the distances between the core cluster and the local clusters\n                for local_cluster_ID in self.dict_of_local_clusters.keys():\n                    # Compute the smallest distance between points of the core cluster and the local cluster\n                    distances_to_local_clusters[local_cluster_ID] = min(\n                        [\n                            self.dict_of_pairwise_distances[core_cluster_pt][local_cluster_pt]\n                            for core_cluster_pt in self.dict_of_core_local_clusters[core_cluster_ID]\n                            for local_cluster_pt in self.dict_of_local_clusters[local_cluster_ID]\n                        ]\n                    )\n\n                # Find closest local cluster to core cluster\n                closest_cluster = min(\n                    distances_to_local_clusters\n                )  # TODO: min(distances_to_local_clusters, key=lambda x: distances_to_local_clusters[x])\n\n                if distances_to_local_clusters[closest_cluster] &gt; self.eps:\n                    merging = False\n\n                else:\n                    # Get the lists of not compatible data_IDs for deciding if clusters are merged\n                    not_compatible_IDs: List[List[str]] = [\n                        [\n                            data_ID_m\n                            for data_ID_m in self.dict_of_local_clusters[closest_cluster]\n                            if (\n                                self.constraints_manager.get_inferred_constraint(\n                                    data_ID1=data_ID_n,\n                                    data_ID2=data_ID_m,\n                                )\n                                == \"CANNOT_LINK\"\n                            )\n                        ]\n                        for data_ID_n in self.dict_of_core_local_clusters[core_cluster_ID]\n                    ]\n\n                    # Check if there is a Cannot-link constraint between the points\n                    no_conflict = True\n                    for core_local_cluster_not_compatible_IDs in not_compatible_IDs:\n                        if core_local_cluster_not_compatible_IDs:\n                            no_conflict = False\n                            break\n\n                    if no_conflict:\n                        # Merge core local cluster and its closest local cluster\n                        self.dict_of_core_local_clusters[core_cluster_ID] = list(\n                            set(\n                                self.dict_of_core_local_clusters[core_cluster_ID]\n                                + self.dict_of_local_clusters[closest_cluster]\n                            )\n                        )\n\n                        self.dict_of_local_clusters.pop(closest_cluster)\n\n                    else:\n                        merging = False\n\n        ###\n        ### DEFINING FINAL CLUSTERS\n        ###\n\n        # Consider the final core local clusters\n        assigned_cluster_id: int = 0\n        for core_cluster in self.dict_of_core_local_clusters.keys():\n            for cluster_point in self.dict_of_core_local_clusters[core_cluster]:\n                self.dict_of_predicted_clusters[cluster_point] = assigned_cluster_id\n            assigned_cluster_id += 1\n\n        # Consider the remaining local clusters\n        for local_cluster in self.dict_of_local_clusters.keys():\n            # Remove points that already are in a final cluster\n            points_to_remove = []\n            for local_cluster_point in self.dict_of_local_clusters[local_cluster]:\n                if local_cluster_point in self.dict_of_predicted_clusters.keys():\n                    points_to_remove.append(local_cluster_point)\n            for data_ID_to_remove in points_to_remove:\n                self.dict_of_local_clusters[local_cluster].remove(data_ID_to_remove)\n\n            # Check that the local cluster is still big enough\n            if len(self.dict_of_local_clusters[local_cluster]) &gt;= self.eps:\n                for core_cluster_point in self.dict_of_local_clusters[local_cluster]:\n                    self.dict_of_predicted_clusters[core_cluster_point] = assigned_cluster_id\n                assigned_cluster_id += 1\n\n        # Rename clusters\n        self.dict_of_predicted_clusters = rename_clusters_by_order(\n            clusters=self.dict_of_predicted_clusters,\n        )\n\n        # Set number of regular clusters\n        self.number_of_regular_clusters = np.unique(np.array(list(self.dict_of_predicted_clusters.values()))).shape[0]\n\n        # Consider ignored points\n        ignored_cluster_id: int = -1\n        for potential_ignored_point in self.list_of_data_IDs:\n            if potential_ignored_point not in self.dict_of_predicted_clusters:\n                self.dict_of_predicted_clusters[potential_ignored_point] = ignored_cluster_id\n                ignored_cluster_id -= 1\n\n        # Set number of single ignored points cluster\n        self.number_of_single_noise_point_clusters = -(ignored_cluster_id + 1)\n\n        # Set total number of clusters\n        self.number_of_clusters = self.number_of_regular_clusters + self.number_of_single_noise_point_clusters\n\n        return self.dict_of_predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/dbscan/#cognitivefactory.interactive_clustering.clustering.dbscan.DBScanConstrainedClustering.__init__","title":"<code>__init__(eps=0.5, min_samples=5, random_seed=None, **kargs)</code>","text":"<p>The constructor for DBScan Constrainted Clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>The maximus radius of a neighborhood around its center. Defaults to <code>0.5</code>.</p> <code>0.5</code> <code>min_samples</code> <code>int</code> <p>The minimum number of points in a neighborhood to consider a center as a core point. Defaults to <code>5</code>.</p> <code>5</code> <code>random_seed</code> <code>Optional[int]</code> <p>The random seed to use to redo the same clustering. Defaults to <code>None</code>.</p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Warns:</p> Type Description <code>FutureWarning</code> <p><code>clustering.dbscan.DBScanConstrainedClustering</code> is still in development and is not fully tested : it is not ready for production use.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\dbscan.py</code> <pre><code>def __init__(\n    self,\n    eps: float = 0.5,\n    min_samples: int = 5,\n    random_seed: Optional[int] = None,\n    **kargs,\n) -&gt; None:\n    \"\"\"\n    The constructor for DBScan Constrainted Clustering class.\n\n    Args:\n        eps (float): The maximus radius of a neighborhood around its center. Defaults to `0.5`.\n        min_samples (int): The minimum number of points in a neighborhood to consider a center as a core point. Defaults to `5`.\n        random_seed (Optional[int]): The random seed to use to redo the same clustering. Defaults to `None`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Warns:\n        FutureWarning: `clustering.dbscan.DBScanConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set.\n    \"\"\"\n\n    # Deprecation warnings\n    warnings.warn(\n        \"`clustering.dbscan.DBScanConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\",\n        FutureWarning,  # DeprecationWarning\n        stacklevel=2,\n    )\n\n    # Store 'self.eps`.\n    if eps &lt;= 0:\n        raise ValueError(\"The `eps` must be greater than 0.\")\n    self.eps: float = eps\n\n    # Store 'self.min_samples`.\n    if min_samples &lt;= 0:\n        raise ValueError(\"The `min_samples` must be greater than or equal to 1.\")\n    self.min_samples: int = min_samples\n\n    # Store `self.random_seed`.\n    self.random_seed: Optional[int] = random_seed\n\n    # Store `self.kargs` for kmeans clustering.\n    self.kargs = kargs\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n\n    # Initialize number of clusters attributes.\n    self.number_of_single_noise_point_clusters: int = 0\n    self.number_of_regular_clusters: int = 0\n    self.number_of_clusters: int = 0\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/dbscan/#cognitivefactory.interactive_clustering.clustering.dbscan.DBScanConstrainedClustering.cluster","title":"<code>cluster(constraints_manager, vectors, nb_clusters=None, verbose=False, **kargs)</code>","text":"<p>The main method used to cluster data with the DBScan model.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs that will force clustering to respect some conditions during computation.</p> required <code>vectors</code> <code>Dict[str, csr_matrix]</code> <p>The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data.</p> required <code>nb_clusters</code> <code>Optional[int]</code> <p>The number of clusters to compute. Here <code>None</code>.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose output. Defaults to <code>False</code>.</p> <code>False</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the clustering.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>vectors</code> and <code>constraints_manager</code> are incompatible, or if some parameters are incorrectly set.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\dbscan.py</code> <pre><code>def cluster(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    vectors: Dict[str, csr_matrix],\n    nb_clusters: Optional[int] = None,\n    verbose: bool = False,\n    **kargs,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    The main method used to cluster data with the DBScan model.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n        vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n        nb_clusters (Optional[int]): The number of clusters to compute. Here `None`.\n        verbose (bool, optional): Enable verbose output. Defaults to `False`.\n        **kargs (dict): Other parameters that can be used in the clustering.\n\n    Raises:\n        ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n    Returns:\n        Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n\n    ###\n    ### GET PARAMETERS\n    ###\n\n    # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n    if not isinstance(constraints_manager, AbstractConstraintsManager):\n        raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n    self.constraints_manager: AbstractConstraintsManager = constraints_manager\n    self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n    # Store `self.vectors`.\n    if not isinstance(vectors, dict):\n        raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n    self.vectors: Dict[str, csr_matrix] = vectors\n\n    # Store `self.nb_clusters`.\n    if nb_clusters is not None:\n        raise ValueError(\"The `nb_clusters` should be 'None' for DBScan clustering.\")\n    self.nb_clusters: Optional[int] = None\n\n    ###\n    ### COMPUTE DISTANCE\n    ###\n\n    # Compute pairwise distances.\n    matrix_of_pairwise_distances: csr_matrix = pairwise_distances(\n        X=vstack(self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()),\n        metric=\"euclidean\",  # TODO get different pairwise_distances config in **kargs\n    )\n\n    # Format pairwise distances in a dictionary and store `self.dict_of_pairwise_distances`.\n    self.dict_of_pairwise_distances: Dict[str, Dict[str, float]] = {\n        vector_ID1: {\n            vector_ID2: float(matrix_of_pairwise_distances[i1, i2])\n            for i2, vector_ID2 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n        }\n        for i1, vector_ID1 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n    }\n\n    ###\n    ### INITIALIZE VARIABLES\n    ###\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters = {}\n\n    # To assign \"CORE\", \"SINGLE_CORE\" or \"NOISE\" labels to the points\n    self.dict_of_data_IDs_labels: Dict[str, str] = {data_ID: \"UNLABELED\" for data_ID in self.list_of_data_IDs}\n\n    # To store the lists of points of each computed local cluster\n    self.dict_of_local_clusters: Dict[str, List[str]] = {}\n\n    # To store the lists of points of each computed core local cluster\n    self.dict_of_core_local_clusters: Dict[str, List[str]] = {data_ID: [] for data_ID in self.list_of_data_IDs}\n\n    ###\n    ### CREATE LOCAL CLUSTERS\n    ###\n\n    for possible_core_ID in self.list_of_data_IDs:\n        if self.dict_of_data_IDs_labels[possible_core_ID] != \"SINGLE_CORE\":\n            # Points involved in a Cannot-link constraint are not associated to other points in this step\n            list_of_possible_neighbors: List[str] = [\n                neighbor_ID\n                for neighbor_ID in self.list_of_data_IDs\n                if self.dict_of_data_IDs_labels[neighbor_ID] != \"SINGLE_CORE\"\n            ]\n\n            # Compute distances to other possible neighbors\n            distances_to_possible_neighbors: Dict[str, float] = {\n                neighbor_ID: self.dict_of_pairwise_distances[possible_core_ID][neighbor_ID]\n                for neighbor_ID in list_of_possible_neighbors\n            }\n\n            # Keep only points within the radius of eps as neighbors\n            list_of_neighbors_ID: List[str] = [\n                neighbor_ID\n                for neighbor_ID in list_of_possible_neighbors\n                if distances_to_possible_neighbors[neighbor_ID] &lt;= self.eps\n            ]\n\n            # Get the lists of not compatible data_IDs for deciding if the points are separated in different clusters\n            not_compatible_cluster_IDs: List[List[str]] = [\n                [\n                    data_ID_i\n                    for data_ID_i in list_of_neighbors_ID\n                    if (\n                        self.constraints_manager.get_inferred_constraint(\n                            data_ID1=data_ID_j,\n                            data_ID2=data_ID_i,\n                        )\n                        == \"CANNOT_LINK\"\n                    )\n                ]\n                for data_ID_j in list_of_neighbors_ID\n            ]\n\n            # Check if there is a Cannot-link constraint between points in the neighborhood\n            no_conflict = True\n            for neighborhood_not_compatible_IDs in not_compatible_cluster_IDs:\n                if neighborhood_not_compatible_IDs:\n                    no_conflict = False\n                    break\n\n            if len(list_of_neighbors_ID) &lt; self.min_samples:\n                self.dict_of_data_IDs_labels[possible_core_ID] = \"NOISE\"\n\n            elif no_conflict is False:\n                for neighbor_ID in list_of_neighbors_ID:\n                    # Each point of the neighborhood will be a single core point cluster\n                    # and won't be involved in other clusters in this step\n\n                    self.dict_of_data_IDs_labels[neighbor_ID] = \"SINGLE_CORE\"\n                    self.dict_of_local_clusters[neighbor_ID] = [neighbor_ID]\n\n            else:\n                self.dict_of_data_IDs_labels[possible_core_ID] = \"CORE\"\n                self.dict_of_local_clusters[possible_core_ID] = list_of_neighbors_ID\n\n    ###\n    ### MERGE LOCAL CLUSTERS UNDER MUST-LINK CONSTRAINTS\n    ###\n\n    # Get the lists of data_IDs for which each point is in a Must-link constraint\n    compatible_cluster_IDs: Dict[str, List[str]] = {\n        data_ID_j: [\n            data_ID_i\n            for data_ID_i in self.list_of_data_IDs\n            if (\n                self.constraints_manager.get_inferred_constraint(\n                    data_ID1=data_ID_j,\n                    data_ID2=data_ID_i,\n                )\n                == \"MUST_LINK\"\n            )\n        ]\n        for data_ID_j in self.list_of_data_IDs\n    }\n\n    # Get the lists of local clusters where each point is in\n    clusters_of_data_IDs: Dict[str, List[str]] = {\n        data_ID_j: [\n            cluster_id\n            for cluster_id in self.dict_of_local_clusters.keys()\n            if (data_ID_j in self.dict_of_local_clusters[cluster_id])\n        ]\n        for data_ID_j in self.list_of_data_IDs\n    }\n\n    # Initialize a variable in order to analyze a point Must-link constraints only once\n    list_of_analyzed_IDs: List[str] = []\n\n    # Initialize a variable in order not to take one point into account in several core local clusters\n    dict_of_assigned_local_cluster: Dict[str, str] = {data_ID: \"NONE\" for data_ID in self.list_of_data_IDs}\n\n    for data_ID_i in self.list_of_data_IDs:\n        if data_ID_i not in list_of_analyzed_IDs:\n            if compatible_cluster_IDs[data_ID_i]:\n                # Choose a coherent ID of core local cluster corresponding to a local cluster ID of data_ID_i\n\n                # Initialize ID of the potential local cluster of data_ID_i and list of involved points\n                local_cluster_i_points: List[str] = []\n\n                if self.dict_of_data_IDs_labels[data_ID_i] == \"NOISE\":\n                    data_ID_i_cluster = data_ID_i\n                    local_cluster_i_points = [data_ID_i]\n\n                elif data_ID_i in self.dict_of_local_clusters.keys():\n                    data_ID_i_cluster = data_ID_i\n                    local_cluster_i_points = self.dict_of_local_clusters[data_ID_i]\n\n                else:\n                    # Choose a local cluster ID where data_ID_i is in,\n                    # and preferably a local cluster ID that is not already in a core local cluster\n\n                    data_ID_i_cluster = clusters_of_data_IDs[data_ID_i][0]\n                    for cluster_i_id in clusters_of_data_IDs[data_ID_i]:\n                        if dict_of_assigned_local_cluster[data_ID_i] == \"NONE\":\n                            data_ID_i_cluster = cluster_i_id\n                            break\n                    local_cluster_i_points = self.dict_of_local_clusters[data_ID_i_cluster]\n\n                for data_ID_j in compatible_cluster_IDs[data_ID_i]:\n                    if self.dict_of_data_IDs_labels[data_ID_j] == \"NOISE\":\n                        # Merge all the available points of the clusters involved in a Must-link constraint\n\n                        list_of_core_cluster_points = []\n                        for data_ID_k in local_cluster_i_points:\n                            if dict_of_assigned_local_cluster[data_ID_k] == \"NONE\":\n                                list_of_core_cluster_points.append(data_ID_k)\n                                dict_of_assigned_local_cluster[data_ID_k] = data_ID_i_cluster\n\n                        self.dict_of_core_local_clusters[data_ID_i_cluster] = list(\n                            set(\n                                self.dict_of_core_local_clusters[data_ID_i_cluster]\n                                + list_of_core_cluster_points\n                                + [data_ID_i, data_ID_j]\n                            )\n                        )\n                    else:\n                        # Initialize ID of the potential local cluster of data_ID_j and the list of involved points\n                        local_cluster_j_points = []\n\n                        if data_ID_j in self.dict_of_local_clusters.keys():\n                            local_cluster_j_points = [data_ID_j]\n\n                        else:\n                            # Choose a local cluster ID where data_ID_j is in,\n                            # and preferably a local cluster ID that is not already in a core local cluster\n\n                            data_ID_j_cluster = clusters_of_data_IDs[data_ID_j][0]\n                            for cluster_j_id in clusters_of_data_IDs[data_ID_j]:\n                                if dict_of_assigned_local_cluster[data_ID_j] == \"NONE\":\n                                    data_ID_j_cluster = cluster_j_id\n                                    break\n                            local_cluster_j_points = self.dict_of_local_clusters[data_ID_j_cluster]\n\n                        # Merge all the available points of the clusters involved in a Must-link constraint\n\n                        list_of_core_cluster_points = []\n                        for data_ID_l in list(set(local_cluster_i_points + local_cluster_j_points)):\n                            if dict_of_assigned_local_cluster[data_ID_l] == \"NONE\":\n                                list_of_core_cluster_points.append(data_ID_l)\n                                dict_of_assigned_local_cluster[data_ID_l] = data_ID_i_cluster\n\n                        self.dict_of_core_local_clusters[data_ID_i_cluster] = list(\n                            set(\n                                self.dict_of_core_local_clusters[data_ID_i_cluster]\n                                + list_of_core_cluster_points\n                                + [data_ID_i, data_ID_j]\n                            )\n                        )\n\n            # Mark the current point as analyzed in order not to have it in two clusters\n            list_of_analyzed_IDs.append(data_ID_i)\n\n    # Clean the `dict_of_core_local_clusters` variable\n    for data_ID in self.list_of_data_IDs:\n        if not self.dict_of_core_local_clusters[data_ID]:\n            # Clean by deleting non-existing core local clusters entries\n            self.dict_of_core_local_clusters.pop(data_ID)\n        elif dict_of_assigned_local_cluster[data_ID] != data_ID:\n            # Clean by deleting core local clusters entries corresponding to another already created core cluster\n            self.dict_of_core_local_clusters.pop(data_ID)\n\n    # Clean the `dict_of_core_local_clusters` variable by removing single-point clusters\n    # because don't make sense in a Must-link constraint\n    for potential_single_data_ID in self.list_of_data_IDs:\n        if (\n            potential_single_data_ID in self.dict_of_core_local_clusters.keys()\n            and len(self.dict_of_core_local_clusters[potential_single_data_ID]) &lt; 2\n        ):\n            self.dict_of_core_local_clusters.pop(potential_single_data_ID)\n\n    ###\n    ### MERGE LOCAL CLUSTERS UNDER CANNOT-LINK CONSTRAINTS\n    ###\n\n    for core_cluster_ID in self.dict_of_core_local_clusters.keys():\n        merging = True\n\n        while merging and self.dict_of_local_clusters:\n            # While there is no conflict and there is still local clusters\n\n            distances_to_local_clusters: Dict[str, float] = {}\n\n            # Compute the distances between the core cluster and the local clusters\n            for local_cluster_ID in self.dict_of_local_clusters.keys():\n                # Compute the smallest distance between points of the core cluster and the local cluster\n                distances_to_local_clusters[local_cluster_ID] = min(\n                    [\n                        self.dict_of_pairwise_distances[core_cluster_pt][local_cluster_pt]\n                        for core_cluster_pt in self.dict_of_core_local_clusters[core_cluster_ID]\n                        for local_cluster_pt in self.dict_of_local_clusters[local_cluster_ID]\n                    ]\n                )\n\n            # Find closest local cluster to core cluster\n            closest_cluster = min(\n                distances_to_local_clusters\n            )  # TODO: min(distances_to_local_clusters, key=lambda x: distances_to_local_clusters[x])\n\n            if distances_to_local_clusters[closest_cluster] &gt; self.eps:\n                merging = False\n\n            else:\n                # Get the lists of not compatible data_IDs for deciding if clusters are merged\n                not_compatible_IDs: List[List[str]] = [\n                    [\n                        data_ID_m\n                        for data_ID_m in self.dict_of_local_clusters[closest_cluster]\n                        if (\n                            self.constraints_manager.get_inferred_constraint(\n                                data_ID1=data_ID_n,\n                                data_ID2=data_ID_m,\n                            )\n                            == \"CANNOT_LINK\"\n                        )\n                    ]\n                    for data_ID_n in self.dict_of_core_local_clusters[core_cluster_ID]\n                ]\n\n                # Check if there is a Cannot-link constraint between the points\n                no_conflict = True\n                for core_local_cluster_not_compatible_IDs in not_compatible_IDs:\n                    if core_local_cluster_not_compatible_IDs:\n                        no_conflict = False\n                        break\n\n                if no_conflict:\n                    # Merge core local cluster and its closest local cluster\n                    self.dict_of_core_local_clusters[core_cluster_ID] = list(\n                        set(\n                            self.dict_of_core_local_clusters[core_cluster_ID]\n                            + self.dict_of_local_clusters[closest_cluster]\n                        )\n                    )\n\n                    self.dict_of_local_clusters.pop(closest_cluster)\n\n                else:\n                    merging = False\n\n    ###\n    ### DEFINING FINAL CLUSTERS\n    ###\n\n    # Consider the final core local clusters\n    assigned_cluster_id: int = 0\n    for core_cluster in self.dict_of_core_local_clusters.keys():\n        for cluster_point in self.dict_of_core_local_clusters[core_cluster]:\n            self.dict_of_predicted_clusters[cluster_point] = assigned_cluster_id\n        assigned_cluster_id += 1\n\n    # Consider the remaining local clusters\n    for local_cluster in self.dict_of_local_clusters.keys():\n        # Remove points that already are in a final cluster\n        points_to_remove = []\n        for local_cluster_point in self.dict_of_local_clusters[local_cluster]:\n            if local_cluster_point in self.dict_of_predicted_clusters.keys():\n                points_to_remove.append(local_cluster_point)\n        for data_ID_to_remove in points_to_remove:\n            self.dict_of_local_clusters[local_cluster].remove(data_ID_to_remove)\n\n        # Check that the local cluster is still big enough\n        if len(self.dict_of_local_clusters[local_cluster]) &gt;= self.eps:\n            for core_cluster_point in self.dict_of_local_clusters[local_cluster]:\n                self.dict_of_predicted_clusters[core_cluster_point] = assigned_cluster_id\n            assigned_cluster_id += 1\n\n    # Rename clusters\n    self.dict_of_predicted_clusters = rename_clusters_by_order(\n        clusters=self.dict_of_predicted_clusters,\n    )\n\n    # Set number of regular clusters\n    self.number_of_regular_clusters = np.unique(np.array(list(self.dict_of_predicted_clusters.values()))).shape[0]\n\n    # Consider ignored points\n    ignored_cluster_id: int = -1\n    for potential_ignored_point in self.list_of_data_IDs:\n        if potential_ignored_point not in self.dict_of_predicted_clusters:\n            self.dict_of_predicted_clusters[potential_ignored_point] = ignored_cluster_id\n            ignored_cluster_id -= 1\n\n    # Set number of single ignored points cluster\n    self.number_of_single_noise_point_clusters = -(ignored_cluster_id + 1)\n\n    # Set total number of clusters\n    self.number_of_clusters = self.number_of_regular_clusters + self.number_of_single_noise_point_clusters\n\n    return self.dict_of_predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/factory/","title":"factory","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.clustering.factory</li> <li>Description:  The factory method used to easily initialize a constrained clustering algorithm.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/factory/#cognitivefactory.interactive_clustering.clustering.factory.clustering_factory","title":"<code>clustering_factory(algorithm='kmeans', **kargs)</code>","text":"<p>A factory to create a new instance of a constrained clustering model.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>str</code> <p>The identification of model to instantiate. Can be <code>\"affinity_propagation\"</code>, <code>\"dbscan\"</code>, <code>\"hierarchical\"</code>, <code>\"kmeans\"</code>, <code>\"mpckmeans\"</code> or <code>\"spectral\"</code>. Defaults to <code>\"kmeans\"</code>.</p> <code>'kmeans'</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Warns:</p> Type Description <code>FutureWarning</code> <p><code>clustering.affinity_propagation.AffinityPropagationConstrainedClustering</code>, <code>clustering.dbscan.DBScanConstrainedClustering</code> and <code>clustering.mpckmeans.MPCKMeansConstrainedClustering</code> are still in development and are not fully tested : it is not ready for production use.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>algorithm</code> is not implemented.</p> <p>Returns:</p> Name Type Description <code>AbstractConstraintsClustering</code> <code>AbstractConstrainedClustering</code> <p>An instance of constrained clustering model.</p> Example <pre><code># Import.\nfrom cognitivefactory.interactive_clustering.clustering.factory import clustering_factory\n\n# Create an instance of kmeans.\nclustering_model = clustering_factory(\n    algorithm=\"kmeans\",\n    model=\"COP\",\n    random_seed=42,\n)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\factory.py</code> <pre><code>def clustering_factory(algorithm: str = \"kmeans\", **kargs) -&gt; \"AbstractConstrainedClustering\":\n    \"\"\"\n    A factory to create a new instance of a constrained clustering model.\n\n    Args:\n        algorithm (str): The identification of model to instantiate. Can be `\"affinity_propagation\"`, `\"dbscan\"`, `\"hierarchical\"`, `\"kmeans\"`, `\"mpckmeans\"` or `\"spectral\"`. Defaults to `\"kmeans\"`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Warns:\n        FutureWarning: `clustering.affinity_propagation.AffinityPropagationConstrainedClustering`, `clustering.dbscan.DBScanConstrainedClustering` and `clustering.mpckmeans.MPCKMeansConstrainedClustering` are still in development and are not fully tested : it is not ready for production use.\n\n    Raises:\n        ValueError: if `algorithm` is not implemented.\n\n    Returns:\n        AbstractConstraintsClustering: An instance of constrained clustering model.\n\n    Example:\n        ```python\n        # Import.\n        from cognitivefactory.interactive_clustering.clustering.factory import clustering_factory\n\n        # Create an instance of kmeans.\n        clustering_model = clustering_factory(\n            algorithm=\"kmeans\",\n            model=\"COP\",\n            random_seed=42,\n        )\n        ```\n    \"\"\"\n\n    # Check that the requested algorithm is implemented.\n    if algorithm not in {\n        \"affinity_propagation\",\n        \"dbscan\",\n        \"hierarchical\",\n        \"kmeans\",\n        \"mpckmeans\",\n        \"spectral\",\n    }:\n        raise ValueError(\"The `algorithm` '\" + str(algorithm) + \"' is not implemented.\")\n\n    # Initialize\n    cluster_object: AbstractConstrainedClustering\n\n    # Case of Affinity Propagation Constrained Clustering.\n    if algorithm == \"affinity_propagation\":\n        cluster_object = AffinityPropagationConstrainedClustering(**kargs)\n\n    # Case of DBScan Constrained Clustering.\n    elif algorithm == \"dbscan\":\n        cluster_object = DBScanConstrainedClustering(**kargs)\n\n    # Case of Hierachical Constrained Clustering.\n    elif algorithm == \"hierarchical\":\n        cluster_object = HierarchicalConstrainedClustering(**kargs)\n\n    # Case of MPC KMmeans Constrained Clustering.\n    elif algorithm == \"mpckmeans\":\n        cluster_object = MPCKMeansConstrainedClustering(**kargs)\n\n    # Case of Spectral Constrained Clustering.\n    elif algorithm == \"spectral\":\n        cluster_object = SpectralConstrainedClustering(**kargs)\n\n    # Default case of KMeans Constrained Clustering (algorithm==\"kmeans\":).\n    else:\n        cluster_object = KMeansConstrainedClustering(**kargs)\n\n    # Return cluster object.\n    return cluster_object\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/","title":"hierarchical","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.clustering.hierarchical</li> <li>Description:  Implementation of constrained hierarchical clustering algorithms.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster","title":"<code>Cluster</code>","text":"<p>This class represents a cluster as a node of the hierarchical clustering tree.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>class Cluster:\n    \"\"\"\n    This class represents a cluster as a node of the hierarchical clustering tree.\n    \"\"\"\n\n    # ==============================================================================\n    # INITIALIZATION\n    # ==============================================================================\n    def __init__(\n        self,\n        vectors: Dict[str, csr_matrix],\n        cluster_ID: int,\n        clustering_iteration: int,\n        children: Optional[List[\"Cluster\"]] = None,\n        members: Optional[List[str]] = None,\n    ) -&gt; None:\n        \"\"\"\n        The constructor for Cluster class.\n\n        Args:\n            vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager` (if `constraints_manager` is set). The value of the dictionary represent the vector of each data.\n            cluster_ID (int): The cluster ID that is defined during `HierarchicalConstrainedClustering.cluster` running.\n            clustering_iteration (int): The cluster iteration that is defined during `HierarchicalConstrainedClustering.cluster` running.\n            children (Optional[List[\"Cluster\"]], optional): A list of clusters children for cluster initialization. Incompatible with `members` parameter. Defaults to `None`.\n            members (Optional[List[str]], optional): A list of data IDs for cluster initialization. Incompatible with `children` parameter. Defaults to `None`.\n\n        Raises:\n            ValueError: if `children` and `members` are both set or both unset.\n        \"\"\"\n\n        # Store links to `vectors`.\n        self.vectors: Dict[str, csr_matrix] = vectors\n\n        # Cluster ID and Clustering iteration.\n        self.cluster_ID: int = cluster_ID\n        self.clustering_iteration: int = clustering_iteration\n\n        # Check children and members.\n        if ((children is not None) and (members is not None)) or ((children is None) and (members is None)):\n            raise ValueError(\n                \"Cluster initialization must be by `children` setting or by `members` setting, but not by both or none of them.\"\n            )\n\n        # Add children (empty or not).\n        self.children: List[\"Cluster\"] = children if (children is not None) else []\n\n        # Cluster inverse depth.\n        self.cluster_inverse_depth: int = (\n            max([child.cluster_inverse_depth for child in self.children]) + 1 if (self.children) else 0\n        )\n\n        # Add members (empty or not).\n        self.members: List[str] = (\n            members if members is not None else [data_ID for child in self.children for data_ID in child.members]\n        )\n\n        # Update centroids\n        self.update_centroid()\n\n    # ==============================================================================\n    # ADD NEW CHILDREN :\n    # ==============================================================================\n    def add_new_children(\n        self,\n        new_children: List[\"Cluster\"],\n        new_clustering_iteration: int,\n    ) -&gt; None:\n        \"\"\"\n        Add new children to the cluster.\n\n        Args:\n            new_children (List[\"Cluster\"]): The list of new clusters children to add.\n            new_clustering_iteration (int): The new cluster iteration that is defined during HierarchicalConstrainedClustering.clusterize running.\n        \"\"\"\n\n        # Update clustering iteration.\n        self.clustering_iteration = new_clustering_iteration\n\n        # Update children.\n        self.children += [new_child for new_child in new_children if new_child not in self.children]\n\n        # Update cluster inverse depth.\n        self.cluster_inverse_depth = max([child.cluster_inverse_depth for child in self.children]) + 1\n\n        # Update members.\n        self.members = [data_ID for child in self.children for data_ID in child.members]\n\n        # Update centroids.\n        self.update_centroid()\n\n    # ==============================================================================\n    # UPDATE CENTROIDS :\n    # ==============================================================================\n    def update_centroid(self) -&gt; None:\n        \"\"\"\n        Update centroid of the cluster.\n        \"\"\"\n\n        # Update centroids.\n        self.centroid: csr_matrix = sum([self.vectors[data_ID] for data_ID in self.members]) / self.get_cluster_size()\n\n    # ==============================================================================\n    # GET CLUSTER SIZE :\n    # ==============================================================================\n    def get_cluster_size(self) -&gt; int:\n        \"\"\"\n        Get cluster size.\n\n        Returns:\n            int: The cluster size, i.e. the number of members in the cluster.\n        \"\"\"\n\n        # Update centroids.\n        return len(self.members)\n\n    # ==============================================================================\n    # TO DICTIONARY :\n    # ==============================================================================\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Transform the Cluster object into a dictionary. It can be used before serialize this object in JSON.\n\n        Returns:\n            Dict[str, Any]: A dictionary that represents the Cluster object.\n        \"\"\"\n\n        # Define the result dictionary.\n        results: Dict[str, Any] = {}\n\n        # Add clustering information.\n        results[\"cluster_ID\"] = self.cluster_ID\n        results[\"clustering_iteration\"] = self.clustering_iteration\n\n        # Add children information.\n        results[\"children\"] = [child.to_dict() for child in self.children]\n        results[\"cluster_inverse_depth\"] = self.cluster_inverse_depth\n\n        # Add members information.\n        results[\"members\"] = self.members\n\n        return results\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.__init__","title":"<code>__init__(vectors, cluster_ID, clustering_iteration, children=None, members=None)</code>","text":"<p>The constructor for Cluster class.</p> <p>Parameters:</p> Name Type Description Default <code>vectors</code> <code>Dict[str, csr_matrix]</code> <p>The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code> (if <code>constraints_manager</code> is set). The value of the dictionary represent the vector of each data.</p> required <code>cluster_ID</code> <code>int</code> <p>The cluster ID that is defined during <code>HierarchicalConstrainedClustering.cluster</code> running.</p> required <code>clustering_iteration</code> <code>int</code> <p>The cluster iteration that is defined during <code>HierarchicalConstrainedClustering.cluster</code> running.</p> required <code>children</code> <code>Optional[List[Cluster]]</code> <p>A list of clusters children for cluster initialization. Incompatible with <code>members</code> parameter. Defaults to <code>None</code>.</p> <code>None</code> <code>members</code> <code>Optional[List[str]]</code> <p>A list of data IDs for cluster initialization. Incompatible with <code>children</code> parameter. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>children</code> and <code>members</code> are both set or both unset.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>def __init__(\n    self,\n    vectors: Dict[str, csr_matrix],\n    cluster_ID: int,\n    clustering_iteration: int,\n    children: Optional[List[\"Cluster\"]] = None,\n    members: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"\n    The constructor for Cluster class.\n\n    Args:\n        vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager` (if `constraints_manager` is set). The value of the dictionary represent the vector of each data.\n        cluster_ID (int): The cluster ID that is defined during `HierarchicalConstrainedClustering.cluster` running.\n        clustering_iteration (int): The cluster iteration that is defined during `HierarchicalConstrainedClustering.cluster` running.\n        children (Optional[List[\"Cluster\"]], optional): A list of clusters children for cluster initialization. Incompatible with `members` parameter. Defaults to `None`.\n        members (Optional[List[str]], optional): A list of data IDs for cluster initialization. Incompatible with `children` parameter. Defaults to `None`.\n\n    Raises:\n        ValueError: if `children` and `members` are both set or both unset.\n    \"\"\"\n\n    # Store links to `vectors`.\n    self.vectors: Dict[str, csr_matrix] = vectors\n\n    # Cluster ID and Clustering iteration.\n    self.cluster_ID: int = cluster_ID\n    self.clustering_iteration: int = clustering_iteration\n\n    # Check children and members.\n    if ((children is not None) and (members is not None)) or ((children is None) and (members is None)):\n        raise ValueError(\n            \"Cluster initialization must be by `children` setting or by `members` setting, but not by both or none of them.\"\n        )\n\n    # Add children (empty or not).\n    self.children: List[\"Cluster\"] = children if (children is not None) else []\n\n    # Cluster inverse depth.\n    self.cluster_inverse_depth: int = (\n        max([child.cluster_inverse_depth for child in self.children]) + 1 if (self.children) else 0\n    )\n\n    # Add members (empty or not).\n    self.members: List[str] = (\n        members if members is not None else [data_ID for child in self.children for data_ID in child.members]\n    )\n\n    # Update centroids\n    self.update_centroid()\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.add_new_children","title":"<code>add_new_children(new_children, new_clustering_iteration)</code>","text":"<p>Add new children to the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>new_children</code> <code>List[Cluster]</code> <p>The list of new clusters children to add.</p> required <code>new_clustering_iteration</code> <code>int</code> <p>The new cluster iteration that is defined during HierarchicalConstrainedClustering.clusterize running.</p> required Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>def add_new_children(\n    self,\n    new_children: List[\"Cluster\"],\n    new_clustering_iteration: int,\n) -&gt; None:\n    \"\"\"\n    Add new children to the cluster.\n\n    Args:\n        new_children (List[\"Cluster\"]): The list of new clusters children to add.\n        new_clustering_iteration (int): The new cluster iteration that is defined during HierarchicalConstrainedClustering.clusterize running.\n    \"\"\"\n\n    # Update clustering iteration.\n    self.clustering_iteration = new_clustering_iteration\n\n    # Update children.\n    self.children += [new_child for new_child in new_children if new_child not in self.children]\n\n    # Update cluster inverse depth.\n    self.cluster_inverse_depth = max([child.cluster_inverse_depth for child in self.children]) + 1\n\n    # Update members.\n    self.members = [data_ID for child in self.children for data_ID in child.members]\n\n    # Update centroids.\n    self.update_centroid()\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.get_cluster_size","title":"<code>get_cluster_size()</code>","text":"<p>Get cluster size.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The cluster size, i.e. the number of members in the cluster.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>def get_cluster_size(self) -&gt; int:\n    \"\"\"\n    Get cluster size.\n\n    Returns:\n        int: The cluster size, i.e. the number of members in the cluster.\n    \"\"\"\n\n    # Update centroids.\n    return len(self.members)\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.to_dict","title":"<code>to_dict()</code>","text":"<p>Transform the Cluster object into a dictionary. It can be used before serialize this object in JSON.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary that represents the Cluster object.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Transform the Cluster object into a dictionary. It can be used before serialize this object in JSON.\n\n    Returns:\n        Dict[str, Any]: A dictionary that represents the Cluster object.\n    \"\"\"\n\n    # Define the result dictionary.\n    results: Dict[str, Any] = {}\n\n    # Add clustering information.\n    results[\"cluster_ID\"] = self.cluster_ID\n    results[\"clustering_iteration\"] = self.clustering_iteration\n\n    # Add children information.\n    results[\"children\"] = [child.to_dict() for child in self.children]\n    results[\"cluster_inverse_depth\"] = self.cluster_inverse_depth\n\n    # Add members information.\n    results[\"members\"] = self.members\n\n    return results\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.update_centroid","title":"<code>update_centroid()</code>","text":"<p>Update centroid of the cluster.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>def update_centroid(self) -&gt; None:\n    \"\"\"\n    Update centroid of the cluster.\n    \"\"\"\n\n    # Update centroids.\n    self.centroid: csr_matrix = sum([self.vectors[data_ID] for data_ID in self.members]) / self.get_cluster_size()\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.HierarchicalConstrainedClustering","title":"<code>HierarchicalConstrainedClustering</code>","text":"<p>             Bases: <code>AbstractConstrainedClustering</code></p> <p>This class implements the hierarchical constrained clustering. It inherits from <code>AbstractConstrainedClustering</code>.</p> References <ul> <li>Hierarchical Clustering: <code>Murtagh, F. et P. Contreras (2012). Algorithms for hierarchical clustering : An overview. Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery 2, 86\u201397.</code></li> <li>Constrained Hierarchical Clustering: <code>Davidson, I. et S. S. Ravi (2005). Agglomerative Hierarchical Clustering with Constraints : Theoretical and Empirical Results. Springer, Berlin, Heidelberg 3721, 12.</code></li> </ul> Example <pre><code># Import.\nfrom scipy.sparse import csr_matrix\nfrom cognitivefactory.interactive_clustering.clustering.hierarchical import HierarchicalConstrainedClustering\n\n# Create an instance of hierarchical clustering.\nclustering_model = HierarchicalConstrainedClustering(\n    linkage=\"ward\",\n    random_seed=2,\n)\n\n# Define vectors.\n# NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\nvectors = {\n    \"0\": csr_matrix([1.00, 0.00, 0.00]),\n    \"1\": csr_matrix([0.95, 0.02, 0.01]),\n    \"2\": csr_matrix([0.98, 0.00, 0.00]),\n    \"3\": csr_matrix([0.99, 0.00, 0.00]),\n    \"4\": csr_matrix([0.01, 0.99, 0.07]),\n    \"5\": csr_matrix([0.02, 0.99, 0.07]),\n    \"6\": csr_matrix([0.01, 0.99, 0.02]),\n    \"7\": csr_matrix([0.01, 0.01, 0.97]),\n    \"8\": csr_matrix([0.00, 0.01, 0.99]),\n    \"9\": csr_matrix([0.00, 0.00, 1.00]),\n}\n\n# Define constraints manager.\nconstraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\n\n# Run clustering.\ndict_of_predicted_clusters = clustering_model.cluster(\n    constraints_manager=constraints_manager,\n    vectors=vectors,\n    nb_clusters=3,\n)\n\n# Print results.\nprint(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 1, \"5\": 1, \"6\": 1, \"7\": 2, \"8\": 2, \"9\": 2,})\nprint(\"Computed results\", \":\", dict_of_predicted_clusters)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>class HierarchicalConstrainedClustering(AbstractConstrainedClustering):\n    \"\"\"\n    This class implements the hierarchical constrained clustering.\n    It inherits from `AbstractConstrainedClustering`.\n\n    References:\n        - Hierarchical Clustering: `Murtagh, F. et P. Contreras (2012). Algorithms for hierarchical clustering : An overview. Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery 2, 86\u201397.`\n        - Constrained Hierarchical Clustering: `Davidson, I. et S. S. Ravi (2005). Agglomerative Hierarchical Clustering with Constraints : Theoretical and Empirical Results. Springer, Berlin, Heidelberg 3721, 12.`\n\n    Example:\n        ```python\n        # Import.\n        from scipy.sparse import csr_matrix\n        from cognitivefactory.interactive_clustering.clustering.hierarchical import HierarchicalConstrainedClustering\n\n        # Create an instance of hierarchical clustering.\n        clustering_model = HierarchicalConstrainedClustering(\n            linkage=\"ward\",\n            random_seed=2,\n        )\n\n        # Define vectors.\n        # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\n        vectors = {\n            \"0\": csr_matrix([1.00, 0.00, 0.00]),\n            \"1\": csr_matrix([0.95, 0.02, 0.01]),\n            \"2\": csr_matrix([0.98, 0.00, 0.00]),\n            \"3\": csr_matrix([0.99, 0.00, 0.00]),\n            \"4\": csr_matrix([0.01, 0.99, 0.07]),\n            \"5\": csr_matrix([0.02, 0.99, 0.07]),\n            \"6\": csr_matrix([0.01, 0.99, 0.02]),\n            \"7\": csr_matrix([0.01, 0.01, 0.97]),\n            \"8\": csr_matrix([0.00, 0.01, 0.99]),\n            \"9\": csr_matrix([0.00, 0.00, 1.00]),\n        }\n\n        # Define constraints manager.\n        constraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\n\n        # Run clustering.\n        dict_of_predicted_clusters = clustering_model.cluster(\n            constraints_manager=constraints_manager,\n            vectors=vectors,\n            nb_clusters=3,\n        )\n\n        # Print results.\n        print(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 1, \"5\": 1, \"6\": 1, \"7\": 2, \"8\": 2, \"9\": 2,})\n        print(\"Computed results\", \":\", dict_of_predicted_clusters)\n        ```\n    \"\"\"\n\n    # ==============================================================================\n    # INITIALIZATION\n    # ==============================================================================\n    def __init__(self, linkage: str = \"ward\", random_seed: Optional[int] = None, **kargs) -&gt; None:\n        \"\"\"\n        The constructor for Hierarchical Constrainted Clustering class.\n\n        Args:\n            linkage (str, optional): The metric used to merge clusters. Several type are implemented :\n                - `\"ward\"`: Merge the two clusters for which the merged cluster from these clusters have the lowest intra-class distance.\n                - `\"average\"`: Merge the two clusters that have the closest barycenters.\n                - `\"complete\"`: Merge the two clusters for which the maximum distance between two data of these clusters is the lowest.\n                - `\"single\"`: Merge the two clusters for which the minimum distance between two data of these clusters is the lowest.\n                Defaults to `\"ward\"`.\n            random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`.\n            **kargs (dict): Other parameters that can be used in the instantiation.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set.\n        \"\"\"\n\n        # Store `self.linkage`.\n        if linkage not in {\"ward\", \"average\", \"complete\", \"single\"}:\n            raise ValueError(\"The `linkage` '\" + str(linkage) + \"' is not implemented.\")\n        self.linkage: str = linkage\n\n        # Store `self.random_seed`\n        self.random_seed: Optional[int] = random_seed\n\n        # Store `self.kargs` for hierarchical clustering.\n        self.kargs = kargs\n\n        # Initialize `self.clustering_root` and `self.dict_of_predicted_clusters`.\n        self.clustering_root: Optional[Cluster] = None\n        self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n\n    # ==============================================================================\n    # MAIN - CLUSTER DATA\n    # ==============================================================================\n    def cluster(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        vectors: Dict[str, csr_matrix],\n        nb_clusters: Optional[int],\n        verbose: bool = False,\n        **kargs,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        The main method used to cluster data with the Hierarchical model.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n            vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n            nb_clusters (Optional[int]): The number of clusters to compute.\n            verbose (bool, optional): Enable verbose output. Defaults to `False`.\n            **kargs (dict): Other parameters that can be used in the clustering.\n\n        Raises:\n            ValueError: If some parameters are incorrectly set.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        ###\n        ### GET PARAMETERS\n        ###\n\n        # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n        if not isinstance(constraints_manager, AbstractConstraintsManager):\n            raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n        self.constraints_manager: AbstractConstraintsManager = constraints_manager\n        self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n        # Store `self.vectors`.\n        if not isinstance(vectors, dict):\n            raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n        self.vectors: Dict[str, csr_matrix] = vectors\n\n        # Store `self.nb_clusters`.\n        if (nb_clusters is None) or (nb_clusters &lt; 2):\n            raise ValueError(\"The `nb_clusters` '\" + str(nb_clusters) + \"' must be greater than or equal to 2.\")\n        self.nb_clusters: int = min(nb_clusters, len(self.list_of_data_IDs))\n\n        # Compute pairwise distances.\n        matrix_of_pairwise_distances: csr_matrix = pairwise_distances(\n            X=vstack(self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()),\n            metric=\"euclidean\",  # TODO get different pairwise_distances config in **kargs\n        )\n\n        # Format pairwise distances in a dictionary and store `self.dict_of_pairwise_distances`.\n        self.dict_of_pairwise_distances: Dict[str, Dict[str, float]] = {\n            vector_ID1: {\n                vector_ID2: float(matrix_of_pairwise_distances[i1, i2])\n                for i2, vector_ID2 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n            }\n            for i1, vector_ID1 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n        }\n\n        ###\n        ### INITIALIZE HIERARCHICAL CONSTRAINED CLUSTERING\n        ###\n\n        # Verbose\n        if verbose:  # pragma: no cover\n            # Verbose - Print progression status.\n            TIME_start: datetime = datetime.now()\n            print(\n                \"    \",\n                \"CLUSTERING_ITERATION=\" + \"INITIALIZATION\",\n                \"(current_time = \" + str(TIME_start - TIME_start).split(\".\")[0] + \")\",\n            )\n\n        # Initialize `self.clustering_root` and `self.dict_of_predicted_clusters`.\n        self.clustering_root = None\n        self.dict_of_predicted_clusters = None\n\n        # Initialize iteration counter.\n        self.clustering_iteration: int = 0\n\n        # Initialize `current_clusters` and `self.clusters_storage`.\n        self.current_clusters: List[int] = []\n        self.clusters_storage: Dict[int, Cluster] = {}\n\n        # Get the list of possibles lists of MUST_LINK data for initialization.\n        list_of_possible_lists_of_MUST_LINK_data: List[List[str]] = self.constraints_manager.get_connected_components()\n\n        # Estimation of max number of iteration.\n        max_clustering_iteration: int = len(list_of_possible_lists_of_MUST_LINK_data) - 1\n\n        # For each list of same data (MUST_LINK constraints).\n        for MUST_LINK_data in list_of_possible_lists_of_MUST_LINK_data:\n            # Create a initial cluster with data that MUST be LINKed.\n            self._add_new_cluster_by_setting_members(\n                members=MUST_LINK_data,\n            )\n\n        # Initialize distance between clusters.\n        self.clusters_distance: Dict[int, Dict[int, float]] = {}\n        for cluster_IDi in self.current_clusters:\n            for cluster_IDj in self.current_clusters:\n                if cluster_IDi &lt; cluster_IDj:\n                    # Compute distance between cluster i and cluster j.\n                    distance: float = self._compute_distance(cluster_IDi=cluster_IDi, cluster_IDj=cluster_IDj)\n                    # Store distance between cluster i and cluster j.\n                    self._set_distance(cluster_IDi=cluster_IDi, cluster_IDj=cluster_IDj, distance=distance)\n\n        # Initialize iterations at first iteration.\n        self.clustering_iteration = 1\n\n        ###\n        ### RUN ITERATIONS OF HIERARCHICAL CONSTRAINED CLUSTERING UNTIL CONVERGENCE\n        ###\n\n        # Iter until convergence of clustering.\n        while len(self.current_clusters) &gt; 1:\n            # Verbose\n            if verbose:  # pragma: no cover\n                # Verbose - Print progression status.\n                TIME_current: datetime = datetime.now()\n                print(\n                    \"    \",\n                    \"CLUSTERING_ITERATION=\"\n                    + str(self.clustering_iteration).zfill(6)\n                    + \"/\"\n                    + str(max_clustering_iteration).zfill(6),\n                    \"(current_time = \" + str(TIME_current - TIME_start).split(\".\")[0] + \")\",\n                    end=\"\\r\",\n                )\n\n            # Get clostest clusters to merge\n            clostest_clusters: Optional[Tuple[int, int]] = self._get_the_two_clostest_clusters()\n\n            # If no clusters to merge, then stop iterations.\n            if clostest_clusters is None:\n                break\n\n            # Merge clusters the two closest clusters and add the merged cluster to the storage.\n            # If merge one cluster \"node\" with a cluster \"leaf\" : add the cluster \"leaf\" to the children of the cluster \"node\".\n            # If merge two clusters \"nodes\" or two clusters \"leaves\" : create a new cluster \"node\".\n            merged_cluster_ID: int = self._add_new_cluster_by_merging_clusters(\n                children=[\n                    clostest_clusters[0],\n                    clostest_clusters[1],\n                ]\n            )\n\n            # Update distances\n            for cluster_ID in self.current_clusters:\n                if cluster_ID != merged_cluster_ID:\n                    # Compute distance between cluster and merged cluster.\n                    distance = self._compute_distance(cluster_IDi=cluster_ID, cluster_IDj=merged_cluster_ID)\n                    # Store distance between cluster and merged cluster.\n                    self._set_distance(cluster_IDi=cluster_ID, cluster_IDj=merged_cluster_ID, distance=distance)\n\n            # Update self.clustering_iteration.\n            self.clustering_iteration += 1\n\n        ###\n        ### END HIERARCHICAL CONSTRAINED CLUSTERING\n        ###\n\n        # Verbose\n        if verbose:  # pragma: no cover\n            # Verbose - Print progression status.\n            TIME_current = datetime.now()\n\n            # Case of clustering not completed.\n            if len(self.current_clusters) &gt; 1:\n                print(\n                    \"    \",\n                    \"CLUSTERING_ITERATION=\" + str(self.clustering_iteration).zfill(5),\n                    \"-\",\n                    \"End : No more cluster to merge\",\n                    \"(current_time = \" + str(TIME_current - TIME_start).split(\".\")[0] + \")\",\n                )\n            else:\n                print(\n                    \"    \",\n                    \"CLUSTERING_ITERATION=\" + str(self.clustering_iteration).zfill(5),\n                    \"-\",\n                    \"End : Full clustering done\",\n                    \"(current_time = \" + str(TIME_current - TIME_start).split(\".\")[0] + \")\",\n                )\n\n        # If several clusters remains, then merge them in a cluster root.\n        if len(self.current_clusters) &gt; 1:\n            # Merge all remaining clusters.\n            # If merge one cluster \"node\" with many cluster \"leaves\" : add clusters \"leaves\" to the children of the cluster \"node\".\n            # If merge many clusters \"nodes\" and/or many clusters \"leaves\" : create a new cluster \"node\".\n            self._add_new_cluster_by_merging_clusters(children=self.current_clusters.copy())\n\n        # Get clustering root.\n        root_ID: int = self.current_clusters[0]\n        self.clustering_root = self.clusters_storage[root_ID]\n\n        ###\n        ### GET PREDICTED CLUSTERS\n        ###\n\n        # Compute predicted clusters.\n        self.dict_of_predicted_clusters = self.compute_predicted_clusters(\n            nb_clusters=self.nb_clusters,\n        )\n\n        return self.dict_of_predicted_clusters\n\n    # ==============================================================================\n    # ADD CLUSTER BY SETTING MEMBERS :\n    # ==============================================================================\n    def _add_new_cluster_by_setting_members(\n        self,\n        members: List[str],\n    ) -&gt; int:\n        \"\"\"\n        Create or Update a cluster by setting its members, and add it to the storage and current clusters.\n\n        Args:\n            members (List[str]): A list of data IDs to define the new cluster by the data it contains.\n\n        Returns:\n            int : ID of the merged cluster.\n        \"\"\"\n        # Get the ID of the new cluster.\n        new_cluster_ID: int = max(self.clusters_storage.keys()) + 1 if (self.clusters_storage) else 0\n\n        # Create the cluster.\n        new_cluster = Cluster(\n            vectors=self.vectors,\n            cluster_ID=new_cluster_ID,\n            clustering_iteration=self.clustering_iteration,\n            members=members,\n        )\n\n        # Add new_cluster to `self.current_clusters` and `self.clusters_storage`.\n        self.current_clusters.append(new_cluster_ID)\n        self.clusters_storage[new_cluster_ID] = new_cluster\n\n        return new_cluster_ID\n\n    # ==============================================================================\n    # ADD CLUSTER BY MERGING CLUSTERS :\n    # ==============================================================================\n    def _add_new_cluster_by_merging_clusters(\n        self,\n        children: List[int],\n    ) -&gt; int:\n        \"\"\"\n        Create or Update a cluster by setting its children, and add it to the storage and current clusters.\n\n        Args:\n            children (List[int]): A list of cluster IDs to define the new cluster by its children.\n\n        Returns:\n            int : ID of the merged cluster.\n        \"\"\"\n\n        # Remove all leaves children clusters from `self.current_clusters`.\n        for child_ID_to_remove in children:\n            self.current_clusters.remove(child_ID_to_remove)\n\n        \"\"\"\n        ###\n        ### Tree optimization : if only one node, then update this node as parent of all leaves.\n        ### TODO : test of check if relevant to use. pros = smarter tree visualisation ; cons = cluster number more difficult to choose.\n        ###\n\n        # List of children nodes.\n        list_of_children_nodes: List[int] = [\n            child_ID\n            for child_ID in children\n            if len(self.clusters_storage[child_ID].children) &gt; 0\n        ]\n\n        if len(list_of_children_nodes) == 1:\n\n            # Get the ID of the cluster to update\n            parent_cluster_ID: int = list_of_children_nodes[0]\n            parent_cluster: Cluster = self.clusters_storage[parent_cluster_ID]\n\n            # Add all leaves\n            parent_cluster.add_new_children(\n                new_children=[\n                    self.clusters_storage[child_ID]\n                    for child_ID in children\n                    if child_ID != parent_cluster_ID\n                ],\n                new_clustering_iteration=self.clustering_iteration\n            )\n\n            # Add new_cluster to `self.current_clusters` and `self.clusters_storage`.\n            self.current_clusters.append(parent_cluster_ID)\n            self.clusters_storage[parent_cluster_ID] = parent_cluster\n\n            # Return the cluster_ID of the created cluster.\n            return parent_cluster_ID\n\n\n        \"\"\"\n\n        ###\n        ### Default case : Create a new node as parent of all children to merge.\n        ###\n\n        # Get the ID of the new cluster.\n        parent_cluster_ID: int = max(self.clusters_storage) + 1\n\n        # Create the cluster\n        parent_cluster = Cluster(\n            vectors=self.vectors,\n            cluster_ID=parent_cluster_ID,\n            clustering_iteration=self.clustering_iteration,\n            children=[self.clusters_storage[child_ID] for child_ID in children],\n        )\n\n        # Add new_cluster to `self.current_clusters` and `self.clusters_storage`.\n        self.current_clusters.append(parent_cluster_ID)\n        self.clusters_storage[parent_cluster_ID] = parent_cluster\n\n        # Return the cluster_ID of the created cluster.\n        return parent_cluster_ID\n\n    # ==============================================================================\n    # COMPUTE DISTANCE BETWEEN CLUSTERING NEW ITERATION OF CLUSTERING :\n    # ==============================================================================\n    def _compute_distance(self, cluster_IDi: int, cluster_IDj: int) -&gt; float:\n        \"\"\"\n        Compute distance between two clusters.\n\n        Args:\n            cluster_IDi (int): ID of the first cluster.\n            cluster_IDj (int): ID of the second cluster.\n\n        Returns:\n            float : Distance between the two clusters.\n        \"\"\"\n\n        # Check `\"CANNOT_LINK\"` constraints.\n        for data_ID1 in self.clusters_storage[cluster_IDi].members:\n            for data_ID2 in self.clusters_storage[cluster_IDj].members:\n                if (\n                    self.constraints_manager.get_inferred_constraint(\n                        data_ID1=data_ID1,\n                        data_ID2=data_ID2,\n                    )\n                    == \"CANNOT_LINK\"\n                ):\n                    return float(\"Inf\")\n\n        # Case 1 : `self.linkage` is \"complete\".\n        if self.linkage == \"complete\":\n            return max(\n                [\n                    self.dict_of_pairwise_distances[data_ID_in_cluster_IDi][data_ID_in_cluster_IDj]\n                    for data_ID_in_cluster_IDi in self.clusters_storage[cluster_IDi].members\n                    for data_ID_in_cluster_IDj in self.clusters_storage[cluster_IDj].members\n                ]\n            )\n\n        # Case 2 : `self.linkage` is \"average\".\n        if self.linkage == \"average\":\n            return pairwise_distances(\n                X=self.clusters_storage[cluster_IDi].centroid,\n                Y=self.clusters_storage[cluster_IDj].centroid,\n                metric=\"euclidean\",  # TODO: Load different parameters for distance computing ?\n            )[0][0]\n\n        # Case 3 : `self.linkage` is \"single\".\n        if self.linkage == \"single\":\n            return min(\n                [\n                    self.dict_of_pairwise_distances[data_ID_in_cluster_IDi][data_ID_in_cluster_IDj]\n                    for data_ID_in_cluster_IDi in self.clusters_storage[cluster_IDi].members\n                    for data_ID_in_cluster_IDj in self.clusters_storage[cluster_IDj].members\n                ]\n            )\n\n        # Case 4 : `self.linkage` is \"ward\".\n        ##if self.linkage == \"ward\": ## DEFAULTS\n        # Compute distance\n        merged_members: List[str] = (\n            self.clusters_storage[cluster_IDi].members + self.clusters_storage[cluster_IDj].members\n        )\n        return sum(\n            [\n                self.dict_of_pairwise_distances[data_IDi][data_IDj]\n                for i, data_IDi in enumerate(merged_members)\n                for j, data_IDj in enumerate(merged_members)\n                if i &lt; j\n            ]\n        ) / (len(self.clusters_storage[cluster_IDi].members) * len(self.clusters_storage[cluster_IDj].members))\n\n    # ==============================================================================\n    # DISTANCE : GETTER\n    # ==============================================================================\n    def _get_distance(self, cluster_IDi: int, cluster_IDj: int) -&gt; float:\n        \"\"\"\n        Get the distance between two clusters.\n\n        Args:\n            cluster_IDi (int): ID of the first cluster.\n            cluster_IDj (int): ID of the second cluster.\n\n        Returns:\n            float : Distance between the two clusters.\n        \"\"\"\n\n        # Sort IDs of cluster.\n        min_cluster_ID: int = min(cluster_IDi, cluster_IDj)\n        max_cluster_ID: int = max(cluster_IDi, cluster_IDj)\n\n        # Return the distance.\n        return self.clusters_distance[min_cluster_ID][max_cluster_ID]\n\n    # ==============================================================================\n    # DISTANCE : SETTER\n    # ==============================================================================\n    def _set_distance(\n        self,\n        distance: float,\n        cluster_IDi: int,\n        cluster_IDj: int,\n    ) -&gt; None:\n        \"\"\"\n        Set the distance between two clusters.\n\n        Args:\n            distance (float): The distance between the two clusters.\n            cluster_IDi (int): ID of the first cluster.\n            cluster_IDj (int): ID of the second cluster.\n        \"\"\"\n\n        # Sort IDs of cluster.\n        min_cluster_ID: int = min(cluster_IDi, cluster_IDj)\n        max_cluster_ID: int = max(cluster_IDi, cluster_IDj)\n\n        # Add distance to the dictionary of distance.\n        if min_cluster_ID not in self.clusters_distance:\n            self.clusters_distance[min_cluster_ID] = {}\n        self.clusters_distance[min_cluster_ID][max_cluster_ID] = distance\n\n    # ==============================================================================\n    # GET THE TWO CLOSEST CLUSTERS\n    # ==============================================================================\n    def _get_the_two_clostest_clusters(self) -&gt; Optional[Tuple[int, int]]:\n        \"\"\"\n        Get the two clusters which are the two closest clusters.\n\n        Returns:\n            Optional(Tuple[int, int]) : The IDs of the two closest clusters to merge. Return None if no cluster is suitable.\n        \"\"\"\n\n        # Compute the two clostest clusters to merge. take the closest distance, then the closest cluster size.\n        clostest_clusters = min(\n            [\n                {\n                    \"cluster_ID1\": cluster_ID1,\n                    \"cluster_ID2\": cluster_ID2,\n                    \"distance\": self._get_distance(cluster_IDi=cluster_ID1, cluster_IDj=cluster_ID2),\n                    \"merged_size\": len(self.clusters_storage[cluster_ID1].members)\n                    + len(self.clusters_storage[cluster_ID2].members)\n                    # TODO : Choose between \"distance then size(count)\" and \"size_type(boolean) then distance\"\n                }\n                for cluster_ID1 in self.current_clusters\n                for cluster_ID2 in self.current_clusters\n                if cluster_ID1 &lt; cluster_ID2\n            ],\n            key=lambda dst: (dst[\"distance\"], dst[\"merged_size\"]),\n        )\n\n        # Get clusters and distance.\n        cluster_ID1: int = int(clostest_clusters[\"cluster_ID1\"])\n        cluster_ID2: int = int(clostest_clusters[\"cluster_ID2\"])\n        distance: float = clostest_clusters[\"distance\"]\n\n        # Check distance.\n        if distance == float(\"Inf\"):\n            return None\n\n        # Return the tow closest clusters.\n        return cluster_ID1, cluster_ID2\n\n    # ==============================================================================\n    # COMPUTE PREDICTED CLUSTERS\n    # ==============================================================================\n    def compute_predicted_clusters(self, nb_clusters: int, by: str = \"size\") -&gt; Dict[str, int]:\n        \"\"\"\n        Compute the predicted clusters based on clustering tree and estimation of number of clusters.\n\n        Args:\n            nb_clusters (int): The number of clusters to compute.\n            by (str, optional): A string to identifies the criteria used to explore `HierarchicalConstrainedClustering` tree. Can be `\"size\"` or `\"iteration\"`. Defaults to `\"size\"`.\n\n        Raises:\n            ValueError: if `clustering_root` was not set.\n\n        Returns:\n            Dict[str,int] : A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        # Check that the clustering has been made.\n        if self.clustering_root is None:\n            raise ValueError(\"The `clustering_root` is not set, probably because clustering was not run.\")\n\n        ###\n        ### EXPLORE CLUSTER TREE\n        ###\n\n        # Define the resulted list of children as the children of `HierarchicalConstrainedClustering` root.\n        list_of_clusters: List[Cluster] = [self.clustering_root]\n\n        # Explore `HierarchicalConstrainedClustering` children until dict_of_predicted_clusters has the right number of children.\n        while len(list_of_clusters) &lt; nb_clusters:\n            if by == \"size\":\n                # Get the biggest cluster in current children from `HierarchicalConstrainedClustering` exploration.\n                # i.e. it's the cluster that has the more data to split.\n                cluster_to_split = max(list_of_clusters, key=lambda c: len(c.members))\n\n            else:  # if by == \"iteration\":\n                # Get the most recent cluster in current children from `HierarchicalConstrainedClustering` exploration.\n                # i.e. it's the cluster that was last merged.\n                cluster_to_split = max(list_of_clusters, key=lambda c: c.clustering_iteration)\n\n            # If the chosen cluster is a leaf : break the `HierarchicalConstrainedClustering` exploration.\n            if cluster_to_split.children == []:  # noqa: WPS520\n                break\n\n            # Otherwise: The chosen cluster is a node, so split it and get its children.\n            else:\n                # ... remove the cluster obtained ...\n                list_of_clusters.remove(cluster_to_split)\n\n                # ... and add all its children.\n                for child in cluster_to_split.children:\n                    list_of_clusters.append(child)\n\n        ###\n        ### GET PREDICTED CLUSTERS\n        ###\n\n        # Initialize the dictionary of predicted clusters.\n        predicted_clusters: Dict[str, int] = {data_ID: -1 for data_ID in self.list_of_data_IDs}\n\n        # For all cluster...\n        for cluster in list_of_clusters:\n            # ... and for all member in each cluster...\n            for data_ID in cluster.members:\n                # ... affect the predicted cluster (cluster ID) to the data.\n                predicted_clusters[data_ID] = cluster.cluster_ID\n\n        # Rename cluster IDs by order.\n        predicted_clusters = rename_clusters_by_order(clusters=predicted_clusters)\n\n        # Return predicted clusters\n        return predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.HierarchicalConstrainedClustering.__init__","title":"<code>__init__(linkage='ward', random_seed=None, **kargs)</code>","text":"<p>The constructor for Hierarchical Constrainted Clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>linkage</code> <code>str</code> <p>The metric used to merge clusters. Several type are implemented : - <code>\"ward\"</code>: Merge the two clusters for which the merged cluster from these clusters have the lowest intra-class distance. - <code>\"average\"</code>: Merge the two clusters that have the closest barycenters. - <code>\"complete\"</code>: Merge the two clusters for which the maximum distance between two data of these clusters is the lowest. - <code>\"single\"</code>: Merge the two clusters for which the minimum distance between two data of these clusters is the lowest. Defaults to <code>\"ward\"</code>.</p> <code>'ward'</code> <code>random_seed</code> <code>Optional[int]</code> <p>The random seed to use to redo the same clustering. Defaults to <code>None</code>.</p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>def __init__(self, linkage: str = \"ward\", random_seed: Optional[int] = None, **kargs) -&gt; None:\n    \"\"\"\n    The constructor for Hierarchical Constrainted Clustering class.\n\n    Args:\n        linkage (str, optional): The metric used to merge clusters. Several type are implemented :\n            - `\"ward\"`: Merge the two clusters for which the merged cluster from these clusters have the lowest intra-class distance.\n            - `\"average\"`: Merge the two clusters that have the closest barycenters.\n            - `\"complete\"`: Merge the two clusters for which the maximum distance between two data of these clusters is the lowest.\n            - `\"single\"`: Merge the two clusters for which the minimum distance between two data of these clusters is the lowest.\n            Defaults to `\"ward\"`.\n        random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set.\n    \"\"\"\n\n    # Store `self.linkage`.\n    if linkage not in {\"ward\", \"average\", \"complete\", \"single\"}:\n        raise ValueError(\"The `linkage` '\" + str(linkage) + \"' is not implemented.\")\n    self.linkage: str = linkage\n\n    # Store `self.random_seed`\n    self.random_seed: Optional[int] = random_seed\n\n    # Store `self.kargs` for hierarchical clustering.\n    self.kargs = kargs\n\n    # Initialize `self.clustering_root` and `self.dict_of_predicted_clusters`.\n    self.clustering_root: Optional[Cluster] = None\n    self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.HierarchicalConstrainedClustering.cluster","title":"<code>cluster(constraints_manager, vectors, nb_clusters, verbose=False, **kargs)</code>","text":"<p>The main method used to cluster data with the Hierarchical model.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs that will force clustering to respect some conditions during computation.</p> required <code>vectors</code> <code>Dict[str, csr_matrix]</code> <p>The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data.</p> required <code>nb_clusters</code> <code>Optional[int]</code> <p>The number of clusters to compute.</p> required <code>verbose</code> <code>bool</code> <p>Enable verbose output. Defaults to <code>False</code>.</p> <code>False</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the clustering.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If some parameters are incorrectly set.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>def cluster(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    vectors: Dict[str, csr_matrix],\n    nb_clusters: Optional[int],\n    verbose: bool = False,\n    **kargs,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    The main method used to cluster data with the Hierarchical model.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n        vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n        nb_clusters (Optional[int]): The number of clusters to compute.\n        verbose (bool, optional): Enable verbose output. Defaults to `False`.\n        **kargs (dict): Other parameters that can be used in the clustering.\n\n    Raises:\n        ValueError: If some parameters are incorrectly set.\n\n    Returns:\n        Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n\n    ###\n    ### GET PARAMETERS\n    ###\n\n    # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n    if not isinstance(constraints_manager, AbstractConstraintsManager):\n        raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n    self.constraints_manager: AbstractConstraintsManager = constraints_manager\n    self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n    # Store `self.vectors`.\n    if not isinstance(vectors, dict):\n        raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n    self.vectors: Dict[str, csr_matrix] = vectors\n\n    # Store `self.nb_clusters`.\n    if (nb_clusters is None) or (nb_clusters &lt; 2):\n        raise ValueError(\"The `nb_clusters` '\" + str(nb_clusters) + \"' must be greater than or equal to 2.\")\n    self.nb_clusters: int = min(nb_clusters, len(self.list_of_data_IDs))\n\n    # Compute pairwise distances.\n    matrix_of_pairwise_distances: csr_matrix = pairwise_distances(\n        X=vstack(self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()),\n        metric=\"euclidean\",  # TODO get different pairwise_distances config in **kargs\n    )\n\n    # Format pairwise distances in a dictionary and store `self.dict_of_pairwise_distances`.\n    self.dict_of_pairwise_distances: Dict[str, Dict[str, float]] = {\n        vector_ID1: {\n            vector_ID2: float(matrix_of_pairwise_distances[i1, i2])\n            for i2, vector_ID2 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n        }\n        for i1, vector_ID1 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n    }\n\n    ###\n    ### INITIALIZE HIERARCHICAL CONSTRAINED CLUSTERING\n    ###\n\n    # Verbose\n    if verbose:  # pragma: no cover\n        # Verbose - Print progression status.\n        TIME_start: datetime = datetime.now()\n        print(\n            \"    \",\n            \"CLUSTERING_ITERATION=\" + \"INITIALIZATION\",\n            \"(current_time = \" + str(TIME_start - TIME_start).split(\".\")[0] + \")\",\n        )\n\n    # Initialize `self.clustering_root` and `self.dict_of_predicted_clusters`.\n    self.clustering_root = None\n    self.dict_of_predicted_clusters = None\n\n    # Initialize iteration counter.\n    self.clustering_iteration: int = 0\n\n    # Initialize `current_clusters` and `self.clusters_storage`.\n    self.current_clusters: List[int] = []\n    self.clusters_storage: Dict[int, Cluster] = {}\n\n    # Get the list of possibles lists of MUST_LINK data for initialization.\n    list_of_possible_lists_of_MUST_LINK_data: List[List[str]] = self.constraints_manager.get_connected_components()\n\n    # Estimation of max number of iteration.\n    max_clustering_iteration: int = len(list_of_possible_lists_of_MUST_LINK_data) - 1\n\n    # For each list of same data (MUST_LINK constraints).\n    for MUST_LINK_data in list_of_possible_lists_of_MUST_LINK_data:\n        # Create a initial cluster with data that MUST be LINKed.\n        self._add_new_cluster_by_setting_members(\n            members=MUST_LINK_data,\n        )\n\n    # Initialize distance between clusters.\n    self.clusters_distance: Dict[int, Dict[int, float]] = {}\n    for cluster_IDi in self.current_clusters:\n        for cluster_IDj in self.current_clusters:\n            if cluster_IDi &lt; cluster_IDj:\n                # Compute distance between cluster i and cluster j.\n                distance: float = self._compute_distance(cluster_IDi=cluster_IDi, cluster_IDj=cluster_IDj)\n                # Store distance between cluster i and cluster j.\n                self._set_distance(cluster_IDi=cluster_IDi, cluster_IDj=cluster_IDj, distance=distance)\n\n    # Initialize iterations at first iteration.\n    self.clustering_iteration = 1\n\n    ###\n    ### RUN ITERATIONS OF HIERARCHICAL CONSTRAINED CLUSTERING UNTIL CONVERGENCE\n    ###\n\n    # Iter until convergence of clustering.\n    while len(self.current_clusters) &gt; 1:\n        # Verbose\n        if verbose:  # pragma: no cover\n            # Verbose - Print progression status.\n            TIME_current: datetime = datetime.now()\n            print(\n                \"    \",\n                \"CLUSTERING_ITERATION=\"\n                + str(self.clustering_iteration).zfill(6)\n                + \"/\"\n                + str(max_clustering_iteration).zfill(6),\n                \"(current_time = \" + str(TIME_current - TIME_start).split(\".\")[0] + \")\",\n                end=\"\\r\",\n            )\n\n        # Get clostest clusters to merge\n        clostest_clusters: Optional[Tuple[int, int]] = self._get_the_two_clostest_clusters()\n\n        # If no clusters to merge, then stop iterations.\n        if clostest_clusters is None:\n            break\n\n        # Merge clusters the two closest clusters and add the merged cluster to the storage.\n        # If merge one cluster \"node\" with a cluster \"leaf\" : add the cluster \"leaf\" to the children of the cluster \"node\".\n        # If merge two clusters \"nodes\" or two clusters \"leaves\" : create a new cluster \"node\".\n        merged_cluster_ID: int = self._add_new_cluster_by_merging_clusters(\n            children=[\n                clostest_clusters[0],\n                clostest_clusters[1],\n            ]\n        )\n\n        # Update distances\n        for cluster_ID in self.current_clusters:\n            if cluster_ID != merged_cluster_ID:\n                # Compute distance between cluster and merged cluster.\n                distance = self._compute_distance(cluster_IDi=cluster_ID, cluster_IDj=merged_cluster_ID)\n                # Store distance between cluster and merged cluster.\n                self._set_distance(cluster_IDi=cluster_ID, cluster_IDj=merged_cluster_ID, distance=distance)\n\n        # Update self.clustering_iteration.\n        self.clustering_iteration += 1\n\n    ###\n    ### END HIERARCHICAL CONSTRAINED CLUSTERING\n    ###\n\n    # Verbose\n    if verbose:  # pragma: no cover\n        # Verbose - Print progression status.\n        TIME_current = datetime.now()\n\n        # Case of clustering not completed.\n        if len(self.current_clusters) &gt; 1:\n            print(\n                \"    \",\n                \"CLUSTERING_ITERATION=\" + str(self.clustering_iteration).zfill(5),\n                \"-\",\n                \"End : No more cluster to merge\",\n                \"(current_time = \" + str(TIME_current - TIME_start).split(\".\")[0] + \")\",\n            )\n        else:\n            print(\n                \"    \",\n                \"CLUSTERING_ITERATION=\" + str(self.clustering_iteration).zfill(5),\n                \"-\",\n                \"End : Full clustering done\",\n                \"(current_time = \" + str(TIME_current - TIME_start).split(\".\")[0] + \")\",\n            )\n\n    # If several clusters remains, then merge them in a cluster root.\n    if len(self.current_clusters) &gt; 1:\n        # Merge all remaining clusters.\n        # If merge one cluster \"node\" with many cluster \"leaves\" : add clusters \"leaves\" to the children of the cluster \"node\".\n        # If merge many clusters \"nodes\" and/or many clusters \"leaves\" : create a new cluster \"node\".\n        self._add_new_cluster_by_merging_clusters(children=self.current_clusters.copy())\n\n    # Get clustering root.\n    root_ID: int = self.current_clusters[0]\n    self.clustering_root = self.clusters_storage[root_ID]\n\n    ###\n    ### GET PREDICTED CLUSTERS\n    ###\n\n    # Compute predicted clusters.\n    self.dict_of_predicted_clusters = self.compute_predicted_clusters(\n        nb_clusters=self.nb_clusters,\n    )\n\n    return self.dict_of_predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.HierarchicalConstrainedClustering.compute_predicted_clusters","title":"<code>compute_predicted_clusters(nb_clusters, by='size')</code>","text":"<p>Compute the predicted clusters based on clustering tree and estimation of number of clusters.</p> <p>Parameters:</p> Name Type Description Default <code>nb_clusters</code> <code>int</code> <p>The number of clusters to compute.</p> required <code>by</code> <code>str</code> <p>A string to identifies the criteria used to explore <code>HierarchicalConstrainedClustering</code> tree. Can be <code>\"size\"</code> or <code>\"iteration\"</code>. Defaults to <code>\"size\"</code>.</p> <code>'size'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>clustering_root</code> was not set.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int] : A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\hierarchical.py</code> <pre><code>def compute_predicted_clusters(self, nb_clusters: int, by: str = \"size\") -&gt; Dict[str, int]:\n    \"\"\"\n    Compute the predicted clusters based on clustering tree and estimation of number of clusters.\n\n    Args:\n        nb_clusters (int): The number of clusters to compute.\n        by (str, optional): A string to identifies the criteria used to explore `HierarchicalConstrainedClustering` tree. Can be `\"size\"` or `\"iteration\"`. Defaults to `\"size\"`.\n\n    Raises:\n        ValueError: if `clustering_root` was not set.\n\n    Returns:\n        Dict[str,int] : A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n\n    # Check that the clustering has been made.\n    if self.clustering_root is None:\n        raise ValueError(\"The `clustering_root` is not set, probably because clustering was not run.\")\n\n    ###\n    ### EXPLORE CLUSTER TREE\n    ###\n\n    # Define the resulted list of children as the children of `HierarchicalConstrainedClustering` root.\n    list_of_clusters: List[Cluster] = [self.clustering_root]\n\n    # Explore `HierarchicalConstrainedClustering` children until dict_of_predicted_clusters has the right number of children.\n    while len(list_of_clusters) &lt; nb_clusters:\n        if by == \"size\":\n            # Get the biggest cluster in current children from `HierarchicalConstrainedClustering` exploration.\n            # i.e. it's the cluster that has the more data to split.\n            cluster_to_split = max(list_of_clusters, key=lambda c: len(c.members))\n\n        else:  # if by == \"iteration\":\n            # Get the most recent cluster in current children from `HierarchicalConstrainedClustering` exploration.\n            # i.e. it's the cluster that was last merged.\n            cluster_to_split = max(list_of_clusters, key=lambda c: c.clustering_iteration)\n\n        # If the chosen cluster is a leaf : break the `HierarchicalConstrainedClustering` exploration.\n        if cluster_to_split.children == []:  # noqa: WPS520\n            break\n\n        # Otherwise: The chosen cluster is a node, so split it and get its children.\n        else:\n            # ... remove the cluster obtained ...\n            list_of_clusters.remove(cluster_to_split)\n\n            # ... and add all its children.\n            for child in cluster_to_split.children:\n                list_of_clusters.append(child)\n\n    ###\n    ### GET PREDICTED CLUSTERS\n    ###\n\n    # Initialize the dictionary of predicted clusters.\n    predicted_clusters: Dict[str, int] = {data_ID: -1 for data_ID in self.list_of_data_IDs}\n\n    # For all cluster...\n    for cluster in list_of_clusters:\n        # ... and for all member in each cluster...\n        for data_ID in cluster.members:\n            # ... affect the predicted cluster (cluster ID) to the data.\n            predicted_clusters[data_ID] = cluster.cluster_ID\n\n    # Rename cluster IDs by order.\n    predicted_clusters = rename_clusters_by_order(clusters=predicted_clusters)\n\n    # Return predicted clusters\n    return predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/kmeans/","title":"kmeans","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.clustering.kmeans</li> <li>Description:  Implementation of constrained kmeans clustering algorithms.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering","title":"<code>KMeansConstrainedClustering</code>","text":"<p>             Bases: <code>AbstractConstrainedClustering</code></p> <p>This class implements the KMeans constrained clustering. It inherits from <code>AbstractConstrainedClustering</code>.</p> References <ul> <li>KMeans Clustering: <code>MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297.</code></li> <li>Constrained 'COP' KMeans Clustering: <code>Wagstaff, K., C. Cardie, S. Rogers, et S. Schroedl (2001). Constrained K-means Clustering with Background Knowledge. International Conference on Machine Learning</code></li> </ul> Example <pre><code># Import.\nfrom scipy.sparse import csr_matrix\nfrom cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\nfrom cognitivefactory.interactive_clustering.clustering.kmeans import KMeansConstrainedClustering\n\n# Create an instance of kmeans clustering.\nclustering_model = KMeansConstrainedClustering(\n    model=\"COP\",\n    random_seed=2,\n)\n\n# Define vectors.\n# NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\nvectors = {\n    \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n    \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n    \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n    \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n    \"4\": csr_matrix([0.50, 0.22, 0.21, 0.07]),\n    \"5\": csr_matrix([0.50, 0.21, 0.22, 0.07]),\n    \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n    \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n    \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n}\n\n# Define constraints manager.\nconstraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"7\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n\n# Run clustering.\ndict_of_predicted_clusters = clustering_model.cluster(\n    constraints_manager=constraints_manager,\n    vectors=vectors,\n    nb_clusters=3,\n)\n\n# Print results.\nprint(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 1, \"3\": 1, \"4\": 2, \"5\": 2, \"6\": 0, \"7\": 0, \"8\": 0,})\nprint(\"Computed results\", \":\", dict_of_predicted_clusters)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\kmeans.py</code> <pre><code>class KMeansConstrainedClustering(AbstractConstrainedClustering):\n    \"\"\"\n    This class implements the KMeans constrained clustering.\n    It inherits from `AbstractConstrainedClustering`.\n\n    References:\n        - KMeans Clustering: `MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297.`\n        - Constrained _'COP'_ KMeans Clustering: `Wagstaff, K., C. Cardie, S. Rogers, et S. Schroedl (2001). Constrained K-means Clustering with Background Knowledge. International Conference on Machine Learning`\n\n    Example:\n        ```python\n        # Import.\n        from scipy.sparse import csr_matrix\n        from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\n        from cognitivefactory.interactive_clustering.clustering.kmeans import KMeansConstrainedClustering\n\n        # Create an instance of kmeans clustering.\n        clustering_model = KMeansConstrainedClustering(\n            model=\"COP\",\n            random_seed=2,\n        )\n\n        # Define vectors.\n        # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\n        vectors = {\n            \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n            \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n            \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n            \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n            \"4\": csr_matrix([0.50, 0.22, 0.21, 0.07]),\n            \"5\": csr_matrix([0.50, 0.21, 0.22, 0.07]),\n            \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n            \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n            \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n        }\n\n        # Define constraints manager.\n        constraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"7\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n\n        # Run clustering.\n        dict_of_predicted_clusters = clustering_model.cluster(\n            constraints_manager=constraints_manager,\n            vectors=vectors,\n            nb_clusters=3,\n        )\n\n        # Print results.\n        print(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 1, \"3\": 1, \"4\": 2, \"5\": 2, \"6\": 0, \"7\": 0, \"8\": 0,})\n        print(\"Computed results\", \":\", dict_of_predicted_clusters)\n        ```\n    \"\"\"\n\n    # ==============================================================================\n    # INITIALIZATION\n    # ==============================================================================\n    def __init__(\n        self,\n        model: str = \"COP\",\n        max_iteration: int = 150,\n        tolerance: float = 1e-4,\n        random_seed: Optional[int] = None,\n        **kargs,\n    ) -&gt; None:\n        \"\"\"\n        The constructor for KMeans Constrainted Clustering class.\n\n        Args:\n            model (str, optional): The kmeans clustering model to use. Available kmeans models are `\"COP\"`. Defaults to `\"COP\"`.\n            max_iteration (int, optional): The maximum number of kmeans iteration for convergence. Defaults to `150`.\n            tolerance (float, optional): The tolerance for convergence computation. Defaults to `1e-4`.\n            random_seed (Optional[int]): The random seed to use to redo the same clustering. Defaults to `None`.\n            **kargs (dict): Other parameters that can be used in the instantiation.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set.\n        \"\"\"\n\n        # Store `self.`model`.\n        if model != \"COP\":  # TODO use `not in {\"COP\"}`.\n            raise ValueError(\"The `model` '\" + str(model) + \"' is not implemented.\")\n        self.model: str = model\n\n        # Store 'self.max_iteration`.\n        if max_iteration &lt; 1:\n            raise ValueError(\"The `max_iteration` must be greater than or equal to 1.\")\n        self.max_iteration: int = max_iteration\n\n        # Store `self.tolerance`.\n        if tolerance &lt; 0:\n            raise ValueError(\"The `tolerance` must be greater than 0.0.\")\n        self.tolerance: float = tolerance\n\n        # Store `self.random_seed`.\n        self.random_seed: Optional[int] = random_seed\n\n        # Store `self.kargs` for kmeans clustering.\n        self.kargs = kargs\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n\n    # ==============================================================================\n    # MAIN - CLUSTER DATA\n    # ==============================================================================\n    def cluster(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        vectors: Dict[str, csr_matrix],\n        nb_clusters: Optional[int],\n        verbose: bool = False,\n        **kargs,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        The main method used to cluster data with the KMeans model.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n            vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n            nb_clusters (Optional[int]): The number of clusters to compute.  #TODO Set defaults to None with elbow method or other method ?\n            verbose (bool, optional): Enable verbose output. Defaults to `False`.\n            **kargs (dict): Other parameters that can be used in the clustering.\n\n        Raises:\n            ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        ###\n        ### GET PARAMETERS\n        ###\n\n        # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n        if not isinstance(constraints_manager, AbstractConstraintsManager):\n            raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n        self.constraints_manager: AbstractConstraintsManager = constraints_manager\n        self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n        # Store `self.vectors`.\n        if not isinstance(vectors, dict):\n            raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n        self.vectors: Dict[str, csr_matrix] = vectors\n\n        # Store `self.nb_clusters`.\n        if (nb_clusters is None) or (nb_clusters &lt; 2):\n            raise ValueError(\"The `nb_clusters` '\" + str(nb_clusters) + \"' must be greater than or equal to 2.\")\n        self.nb_clusters: int = min(nb_clusters, len(self.list_of_data_IDs))\n\n        ###\n        ### RUN KMEANS CONSTRAINED CLUSTERING\n        ###\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters = None\n\n        # Case of `\"COP\"` KMeans clustering.\n        ##### DEFAULTS : if self.model==\"COP\":\n        self.dict_of_predicted_clusters = self._clustering_kmeans_model_COP(verbose=verbose)\n\n        ###\n        ### RETURN PREDICTED CLUSTERS\n        ###\n\n        return self.dict_of_predicted_clusters\n\n    # ==============================================================================\n    # IMPLEMENTATION - COP KMEANS CLUSTERING\n    # ==============================================================================\n    def _clustering_kmeans_model_COP(self, verbose: bool = False) -&gt; Dict[str, int]:\n        \"\"\"\n        Implementation of COP-kmeans algorithm, based on constraints violation check during cluster affectation.\n\n        References:\n            - Constrained _'COP'_ KMeans Clustering: `Wagstaff, K., C. Cardie, S. Rogers, et S. Schroedl (2001). Constrained K-means Clustering with Background Knowledge. International Conference on Machine Learning`\n\n        Args:\n            verbose (bool, optional): Enable verbose output. Defaults is `False`.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        ###\n        ### INITIALIZATION OF CENTROIDS\n        ###\n\n        # Initialize `self.centroids`.\n        self.centroids: Dict[int, csr_matrix] = self.initialize_centroids()\n\n        # Initialize clusters\n        self.clusters: Dict[str, int] = {data_ID: -1 for data_ID in self.list_of_data_IDs}\n\n        ###\n        ### RUN KMEANS UNTIL CONVERGENCE\n        ###\n\n        # Define variable of convergence checks.\n        converged: bool = False\n        shift: float = float(\"Inf\")\n        current_iteration: int = 0\n\n        # While no convergence\n        while (not converged) and (current_iteration &lt; self.max_iteration):\n            # Increase current iteration number.\n            current_iteration += 1\n\n            ###\n            ### ASSIGN DATA TO THE CLOSEST CLUSTER WITH CONSTRAINTS RESPECT\n            ###\n\n            # Initialize temporary results.\n            new_clusters: Dict[str, int] = {data_ID: -1 for data_ID in self.list_of_data_IDs}\n\n            # For each data ID.\n            list_of_data_IDs_to_assign: List[str] = self.list_of_data_IDs.copy()\n\n            # Precompute pairwise distances between data IDs and clusters centroids.\n            matrix_of_pairwise_distances_between_data_and_clusters: csr_matrix = pairwise_distances(\n                X=vstack(  # Vectors of data IDs.\n                    self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()\n                ),\n                Y=vstack(  # Vectors of cluster centroids.\n                    centroid_vector for centroid_vector in self.centroids.values()\n                ),\n                metric=\"euclidean\",  # TODO get different pairwise_distances config in **kargs\n            )\n\n            # Format pairwise distances in a dictionary.\n            dict_of_pairwise_distances_between_data_and_clusters: Dict[str, Dict[int, float]] = {\n                data_ID: {\n                    cluster_ID: float(matrix_of_pairwise_distances_between_data_and_clusters[i_data, i_cluster])\n                    for i_cluster, cluster_ID in enumerate(self.centroids.keys())\n                }\n                for i_data, data_ID in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n            }\n\n            # While all data aren't assigned.\n            while list_of_data_IDs_to_assign:\n                # Get a data_ID to assign\n                data_ID_to_assign: str = list_of_data_IDs_to_assign.pop()\n\n                # Get the list of not compatible cluster IDs for assignation\n                not_compatible_cluster_IDs: List[int] = [\n                    new_clusters[data_ID]\n                    for data_ID in self.list_of_data_IDs\n                    if (new_clusters[data_ID] != -1)\n                    and (\n                        self.constraints_manager.get_inferred_constraint(\n                            data_ID1=data_ID_to_assign,\n                            data_ID2=data_ID,\n                        )\n                        == \"CANNOT_LINK\"\n                    )\n                ]\n\n                # Get the list of possible cluster IDs for assignation.\n                possible_cluster_IDs: List[int] = [\n                    cluster_ID for cluster_ID in self.centroids.keys() if cluster_ID not in not_compatible_cluster_IDs\n                ]\n\n                # If there is no possible cluster...\n                if possible_cluster_IDs == []:  # noqa: WPS520\n                    # Assign the data ID to a new cluster\n                    new_clusters[data_ID_to_assign] = max(\n                        max([cluster_ID for _, cluster_ID in new_clusters.items()]) + 1, self.nb_clusters\n                    )\n\n                # If there is possible clusters...\n                else:\n                    # Get distance between data ID and all possible centroids.\n                    distances_to_cluster_ID: Dict[float, int] = {\n                        dict_of_pairwise_distances_between_data_and_clusters[data_ID_to_assign][cluster_ID]: cluster_ID\n                        for cluster_ID in possible_cluster_IDs\n                    }\n\n                    # Get the clostest cluster to data ID by distance.\n                    min_distance: float = min(distances_to_cluster_ID)\n                    new_clusters[data_ID_to_assign] = distances_to_cluster_ID[min_distance]\n\n                # Assign all data ID that are linked by a `\"MUST_LINK\"` constraint to the new cluster.\n                for data_ID in self.list_of_data_IDs:\n                    if (\n                        self.constraints_manager.get_inferred_constraint(\n                            data_ID1=data_ID_to_assign,\n                            data_ID2=data_ID,\n                        )\n                        == \"MUST_LINK\"\n                    ):\n                        if data_ID in list_of_data_IDs_to_assign:\n                            list_of_data_IDs_to_assign.remove(data_ID)\n                        new_clusters[data_ID] = new_clusters[data_ID_to_assign]\n\n            ###\n            ### COMPUTE NEW CENTROIDS\n            ###\n\n            # Compute new centroids\n            new_centroids: Dict[int, csr_matrix] = self.compute_centroids(clusters=new_clusters)\n\n            ###\n            ### CHECK CONVERGENCE\n            ###\n\n            # Check by centroids difference (with tolerance).\n            if set(self.clusters.values()) == set(new_clusters.values()):\n                # Precompute distance between old and new cluster centroids.\n                matrix_of_pairwise_distances_between_old_and_new_clusters: csr_matrix = pairwise_distances(\n                    X=vstack(  # Old clusters centroids.\n                        self.centroids[cluster_ID] for cluster_ID in self.centroids.keys()\n                    ),\n                    Y=vstack(  # New clusters centroids.\n                        new_centroids[cluster_ID] for cluster_ID in self.centroids.keys()\n                    ),\n                    metric=\"euclidean\",  # TODO get different pairwise_distances config in **kargs\n                )\n\n                # Compute shift between kmeans iterations.\n                shift = sum(\n                    matrix_of_pairwise_distances_between_old_and_new_clusters[i][i]\n                    for i in range(matrix_of_pairwise_distances_between_old_and_new_clusters.shape[0])\n                )\n\n                # If shift is under tolerance : convergence !\n                converged = shift &lt; self.tolerance\n\n            # Check if number of clusters have changed.\n            else:\n                # Uncomparable shift.\n                shift = float(\"Inf\")\n\n                # Don't converge.\n                converged = False\n\n            ###\n            ### APPLY NEW COMPUTATIONS\n            ###\n\n            # Affect new clusters.\n            self.clusters = new_clusters\n\n            # Affect new centroids.\n            self.centroids = new_centroids\n\n            # Verbose.\n            if verbose and (current_iteration % 5 == 0):  # pragma: no cover\n                # Verbose - Print progression status.\n                print(\"    CLUSTERING_ITERATION=\" + str(current_iteration), \",\", \"shift=\" + str(shift))\n\n        # Verbose.\n        if verbose:  # pragma: no cover\n            # Verbose - Print progression status.\n            print(\"    CLUSTERING_ITERATION=\" + str(current_iteration), \",\", \"converged=\" + str(converged))\n\n        # Rename cluster IDs by order.\n        self.clusters = rename_clusters_by_order(clusters=self.clusters)\n        self.centroids = self.compute_centroids(clusters=new_clusters)\n\n        return self.clusters\n\n    # ==============================================================================\n    # INITIALIZATION OF CLUSTERS\n    # ==============================================================================\n    def initialize_centroids(\n        self,\n    ) -&gt; Dict[int, csr_matrix]:\n        \"\"\"\n        Initialize the centroid of each cluster by a vector.\n        The choice is based on a random selection among data to cluster.\n\n        Returns:\n            Dict[int, csr_matrix]: A dictionary which represent each cluster by a centroid.\n        \"\"\"\n\n        # Get the list of possible indices.\n        indices: List[str] = self.list_of_data_IDs.copy()\n\n        # Shuffle the list of possible indices.\n        random.seed(self.random_seed)\n        random.shuffle(indices)\n\n        # Subset indices.\n        indices = indices[: self.nb_clusters]\n\n        # Set initial centroids based on vectors.\n        centroids: Dict[int, csr_matrix] = {\n            cluster_ID: self.vectors[indices[cluster_ID]] for cluster_ID in range(self.nb_clusters)\n        }\n\n        # Return centroids.\n        return centroids\n\n    # ==============================================================================\n    # COMPUTE NEW CENTROIDS\n    # ==============================================================================\n    def compute_centroids(\n        self,\n        clusters: Dict[str, int],\n    ) -&gt; Dict[int, csr_matrix]:\n        \"\"\"\n        Compute the centroids of each cluster.\n\n        Args:\n            clusters (Dict[str,int]): Current clusters assignation.\n\n        Returns:\n            Dict[int, csr_matrix]: A dictionary which represent each cluster by a centroid.\n        \"\"\"\n\n        # Initialize centroids.\n        centroids: Dict[int, csr_matrix] = {}\n\n        # For all possible cluster ID.\n        for cluster_ID in set(clusters.values()):\n            # Compute cluster members.\n            members_of_cluster_ID: List[csr_matrix] = [\n                self.vectors[data_ID]\n                for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()\n                if clusters[data_ID] == cluster_ID\n            ]\n\n            # Compute centroid.\n            centroid_for_cluster_ID: csr_matrix = sum(members_of_cluster_ID) / len(members_of_cluster_ID)\n\n            # Add centroids.\n            centroids[cluster_ID] = centroid_for_cluster_ID\n\n        # Return centroids.\n        return centroids\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering.__init__","title":"<code>__init__(model='COP', max_iteration=150, tolerance=0.0001, random_seed=None, **kargs)</code>","text":"<p>The constructor for KMeans Constrainted Clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The kmeans clustering model to use. Available kmeans models are <code>\"COP\"</code>. Defaults to <code>\"COP\"</code>.</p> <code>'COP'</code> <code>max_iteration</code> <code>int</code> <p>The maximum number of kmeans iteration for convergence. Defaults to <code>150</code>.</p> <code>150</code> <code>tolerance</code> <code>float</code> <p>The tolerance for convergence computation. Defaults to <code>1e-4</code>.</p> <code>0.0001</code> <code>random_seed</code> <code>Optional[int]</code> <p>The random seed to use to redo the same clustering. Defaults to <code>None</code>.</p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\kmeans.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"COP\",\n    max_iteration: int = 150,\n    tolerance: float = 1e-4,\n    random_seed: Optional[int] = None,\n    **kargs,\n) -&gt; None:\n    \"\"\"\n    The constructor for KMeans Constrainted Clustering class.\n\n    Args:\n        model (str, optional): The kmeans clustering model to use. Available kmeans models are `\"COP\"`. Defaults to `\"COP\"`.\n        max_iteration (int, optional): The maximum number of kmeans iteration for convergence. Defaults to `150`.\n        tolerance (float, optional): The tolerance for convergence computation. Defaults to `1e-4`.\n        random_seed (Optional[int]): The random seed to use to redo the same clustering. Defaults to `None`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set.\n    \"\"\"\n\n    # Store `self.`model`.\n    if model != \"COP\":  # TODO use `not in {\"COP\"}`.\n        raise ValueError(\"The `model` '\" + str(model) + \"' is not implemented.\")\n    self.model: str = model\n\n    # Store 'self.max_iteration`.\n    if max_iteration &lt; 1:\n        raise ValueError(\"The `max_iteration` must be greater than or equal to 1.\")\n    self.max_iteration: int = max_iteration\n\n    # Store `self.tolerance`.\n    if tolerance &lt; 0:\n        raise ValueError(\"The `tolerance` must be greater than 0.0.\")\n    self.tolerance: float = tolerance\n\n    # Store `self.random_seed`.\n    self.random_seed: Optional[int] = random_seed\n\n    # Store `self.kargs` for kmeans clustering.\n    self.kargs = kargs\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering.cluster","title":"<code>cluster(constraints_manager, vectors, nb_clusters, verbose=False, **kargs)</code>","text":"<p>The main method used to cluster data with the KMeans model.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs that will force clustering to respect some conditions during computation.</p> required <code>vectors</code> <code>Dict[str, csr_matrix]</code> <p>The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data.</p> required <code>nb_clusters</code> <code>Optional[int]</code> <p>The number of clusters to compute.  #TODO Set defaults to None with elbow method or other method ?</p> required <code>verbose</code> <code>bool</code> <p>Enable verbose output. Defaults to <code>False</code>.</p> <code>False</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the clustering.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>vectors</code> and <code>constraints_manager</code> are incompatible, or if some parameters are incorrectly set.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\kmeans.py</code> <pre><code>def cluster(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    vectors: Dict[str, csr_matrix],\n    nb_clusters: Optional[int],\n    verbose: bool = False,\n    **kargs,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    The main method used to cluster data with the KMeans model.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n        vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n        nb_clusters (Optional[int]): The number of clusters to compute.  #TODO Set defaults to None with elbow method or other method ?\n        verbose (bool, optional): Enable verbose output. Defaults to `False`.\n        **kargs (dict): Other parameters that can be used in the clustering.\n\n    Raises:\n        ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n    Returns:\n        Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n\n    ###\n    ### GET PARAMETERS\n    ###\n\n    # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n    if not isinstance(constraints_manager, AbstractConstraintsManager):\n        raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n    self.constraints_manager: AbstractConstraintsManager = constraints_manager\n    self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n    # Store `self.vectors`.\n    if not isinstance(vectors, dict):\n        raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n    self.vectors: Dict[str, csr_matrix] = vectors\n\n    # Store `self.nb_clusters`.\n    if (nb_clusters is None) or (nb_clusters &lt; 2):\n        raise ValueError(\"The `nb_clusters` '\" + str(nb_clusters) + \"' must be greater than or equal to 2.\")\n    self.nb_clusters: int = min(nb_clusters, len(self.list_of_data_IDs))\n\n    ###\n    ### RUN KMEANS CONSTRAINED CLUSTERING\n    ###\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters = None\n\n    # Case of `\"COP\"` KMeans clustering.\n    ##### DEFAULTS : if self.model==\"COP\":\n    self.dict_of_predicted_clusters = self._clustering_kmeans_model_COP(verbose=verbose)\n\n    ###\n    ### RETURN PREDICTED CLUSTERS\n    ###\n\n    return self.dict_of_predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering.compute_centroids","title":"<code>compute_centroids(clusters)</code>","text":"<p>Compute the centroids of each cluster.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>Dict[str, int]</code> <p>Current clusters assignation.</p> required <p>Returns:</p> Type Description <code>Dict[int, csr_matrix]</code> <p>Dict[int, csr_matrix]: A dictionary which represent each cluster by a centroid.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\kmeans.py</code> <pre><code>def compute_centroids(\n    self,\n    clusters: Dict[str, int],\n) -&gt; Dict[int, csr_matrix]:\n    \"\"\"\n    Compute the centroids of each cluster.\n\n    Args:\n        clusters (Dict[str,int]): Current clusters assignation.\n\n    Returns:\n        Dict[int, csr_matrix]: A dictionary which represent each cluster by a centroid.\n    \"\"\"\n\n    # Initialize centroids.\n    centroids: Dict[int, csr_matrix] = {}\n\n    # For all possible cluster ID.\n    for cluster_ID in set(clusters.values()):\n        # Compute cluster members.\n        members_of_cluster_ID: List[csr_matrix] = [\n            self.vectors[data_ID]\n            for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()\n            if clusters[data_ID] == cluster_ID\n        ]\n\n        # Compute centroid.\n        centroid_for_cluster_ID: csr_matrix = sum(members_of_cluster_ID) / len(members_of_cluster_ID)\n\n        # Add centroids.\n        centroids[cluster_ID] = centroid_for_cluster_ID\n\n    # Return centroids.\n    return centroids\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering.initialize_centroids","title":"<code>initialize_centroids()</code>","text":"<p>Initialize the centroid of each cluster by a vector. The choice is based on a random selection among data to cluster.</p> <p>Returns:</p> Type Description <code>Dict[int, csr_matrix]</code> <p>Dict[int, csr_matrix]: A dictionary which represent each cluster by a centroid.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\kmeans.py</code> <pre><code>def initialize_centroids(\n    self,\n) -&gt; Dict[int, csr_matrix]:\n    \"\"\"\n    Initialize the centroid of each cluster by a vector.\n    The choice is based on a random selection among data to cluster.\n\n    Returns:\n        Dict[int, csr_matrix]: A dictionary which represent each cluster by a centroid.\n    \"\"\"\n\n    # Get the list of possible indices.\n    indices: List[str] = self.list_of_data_IDs.copy()\n\n    # Shuffle the list of possible indices.\n    random.seed(self.random_seed)\n    random.shuffle(indices)\n\n    # Subset indices.\n    indices = indices[: self.nb_clusters]\n\n    # Set initial centroids based on vectors.\n    centroids: Dict[int, csr_matrix] = {\n        cluster_ID: self.vectors[indices[cluster_ID]] for cluster_ID in range(self.nb_clusters)\n    }\n\n    # Return centroids.\n    return centroids\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/mpckmeans/","title":"mpckmeans","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.clustering.mpckmeans</li> <li>Description:  Implementation of constrained mpckmeans clustering algorithms.</li> <li>Author:       Esther LENOTRE, David NICOLAZO, Marc TRUTT</li> <li>Created:      10/09/2022</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/mpckmeans/#cognitivefactory.interactive_clustering.clustering.mpckmeans.MPCKMeansConstrainedClustering","title":"<code>MPCKMeansConstrainedClustering</code>","text":"<p>             Bases: <code>AbstractConstrainedClustering</code></p> <p>This class implements the MPCkmeans constrained clustering. It inherits from <code>AbstractConstrainedClustering</code>.</p> <p>Forked from https://github.com/Behrouz-Babaki/COP-Kmeans/blob/master/copkmeans/cop_kmeans.py Modified by Esther LENOTRE git@estherlenotre.fr according to https://proceedings.mlr.press/v5/givoni09a.html</p> References <ul> <li>KMeans Clustering: <code>MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297.</code></li> <li>Constrained 'MPC' KMeans Clustering: <code>Khan, Md. A., Tamim, I., Ahmed, E., &amp; Awal, M. A. (2012). Multiple Parameter Based Clustering (MPC): Prospective Analysis for Effective Clustering in Wireless Sensor Network (WSN) Using K-Means Algorithm. In Wireless Sensor Network (Vol. 04, Issue 01, pp. 18\u201324). Scientific Research Publishing, Inc. https://doi.org/10.4236/wsn.2012.41003</code></li> </ul> Example <pre><code># Import.\nfrom scipy.sparse import csr_matrix\nfrom cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\nfrom cognitivefactory.interactive_clustering.clustering.dbscan import MPCKMeansConstrainedClustering\n\n# NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\n\nvectors = {\n    \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n    \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n    \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n    \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n    \"4\": csr_matrix([0.50, 0.22, 0.21, 0.07]),\n    \"5\": csr_matrix([0.50, 0.21, 0.22, 0.07]),\n    \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n    \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n    \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n}\n\n# Define constraints manager.\nconstraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"7\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n\ncluster_model = MPCKMeansConstrainedClustering()\ndict_of_predicted_clusters = cluster_model.cluster(\n    constraints_manager=constraints_manager,\n    vectors=vectors,\n    nb_clusters=3,\n)\n\n# Print results.\nprint(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 1, \"3\": 1, \"4\": 2, \"5\": 2, \"6\": 0, \"7\": 0, \"8\": 0,})\nprint(\"Computed results\", \":\", dict_of_predicted_clusters)\n</code></pre> <p>Warns:</p> Type Description <code>FutureWarning</code> <p><code>clustering.mpckmeans.MPCKMeansConstrainedClustering</code> is still in development and is not fully tested : it is not ready for production use.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\mpckmeans.py</code> <pre><code>class MPCKMeansConstrainedClustering(AbstractConstrainedClustering):\n    \"\"\"\n    This class implements the MPCkmeans constrained clustering.\n    It inherits from `AbstractConstrainedClustering`.\n\n    Forked from https://github.com/Behrouz-Babaki/COP-Kmeans/blob/master/copkmeans/cop_kmeans.py\n    Modified by Esther LENOTRE &lt;git@estherlenotre.fr&gt; according to https://proceedings.mlr.press/v5/givoni09a.html\n\n    References:\n        - KMeans Clustering: `MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297.`\n        - Constrained _'MPC'_ KMeans Clustering: `Khan, Md. A., Tamim, I., Ahmed, E., &amp; Awal, M. A. (2012). Multiple Parameter Based Clustering (MPC): Prospective Analysis for Effective Clustering in Wireless Sensor Network (WSN) Using K-Means Algorithm. In Wireless Sensor Network (Vol. 04, Issue 01, pp. 18\u201324). Scientific Research Publishing, Inc. https://doi.org/10.4236/wsn.2012.41003`\n\n    Example:\n        ```python\n        # Import.\n        from scipy.sparse import csr_matrix\n        from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\n        from cognitivefactory.interactive_clustering.clustering.dbscan import MPCKMeansConstrainedClustering\n\n        # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\n\n        vectors = {\n            \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n            \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n            \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n            \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n            \"4\": csr_matrix([0.50, 0.22, 0.21, 0.07]),\n            \"5\": csr_matrix([0.50, 0.21, 0.22, 0.07]),\n            \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n            \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n            \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n        }\n\n        # Define constraints manager.\n        constraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"7\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n\n        cluster_model = MPCKMeansConstrainedClustering()\n        dict_of_predicted_clusters = cluster_model.cluster(\n            constraints_manager=constraints_manager,\n            vectors=vectors,\n            nb_clusters=3,\n        )\n\n        # Print results.\n        print(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 1, \"3\": 1, \"4\": 2, \"5\": 2, \"6\": 0, \"7\": 0, \"8\": 0,})\n        print(\"Computed results\", \":\", dict_of_predicted_clusters)\n        ```\n\n    Warns:\n        FutureWarning: `clustering.mpckmeans.MPCKMeansConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n    \"\"\"\n\n    # ==============================================================================\n    # INITIALIZATION\n    # ==============================================================================\n    def __init__(\n        self,\n        model: str = \"MPC\",\n        max_iteration: int = 150,\n        w: float = 1.0,\n        random_seed: Optional[int] = None,\n        **kargs,\n    ) -&gt; None:\n        \"\"\"\n        The constructor for MPCKMeans Constrainted Clustering class.\n\n        Args:\n            model (str, optional): The kmeans clustering model to use. Available kmeans models are `\"MPC\"`. Defaults to `\"MPC\"`.\n            max_iteration (int, optional): The maximum number of kmeans iteration for convergence. Defaults to `150`.\n            w (float, optional): Weight for the constraints\n            random_seed (Optional[int]): The random seed to use to redo the same clustering. Defaults to `None`.\n            **kargs (dict): Other parameters that can be used in the instantiation.\n\n        Warns:\n            FutureWarning: `clustering.mpckmeans.MPCKMeansConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set.\n        \"\"\"\n\n        # Deprecation warnings\n        warnings.warn(\n            \"`clustering.mpckmeans.MPCKMeansConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\",\n            FutureWarning,  # DeprecationWarning\n            stacklevel=2,\n        )\n\n        # Store `self.`model`.\n        if model != \"MPC\":  # TODO use `not in {\"MPC\"}`.\n            raise ValueError(\"The `model` '\" + str(model) + \"' is not implemented.\")\n        self.model: str = model\n\n        # Store 'self.max_iteration`.\n        if max_iteration &lt; 1:\n            raise ValueError(\"The `max_iteration` must be greater than or equal to 1.\")\n        self.max_iteration: int = max_iteration\n\n        # Store `self.weight`.\n        if w &lt; 0:\n            raise ValueError(\"The `weight` must be greater than 0.0.\")\n        self.w: float = w\n\n        # Store `self.random_seed`.\n        self.random_seed: Optional[int] = random_seed\n\n        # Store `self.kargs` for clustering.\n        self.kargs = kargs\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n\n        # Initialize `ml_graph` and `cl_graph`.\n        self.ml_graph: Dict[str, List[str]] = {}\n        self.cl_graph: Dict[str, List[str]] = {}\n\n    # ==============================================================================\n    # MAIN - CLUSTER DATA\n    # ==============================================================================\n\n    def cluster(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        vectors: Dict[str, csr_matrix],\n        nb_clusters: Optional[int],\n        verbose: bool = False,\n        y=None,\n        **kargs,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        The main method used to cluster data with the KMeans model.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n            vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n            nb_clusters (Optional[int]): The number of clusters to compute. Here None.\n            verbose (bool, optional): Enable verbose output. Defaults to `False`.\n            y : Something.\n            **kargs (dict): Other parameters that can be used in the clustering.\n\n        Raises:\n            ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        ###\n        ### GET PARAMETERS\n        ###\n\n        # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n        if not isinstance(constraints_manager, AbstractConstraintsManager):\n            raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n        self.constraints_manager: AbstractConstraintsManager = constraints_manager\n        self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n        # Store `self.vectors`.\n        if not isinstance(vectors, dict):\n            raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n        self.vectors: Dict[str, csr_matrix] = vectors\n\n        # Store `self.nb_clusters`.\n        if (nb_clusters is None) or (nb_clusters &lt; 2):\n            raise ValueError(\"The `nb_clusters` '\" + str(nb_clusters) + \"' must be greater than or equal to 2.\")\n        self.nb_clusters: int = min(nb_clusters, len(self.list_of_data_IDs))\n\n        # TODO: Reformat vectors\n        id_names: np.ndarray = np.array(list(vectors.keys()))\n        X: np.ndarray = np.array(\n            [\n                (np.array(v).flatten() if isinstance(v, (np.ndarray, list)) else v.toarray().flatten())\n                for v in vectors.values()\n            ]\n        )\n\n        # TODO: reformat constraints\n        self.ml: List[Tuple[int, int]] = [\n            (i, j)\n            for i, data_ID_i in enumerate(self.list_of_data_IDs)\n            for j, data_ID_j in enumerate(self.list_of_data_IDs)\n            if (\n                self.constraints_manager.get_inferred_constraint(\n                    data_ID1=data_ID_j,\n                    data_ID2=data_ID_i,\n                )\n                == \"MUST_LINK\"\n            )\n        ]\n\n        # TODO: reformat constraints\n        self.cl: List[Tuple[int, int]] = [\n            (i, j)\n            for i, data_ID_i in enumerate(self.list_of_data_IDs)\n            for j, data_ID_j in enumerate(self.list_of_data_IDs)\n            if (\n                self.constraints_manager.get_inferred_constraint(\n                    data_ID1=data_ID_j,\n                    data_ID2=data_ID_i,\n                )\n                == \"CANNOT_LINK\"\n            )\n        ]\n\n        # Preprocess constraints\n        ml_graph, cl_graph, neighborhoods = self.preprocess_constraints()\n\n        # Initialize cluster centers\n        cluster_centers = self._initialize_cluster_centers(X, neighborhoods)\n\n        # Initialize metrics\n        A = np.identity(X.shape[1])\n\n        iteration = 0\n        converged = False\n\n        # Repeat until convergence\n        while not converged and iteration &lt; self.max_iteration:\n            prev_cluster_centers = cluster_centers.copy()\n\n            # Find farthest pair of points according to each metric\n            farthest = self._find_farthest_pairs_of_points(X, A)\n\n            # Assign clusters\n            labels = self._assign_clusters(X, y, cluster_centers, A, farthest, ml_graph, cl_graph, self.w)\n\n            # Estimate means\n            cluster_centers = self._get_cluster_centers(X, labels)\n\n            # Update metrics\n            A = self._update_metrics(X, labels, cluster_centers, farthest, ml_graph, cl_graph, self.w)\n\n            # Check for convergence\n            cluster_centers_shift = prev_cluster_centers - cluster_centers\n            converged = np.allclose(cluster_centers_shift, np.zeros(cluster_centers.shape), atol=1e-6, rtol=0)\n            iteration += 1\n\n        self.cluster_centers = cluster_centers\n        self.labels = labels\n\n        self.dict_of_predicted_clusters = {id_names[i]: self.labels[i] for i, label in enumerate(labels) if label != -1}\n\n        self.dict_of_predicted_clusters = rename_clusters_by_order(clusters=self.dict_of_predicted_clusters)\n\n        for data_ID in self.list_of_data_IDs:\n            if data_ID not in self.dict_of_predicted_clusters.keys():\n                self.dict_of_predicted_clusters[data_ID] = -1\n\n        return self.dict_of_predicted_clusters\n\n    # ==============================================================================\n    # USEFUL FUNCTIONS\n    # ==============================================================================\n\n    def dist(self, i: int, S: List[int], points: np.ndarray) -&gt; float:\n        \"\"\"\n        Computes the minimum distance of a single point to a group of points.\n\n        Args:\n            i (int): Index of the single point.\n            S (List[int]): List of the index of the group of points .\n            points (np.ndarray): Array containing all the points.\n\n        Returns:\n            float: Minimum distance of the single to the group of points.\n        \"\"\"\n\n        distances = np.array([np.sqrt(((points[i] - points[j]) ** 2).sum()) for j in S])\n        return distances.min()\n\n    def _dist(self, x: np.ndarray, y: np.ndarray, A: np.ndarray) -&gt; float:\n        \"\"\"\n        Computes the Mahalanobis distance between two points.\n        \"(x - y)^T A (x - y)\"\n\n        Args:\n            x (np.ndarray): First point.\n            y (np.ndarray): Second point .\n            A (np.ndarray): Inverse of a covariance matrix.\n\n        Returns:\n            float: Minimum distance of the single to the group of points.\n        \"\"\"\n\n        return scipy.spatial.distance.mahalanobis(x, y, A) ** 2\n\n    def _find_farthest_pairs_of_points(self, X: np.ndarray, A: np.ndarray) -&gt; Tuple[int, int, float]:\n        \"\"\"\n        Finds the farthest pair of points.\n\n        Args:\n            X (np.ndarray): Set of points.\n            A (np.ndarray): Positive-definite matrix used for the distances.\n\n        Returns:\n            Tuple[int, int, float]: Indexes of the farthest pair of points and the corresponding distance.\n        \"\"\"\n\n        farthest = None\n        n = X.shape[0]\n        max_distance = 0.0\n\n        for i in range(n):\n            for j in range(n):\n                if j &lt; i:\n                    distance = self._dist(X[i], X[j], A)\n                    if distance &gt; max_distance:\n                        max_distance = distance\n                        farthest = (i, j, distance)\n\n        assert farthest is not None\n\n        return farthest\n\n    def weighted_farthest_first_traversal(self, points: np.ndarray, weights: np.ndarray, k: int) -&gt; List[int]:\n        \"\"\"\n        Applies weighted farthest first traversal algorithm.\n\n        Args:\n            points (np.ndarray): Set of points.\n            weights (np.ndarray): Weights for the distances.\n            k (int): Number of points to be traversed\n\n        Returns:\n            List[int]: Indexes of the traversed points.\n        \"\"\"\n        traversed = []\n\n        # Choose the first point randomly (weighted)\n        i = np.random.choice(len(points), size=1, p=weights)[0]\n        traversed.append(i)\n\n        # Find remaining n - 1 maximally separated points\n        for _ in range(k - 1):\n            max_dst, max_dst_index = 0, None\n\n            number_of_points = len(points)\n            for j in range(number_of_points):\n                if j not in traversed:\n                    dst = self.dist(j, traversed, points)\n                    weighted_dst = weights[j] * dst\n\n                    if weighted_dst &gt; max_dst:\n                        max_dst = weighted_dst\n                        max_dst_index = j\n\n            traversed.append(max_dst_index)\n\n        return traversed\n\n    # ==============================================================================\n    # INITIALIZATION OF CLUSTERS\n    # ==============================================================================\n    def _add_both(self, d, i, j):\n        d[i].add(j)\n        d[j].add(i)\n\n    def _dfs(self, i, graph, visited, component):\n        visited[i] = True\n        for j in graph[i]:\n            if not visited[j]:\n                self._dfs(j, graph, visited, component)\n        component.append(i)\n\n    def preprocess_constraints(\n        self,\n    ) -&gt; Tuple[Dict[int, Set[int]], Dict[int, Set[int]], List[List[int]]]:\n        \"\"\"\n        Initialize each cluster.\n        The choice is based on the neighborhoods created by the initial constraints.\n\n        Raises:\n            ValueError: if there is a Cannot-link constraint in conflict with a Must-link constraint involving both one same point.\n\n        Returns:\n            Tuple[Dict[int, set], Dict[int, set], List[List[int]]]:\n            A new list of must-link and cannot-link constraints as well as the lambda starting neighborhoods.\n        \"\"\"\n\n        # Get the list of possible indices.\n        indices: List[str] = self.list_of_data_IDs.copy()\n\n        n: int = len(indices)\n\n        # Represent the graphs using adjacency-lists.\n        ml_graph: Dict[int, Set[int]] = {}\n        cl_graph: Dict[int, Set[int]] = {}\n\n        for k in range(n):\n            ml_graph[k] = set()\n            cl_graph[k] = set()\n\n        for data_ID_i1, data_ID_j1 in self.ml:\n            ml_graph[data_ID_i1].add(data_ID_j1)\n            ml_graph[data_ID_j1].add(data_ID_i1)\n\n        for data_ID_i2, data_ID_j2 in self.cl:\n            cl_graph[data_ID_i2].add(data_ID_j2)\n            cl_graph[data_ID_j2].add(data_ID_i2)\n\n        visited = [False for _ in range(n)]\n        neighborhoods = []\n        for index in range(n):\n            if not visited[index] and ml_graph[index]:\n                component: List[int] = []\n                self._dfs(index, ml_graph, visited, component)\n                for x1 in component:\n                    for x2 in component:\n                        if x1 != x2:\n                            ml_graph[x1].add(x2)\n                neighborhoods.append(component)\n\n        for data_ID_i3, data_ID_j3 in self.cl:\n            for x in ml_graph[data_ID_i3]:\n                self._add_both(cl_graph, x, data_ID_j3)\n\n            for y in ml_graph[data_ID_j3]:\n                self._add_both(cl_graph, data_ID_i3, y)\n\n            for a in ml_graph[data_ID_i3]:\n                for b in ml_graph[data_ID_j3]:\n                    self._add_both(cl_graph, a, b)\n\n        for index_1, _ in ml_graph.items():\n            for index_2 in ml_graph[index_1]:\n                if index_2 != index_1 and index_2 in cl_graph[index_1]:\n                    raise ValueError(\"Inconsistent constraints between \" + str(index_1) + \" and \" + str(index_2))\n\n        return ml_graph, cl_graph, neighborhoods\n\n    def _initialize_cluster_centers(self, X: np.ndarray, neighborhoods: List[List[int]]) -&gt; np.ndarray:\n        \"\"\"\n        Initialises cluster centers.\n\n        Args:\n            X (np.ndarray): Set of points.\n            neighborhoods (List[List[int]]): Lists of neighbors for each point.\n\n        Returns:\n            np.ndarray: Computed centers.\n        \"\"\"\n\n        neighborhood_centers = np.array([X[neighborhood].mean(axis=0) for neighborhood in neighborhoods])\n        neighborhood_sizes = np.array([len(neighborhood) for neighborhood in neighborhoods])\n        neighborhood_weights = neighborhood_sizes / neighborhood_sizes.sum()\n\n        # print('\\t', len(neighborhoods), neighborhood_sizes)\n\n        if len(neighborhoods) &gt; self.nb_clusters:\n            cluster_centers = neighborhood_centers[\n                self.weighted_farthest_first_traversal(neighborhood_centers, neighborhood_weights, self.nb_clusters)\n            ]\n        else:\n            if neighborhoods:\n                cluster_centers = neighborhood_centers\n            else:\n                cluster_centers = np.empty((0, X.shape[1]))\n\n            if len(neighborhoods) &lt; self.nb_clusters:\n                remaining_cluster_centers = X[\n                    np.random.choice(X.shape[0], self.nb_clusters - len(neighborhoods), replace=False), :\n                ]\n                cluster_centers = np.concatenate([cluster_centers, remaining_cluster_centers])\n\n        return cluster_centers\n\n    # ==============================================================================\n    # COMPUTE CLUSTERS\n    # ==============================================================================\n\n    def _f_m(self, X: np.ndarray, i: int, j: int, A) -&gt; float:\n        return self._dist(X[i], X[j], A)\n\n    def _f_c(self, X: np.ndarray, i: int, j: int, A, farthest) -&gt; float:\n        return farthest[2] - self._dist(X[i], X[j], A)\n\n    def _objective_fn(\n        self, X: np.ndarray, i: int, labels, cluster_centers, cluster_id, A, farthest, ml_graph, cl_graph, w\n    ) -&gt; float:\n        sign, logdet = np.linalg.slogdet(A)\n        log_det_a = sign * logdet\n\n        if log_det_a == np.inf:\n            log_det_a = 0\n\n        term_d: float = self._dist(X[i], cluster_centers[cluster_id], A) - log_det_a\n\n        term_m: float = 0\n        for j in ml_graph[i]:\n            if labels[j] &gt;= 0 and labels[j] != cluster_id:\n                term_m += 2 * w * self._f_m(X, i, j, A)\n\n        term_c: float = 0\n        for k in cl_graph[i]:\n            if labels[k] == cluster_id:\n                # assert self._f_c(i, k, A, farthest) &gt;= 0\n                term_c += 2 * w * self._f_c(X, i, k, A, farthest)\n\n        return term_d + term_m + term_c\n\n    def _assign_clusters(self, X, y, cluster_centers, A, farthest, ml_graph, cl_graph, w):\n        labels = np.full(X.shape[0], fill_value=-1)\n\n        index = list(range(X.shape[0]))\n        np.random.shuffle(index)\n        for i in index:\n            labels[i] = np.argmin(\n                [\n                    self._objective_fn(X, i, labels, cluster_centers, cluster_id, A, farthest, ml_graph, cl_graph, w)\n                    for cluster_id, cluster_center in enumerate(cluster_centers)\n                ]\n            )\n\n        # Handle empty clusters\n        n_samples_in_cluster = np.bincount(labels, minlength=self.nb_clusters)\n        empty_clusters = np.where(n_samples_in_cluster == 0)[0]\n\n        for empty_cluster_id in empty_clusters:\n            # Get clusters that have at least 2 points and can give one of them to another cluster\n            filled_clusters = np.where(n_samples_in_cluster &gt; 1)[0]\n\n            # Get points from filled_clusters, and compute distance to their center\n            distances_to_clusters: Dict[str, float] = {}\n\n            for cluster_id in filled_clusters:\n                available_cluster_points = np.where(labels == cluster_id)[0]\n                for point in available_cluster_points:\n                    distances_to_clusters[point] = self._dist(X[point], cluster_centers[cluster_id], A)\n\n            # Fill empty clusters with the farthest points regarding their respective centers\n            filling_point: float = max(distances_to_clusters, key=distances_to_clusters.get)\n            labels[filling_point] = empty_cluster_id\n\n            n_samples_in_cluster = np.bincount(labels, minlength=10)\n\n        empty_clusters = np.where(n_samples_in_cluster == 0)[0]\n\n        if empty_clusters.size:\n            # print(\"Empty clusters\")\n            raise ValueError(\"Empty Clusters Exception\")\n        return labels\n\n    def _get_cluster_centers(self, X, labels):\n        return np.array([X[labels == i].mean(axis=0) for i in range(self.nb_clusters)])\n\n    # ==============================================================================\n    # UPDATE METRICS\n    # ==============================================================================\n\n    def _update_metrics(self, X, labels, cluster_centers, farthest, ml_graph, cl_graph, w):\n        N, D = X.shape\n        A = np.zeros((D, D))\n\n        for d in range(D):\n            term_x = np.sum([(x[d] - cluster_centers[labels[i], d]) ** 2 for i, x in enumerate(X)])\n\n            term_m = 0\n            for i in range(N):\n                for j in ml_graph[i]:\n                    if labels[i] != labels[j]:\n                        term_m += 1 / 2 * w * (X[i, d] - X[j, d]) ** 2\n\n            term_c = 0\n            for k in range(N):\n                for m in cl_graph[k]:\n                    if labels[k] == labels[m]:\n                        tmp = (X[farthest[0], d] - X[farthest[1], d]) ** 2 - (X[k, d] - X[m, d]) ** 2\n                        term_c += w * max(tmp, 0)\n\n            # print('term_x', term_x, 'term_m', term_m, 'term_c', term_c)\n\n            A[d, d] = N / max(term_x + term_m + term_c, 1e-9)\n\n        return A\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/mpckmeans/#cognitivefactory.interactive_clustering.clustering.mpckmeans.MPCKMeansConstrainedClustering.__init__","title":"<code>__init__(model='MPC', max_iteration=150, w=1.0, random_seed=None, **kargs)</code>","text":"<p>The constructor for MPCKMeans Constrainted Clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The kmeans clustering model to use. Available kmeans models are <code>\"MPC\"</code>. Defaults to <code>\"MPC\"</code>.</p> <code>'MPC'</code> <code>max_iteration</code> <code>int</code> <p>The maximum number of kmeans iteration for convergence. Defaults to <code>150</code>.</p> <code>150</code> <code>w</code> <code>float</code> <p>Weight for the constraints</p> <code>1.0</code> <code>random_seed</code> <code>Optional[int]</code> <p>The random seed to use to redo the same clustering. Defaults to <code>None</code>.</p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Warns:</p> Type Description <code>FutureWarning</code> <p><code>clustering.mpckmeans.MPCKMeansConstrainedClustering</code> is still in development and is not fully tested : it is not ready for production use.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\mpckmeans.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"MPC\",\n    max_iteration: int = 150,\n    w: float = 1.0,\n    random_seed: Optional[int] = None,\n    **kargs,\n) -&gt; None:\n    \"\"\"\n    The constructor for MPCKMeans Constrainted Clustering class.\n\n    Args:\n        model (str, optional): The kmeans clustering model to use. Available kmeans models are `\"MPC\"`. Defaults to `\"MPC\"`.\n        max_iteration (int, optional): The maximum number of kmeans iteration for convergence. Defaults to `150`.\n        w (float, optional): Weight for the constraints\n        random_seed (Optional[int]): The random seed to use to redo the same clustering. Defaults to `None`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Warns:\n        FutureWarning: `clustering.mpckmeans.MPCKMeansConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set.\n    \"\"\"\n\n    # Deprecation warnings\n    warnings.warn(\n        \"`clustering.mpckmeans.MPCKMeansConstrainedClustering` is still in development and is not fully tested : it is not ready for production use.\",\n        FutureWarning,  # DeprecationWarning\n        stacklevel=2,\n    )\n\n    # Store `self.`model`.\n    if model != \"MPC\":  # TODO use `not in {\"MPC\"}`.\n        raise ValueError(\"The `model` '\" + str(model) + \"' is not implemented.\")\n    self.model: str = model\n\n    # Store 'self.max_iteration`.\n    if max_iteration &lt; 1:\n        raise ValueError(\"The `max_iteration` must be greater than or equal to 1.\")\n    self.max_iteration: int = max_iteration\n\n    # Store `self.weight`.\n    if w &lt; 0:\n        raise ValueError(\"The `weight` must be greater than 0.0.\")\n    self.w: float = w\n\n    # Store `self.random_seed`.\n    self.random_seed: Optional[int] = random_seed\n\n    # Store `self.kargs` for clustering.\n    self.kargs = kargs\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n\n    # Initialize `ml_graph` and `cl_graph`.\n    self.ml_graph: Dict[str, List[str]] = {}\n    self.cl_graph: Dict[str, List[str]] = {}\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/mpckmeans/#cognitivefactory.interactive_clustering.clustering.mpckmeans.MPCKMeansConstrainedClustering.cluster","title":"<code>cluster(constraints_manager, vectors, nb_clusters, verbose=False, y=None, **kargs)</code>","text":"<p>The main method used to cluster data with the KMeans model.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs that will force clustering to respect some conditions during computation.</p> required <code>vectors</code> <code>Dict[str, csr_matrix]</code> <p>The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data.</p> required <code>nb_clusters</code> <code>Optional[int]</code> <p>The number of clusters to compute. Here None.</p> required <code>verbose</code> <code>bool</code> <p>Enable verbose output. Defaults to <code>False</code>.</p> <code>False</code> <code>y</code> <p>Something.</p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the clustering.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>vectors</code> and <code>constraints_manager</code> are incompatible, or if some parameters are incorrectly set.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\mpckmeans.py</code> <pre><code>def cluster(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    vectors: Dict[str, csr_matrix],\n    nb_clusters: Optional[int],\n    verbose: bool = False,\n    y=None,\n    **kargs,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    The main method used to cluster data with the KMeans model.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n        vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n        nb_clusters (Optional[int]): The number of clusters to compute. Here None.\n        verbose (bool, optional): Enable verbose output. Defaults to `False`.\n        y : Something.\n        **kargs (dict): Other parameters that can be used in the clustering.\n\n    Raises:\n        ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n    Returns:\n        Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n\n    ###\n    ### GET PARAMETERS\n    ###\n\n    # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n    if not isinstance(constraints_manager, AbstractConstraintsManager):\n        raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n    self.constraints_manager: AbstractConstraintsManager = constraints_manager\n    self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n    # Store `self.vectors`.\n    if not isinstance(vectors, dict):\n        raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n    self.vectors: Dict[str, csr_matrix] = vectors\n\n    # Store `self.nb_clusters`.\n    if (nb_clusters is None) or (nb_clusters &lt; 2):\n        raise ValueError(\"The `nb_clusters` '\" + str(nb_clusters) + \"' must be greater than or equal to 2.\")\n    self.nb_clusters: int = min(nb_clusters, len(self.list_of_data_IDs))\n\n    # TODO: Reformat vectors\n    id_names: np.ndarray = np.array(list(vectors.keys()))\n    X: np.ndarray = np.array(\n        [\n            (np.array(v).flatten() if isinstance(v, (np.ndarray, list)) else v.toarray().flatten())\n            for v in vectors.values()\n        ]\n    )\n\n    # TODO: reformat constraints\n    self.ml: List[Tuple[int, int]] = [\n        (i, j)\n        for i, data_ID_i in enumerate(self.list_of_data_IDs)\n        for j, data_ID_j in enumerate(self.list_of_data_IDs)\n        if (\n            self.constraints_manager.get_inferred_constraint(\n                data_ID1=data_ID_j,\n                data_ID2=data_ID_i,\n            )\n            == \"MUST_LINK\"\n        )\n    ]\n\n    # TODO: reformat constraints\n    self.cl: List[Tuple[int, int]] = [\n        (i, j)\n        for i, data_ID_i in enumerate(self.list_of_data_IDs)\n        for j, data_ID_j in enumerate(self.list_of_data_IDs)\n        if (\n            self.constraints_manager.get_inferred_constraint(\n                data_ID1=data_ID_j,\n                data_ID2=data_ID_i,\n            )\n            == \"CANNOT_LINK\"\n        )\n    ]\n\n    # Preprocess constraints\n    ml_graph, cl_graph, neighborhoods = self.preprocess_constraints()\n\n    # Initialize cluster centers\n    cluster_centers = self._initialize_cluster_centers(X, neighborhoods)\n\n    # Initialize metrics\n    A = np.identity(X.shape[1])\n\n    iteration = 0\n    converged = False\n\n    # Repeat until convergence\n    while not converged and iteration &lt; self.max_iteration:\n        prev_cluster_centers = cluster_centers.copy()\n\n        # Find farthest pair of points according to each metric\n        farthest = self._find_farthest_pairs_of_points(X, A)\n\n        # Assign clusters\n        labels = self._assign_clusters(X, y, cluster_centers, A, farthest, ml_graph, cl_graph, self.w)\n\n        # Estimate means\n        cluster_centers = self._get_cluster_centers(X, labels)\n\n        # Update metrics\n        A = self._update_metrics(X, labels, cluster_centers, farthest, ml_graph, cl_graph, self.w)\n\n        # Check for convergence\n        cluster_centers_shift = prev_cluster_centers - cluster_centers\n        converged = np.allclose(cluster_centers_shift, np.zeros(cluster_centers.shape), atol=1e-6, rtol=0)\n        iteration += 1\n\n    self.cluster_centers = cluster_centers\n    self.labels = labels\n\n    self.dict_of_predicted_clusters = {id_names[i]: self.labels[i] for i, label in enumerate(labels) if label != -1}\n\n    self.dict_of_predicted_clusters = rename_clusters_by_order(clusters=self.dict_of_predicted_clusters)\n\n    for data_ID in self.list_of_data_IDs:\n        if data_ID not in self.dict_of_predicted_clusters.keys():\n            self.dict_of_predicted_clusters[data_ID] = -1\n\n    return self.dict_of_predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/mpckmeans/#cognitivefactory.interactive_clustering.clustering.mpckmeans.MPCKMeansConstrainedClustering.dist","title":"<code>dist(i, S, points)</code>","text":"<p>Computes the minimum distance of a single point to a group of points.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the single point.</p> required <code>S</code> <code>List[int]</code> <p>List of the index of the group of points .</p> required <code>points</code> <code>ndarray</code> <p>Array containing all the points.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Minimum distance of the single to the group of points.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\mpckmeans.py</code> <pre><code>def dist(self, i: int, S: List[int], points: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes the minimum distance of a single point to a group of points.\n\n    Args:\n        i (int): Index of the single point.\n        S (List[int]): List of the index of the group of points .\n        points (np.ndarray): Array containing all the points.\n\n    Returns:\n        float: Minimum distance of the single to the group of points.\n    \"\"\"\n\n    distances = np.array([np.sqrt(((points[i] - points[j]) ** 2).sum()) for j in S])\n    return distances.min()\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/mpckmeans/#cognitivefactory.interactive_clustering.clustering.mpckmeans.MPCKMeansConstrainedClustering.preprocess_constraints","title":"<code>preprocess_constraints()</code>","text":"<p>Initialize each cluster. The choice is based on the neighborhoods created by the initial constraints.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if there is a Cannot-link constraint in conflict with a Must-link constraint involving both one same point.</p> <p>Returns:</p> Type Description <code>Dict[int, Set[int]]</code> <p>Tuple[Dict[int, set], Dict[int, set], List[List[int]]]:</p> <code>Dict[int, Set[int]]</code> <p>A new list of must-link and cannot-link constraints as well as the lambda starting neighborhoods.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\mpckmeans.py</code> <pre><code>def preprocess_constraints(\n    self,\n) -&gt; Tuple[Dict[int, Set[int]], Dict[int, Set[int]], List[List[int]]]:\n    \"\"\"\n    Initialize each cluster.\n    The choice is based on the neighborhoods created by the initial constraints.\n\n    Raises:\n        ValueError: if there is a Cannot-link constraint in conflict with a Must-link constraint involving both one same point.\n\n    Returns:\n        Tuple[Dict[int, set], Dict[int, set], List[List[int]]]:\n        A new list of must-link and cannot-link constraints as well as the lambda starting neighborhoods.\n    \"\"\"\n\n    # Get the list of possible indices.\n    indices: List[str] = self.list_of_data_IDs.copy()\n\n    n: int = len(indices)\n\n    # Represent the graphs using adjacency-lists.\n    ml_graph: Dict[int, Set[int]] = {}\n    cl_graph: Dict[int, Set[int]] = {}\n\n    for k in range(n):\n        ml_graph[k] = set()\n        cl_graph[k] = set()\n\n    for data_ID_i1, data_ID_j1 in self.ml:\n        ml_graph[data_ID_i1].add(data_ID_j1)\n        ml_graph[data_ID_j1].add(data_ID_i1)\n\n    for data_ID_i2, data_ID_j2 in self.cl:\n        cl_graph[data_ID_i2].add(data_ID_j2)\n        cl_graph[data_ID_j2].add(data_ID_i2)\n\n    visited = [False for _ in range(n)]\n    neighborhoods = []\n    for index in range(n):\n        if not visited[index] and ml_graph[index]:\n            component: List[int] = []\n            self._dfs(index, ml_graph, visited, component)\n            for x1 in component:\n                for x2 in component:\n                    if x1 != x2:\n                        ml_graph[x1].add(x2)\n            neighborhoods.append(component)\n\n    for data_ID_i3, data_ID_j3 in self.cl:\n        for x in ml_graph[data_ID_i3]:\n            self._add_both(cl_graph, x, data_ID_j3)\n\n        for y in ml_graph[data_ID_j3]:\n            self._add_both(cl_graph, data_ID_i3, y)\n\n        for a in ml_graph[data_ID_i3]:\n            for b in ml_graph[data_ID_j3]:\n                self._add_both(cl_graph, a, b)\n\n    for index_1, _ in ml_graph.items():\n        for index_2 in ml_graph[index_1]:\n            if index_2 != index_1 and index_2 in cl_graph[index_1]:\n                raise ValueError(\"Inconsistent constraints between \" + str(index_1) + \" and \" + str(index_2))\n\n    return ml_graph, cl_graph, neighborhoods\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/mpckmeans/#cognitivefactory.interactive_clustering.clustering.mpckmeans.MPCKMeansConstrainedClustering.weighted_farthest_first_traversal","title":"<code>weighted_farthest_first_traversal(points, weights, k)</code>","text":"<p>Applies weighted farthest first traversal algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>Set of points.</p> required <code>weights</code> <code>ndarray</code> <p>Weights for the distances.</p> required <code>k</code> <code>int</code> <p>Number of points to be traversed</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: Indexes of the traversed points.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\mpckmeans.py</code> <pre><code>def weighted_farthest_first_traversal(self, points: np.ndarray, weights: np.ndarray, k: int) -&gt; List[int]:\n    \"\"\"\n    Applies weighted farthest first traversal algorithm.\n\n    Args:\n        points (np.ndarray): Set of points.\n        weights (np.ndarray): Weights for the distances.\n        k (int): Number of points to be traversed\n\n    Returns:\n        List[int]: Indexes of the traversed points.\n    \"\"\"\n    traversed = []\n\n    # Choose the first point randomly (weighted)\n    i = np.random.choice(len(points), size=1, p=weights)[0]\n    traversed.append(i)\n\n    # Find remaining n - 1 maximally separated points\n    for _ in range(k - 1):\n        max_dst, max_dst_index = 0, None\n\n        number_of_points = len(points)\n        for j in range(number_of_points):\n            if j not in traversed:\n                dst = self.dist(j, traversed, points)\n                weighted_dst = weights[j] * dst\n\n                if weighted_dst &gt; max_dst:\n                    max_dst = weighted_dst\n                    max_dst_index = j\n\n        traversed.append(max_dst_index)\n\n    return traversed\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/spectral/","title":"spectral","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.clustering.spectral</li> <li>Description:  Implementation of constrained spectral clustering algorithms.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/spectral/#cognitivefactory.interactive_clustering.clustering.spectral.SpectralConstrainedClustering","title":"<code>SpectralConstrainedClustering</code>","text":"<p>             Bases: <code>AbstractConstrainedClustering</code></p> <p>This class implements the spectral constrained clustering. It inherits from <code>AbstractConstrainedClustering</code>.</p> References <ul> <li>Spectral Clustering: <code>Ng, A. Y., M. I. Jordan, et Y.Weiss (2002). On Spectral Clustering: Analysis and an algorithm. In T. G. Dietterich, S. Becker, et Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press.</code></li> <li>Constrained 'SPEC' Spectral Clustering: <code>Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566.</code></li> </ul> Example <pre><code># Import.\nfrom scipy.sparse import csr_matrix\nfrom cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\nfrom cognitivefactory.interactive_clustering.clustering.spectral import SpectralConstrainedClustering\n\n# Create an instance of spectral clustering.\nclustering_model = SpectralConstrainedClustering(\n    model=\"SPEC\",\n    random_seed=1,\n)\n\n# Define vectors.\n# NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\nvectors = {\n    \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n    \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n    \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n    \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n    \"4\": csr_matrix([0.60, 0.17, 0.16, 0.07]),\n    \"5\": csr_matrix([0.60, 0.16, 0.17, 0.07]),\n    \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n    \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n    \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n}\n\n# Define constraints manager.\nconstraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"3\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"7\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"7\", constraint_type=\"CANNOT_LINK\")\n\n# Run clustering.\ndict_of_predicted_clusters = clustering_model.cluster(\n    constraints_manager=constraints_manager,\n    vectors=vectors,\n    nb_clusters=3,\n)\n\n# Print results.\nprint(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 1, \"5\": 1, \"6\": 2, \"7\": 2, \"8\": 2,})\nprint(\"Computed results\", \":\", dict_of_predicted_clusters)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\spectral.py</code> <pre><code>class SpectralConstrainedClustering(AbstractConstrainedClustering):\n    \"\"\"\n    This class implements the spectral constrained clustering.\n    It inherits from `AbstractConstrainedClustering`.\n\n    References:\n        - Spectral Clustering: `Ng, A. Y., M. I. Jordan, et Y.Weiss (2002). On Spectral Clustering: Analysis and an algorithm. In T. G. Dietterich, S. Becker, et Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press.`\n        - Constrained _'SPEC'_ Spectral Clustering: `Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566.`\n\n    Example:\n        ```python\n        # Import.\n        from scipy.sparse import csr_matrix\n        from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\n        from cognitivefactory.interactive_clustering.clustering.spectral import SpectralConstrainedClustering\n\n        # Create an instance of spectral clustering.\n        clustering_model = SpectralConstrainedClustering(\n            model=\"SPEC\",\n            random_seed=1,\n        )\n\n        # Define vectors.\n        # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts.\n        vectors = {\n            \"0\": csr_matrix([1.00, 0.00, 0.00, 0.00]),\n            \"1\": csr_matrix([0.95, 0.02, 0.02, 0.01]),\n            \"2\": csr_matrix([0.98, 0.00, 0.02, 0.00]),\n            \"3\": csr_matrix([0.99, 0.00, 0.01, 0.00]),\n            \"4\": csr_matrix([0.60, 0.17, 0.16, 0.07]),\n            \"5\": csr_matrix([0.60, 0.16, 0.17, 0.07]),\n            \"6\": csr_matrix([0.01, 0.01, 0.01, 0.97]),\n            \"7\": csr_matrix([0.00, 0.01, 0.00, 0.99]),\n            \"8\": csr_matrix([0.00, 0.00, 0.00, 1.00]),\n        }\n\n        # Define constraints manager.\n        constraints_manager = BinaryConstraintsManager(list_of_data_IDs=list(vectors.keys()))\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"3\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"5\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"7\", data_ID2=\"8\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"4\", constraint_type=\"CANNOT_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"4\", data_ID2=\"7\", constraint_type=\"CANNOT_LINK\")\n\n        # Run clustering.\n        dict_of_predicted_clusters = clustering_model.cluster(\n            constraints_manager=constraints_manager,\n            vectors=vectors,\n            nb_clusters=3,\n        )\n\n        # Print results.\n        print(\"Expected results\", \";\", {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 1, \"5\": 1, \"6\": 2, \"7\": 2, \"8\": 2,})\n        print(\"Computed results\", \":\", dict_of_predicted_clusters)\n        ```\n    \"\"\"\n\n    # ==============================================================================\n    # INITIALIZATION\n    # ==============================================================================\n    def __init__(\n        self, model: str = \"SPEC\", nb_components: Optional[int] = None, random_seed: Optional[int] = None, **kargs\n    ) -&gt; None:\n        \"\"\"\n        The constructor for Spectral Constrainted Clustering class.\n\n        Args:\n            model (str, optional): The spectral clustering model to use. Available spectral models are `\"SPEC\"` and `\"CCSR\"`. Defaults to `\"SPEC\"`.\n            nb_components (Optional[int], optional): The number of eigenvectors to compute in the spectral clustering. If `None`, set the number of components to the number of clusters. Defaults to `None`.\n            random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`.\n            **kargs (dict): Other parameters that can be used in the instantiation.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set.\n        \"\"\"\n\n        # Store `self.model`.\n        if model != \"SPEC\":  # TODO use `not in {\"SPEC\"}`. # TODO `\"CCSR\"` to add after correction.\n            raise ValueError(\"The `model` '\" + str(model) + \"' is not implemented.\")\n        self.model: str = model\n\n        # Store `self.nb_components`.\n        if (nb_components is not None) and (nb_components &lt; 2):\n            raise ValueError(\n                \"The `nb_components` '\" + str(nb_components) + \"' must be `None` or greater than or equal to 2.\"\n            )\n        self.nb_components: Optional[int] = nb_components\n\n        # Store `self.random_seed`.\n        self.random_seed: Optional[int] = random_seed\n\n        # Store `self.kargs` for kmeans clustering.\n        self.kargs = kargs\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n\n    # ==============================================================================\n    # MAIN - CLUSTER DATA\n    # ==============================================================================\n    def cluster(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        vectors: Dict[str, csr_matrix],\n        nb_clusters: Optional[int],\n        verbose: bool = False,\n        **kargs,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        The main method used to cluster data with the Spectral model.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n            vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n            nb_clusters (Optional[int]): The number of clusters to compute.\n            verbose (bool, optional): Enable verbose output. Defaults to `False`.\n            **kargs (dict): Other parameters that can be used in the clustering.\n\n        Raises:\n            ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        ###\n        ### GET PARAMETERS\n        ###\n\n        # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n        if not isinstance(constraints_manager, AbstractConstraintsManager):\n            raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n        self.constraints_manager: AbstractConstraintsManager = constraints_manager\n        self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n        # Store `self.vectors`.\n        if not isinstance(vectors, dict):\n            raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n        self.vectors: Dict[str, csr_matrix] = vectors\n\n        # Store `self.nb_clusters`.\n        if (nb_clusters is None) or (nb_clusters &lt; 2):\n            raise ValueError(\"The `nb_clusters` '\" + str(nb_clusters) + \"' must be greater than or equal to 2.\")\n        self.nb_clusters: int = min(nb_clusters, len(self.list_of_data_IDs))\n\n        # Define `self.current_nb_components`.\n        self.current_nb_components: int = (\n            self.nb_components\n            if ((self.nb_components is not None) and (self.nb_clusters &lt; self.nb_components))\n            else self.nb_clusters\n        )\n\n        # Compute `self.pairwise_similarity_matrix`.\n        self.pairwise_similarity_matrix: csr_matrix = pairwise_kernels(\n            X=vstack(self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()),\n            metric=\"rbf\",  # TODO get different pairwise_distances config in **kargs\n        )\n\n        ###\n        ### RUN SPECTRAL CONSTRAINED CLUSTERING\n        ###\n\n        # Initialize `self.dict_of_predicted_clusters`.\n        self.dict_of_predicted_clusters = None\n\n        # Case of `\"CCSR\"` spectral clustering.\n        # TODO Don't work.\n        ##if self.model == \"CCSR\":\n        ##    self.dict_of_predicted_clusters = self.clustering_spectral_model_CCSR(verbose=verbose)\n\n        # Case of `\"SPEC\"` spectral clustering.\n        ##### DEFAULTS : if self.model==\"SPEC\":\n        self.dict_of_predicted_clusters = self.clustering_spectral_model_SPEC(verbose=verbose)\n\n        ###\n        ### RETURN PREDICTED CLUSTERS\n        ###\n\n        return self.dict_of_predicted_clusters\n\n    # ==============================================================================\n    # IMPLEMENTATION - SPEC SPECTRAL CLUSTERING\n    # ==============================================================================\n    def clustering_spectral_model_SPEC(\n        self,\n        verbose: bool = False,\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        Implementation of a simple Spectral clustering algorithm, based affinity matrix modifications.\n\n        References :\n            - Constrained _'SPEC'_ Spectral Clustering: `Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566.`\n\n        Args:\n            verbose (bool, optional): Enable verbose output. Default is `False`.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \"\"\"\n\n        ###\n        ### MODIFY CONSTRAINTS MATRIX WITH CONSTRAINTS\n        ###\n\n        # Modify the similarity over data IDs.\n        for ID1, data_ID1 in enumerate(self.list_of_data_IDs):\n            for ID2, data_ID2 in enumerate(self.list_of_data_IDs):\n                # Symetry is already handled in next instructions.\n                if ID1 &gt; ID2:\n                    continue\n\n                # For each `\"MUST_LINK\"` constraint, set the similarity to 1.0.\n                if (\n                    self.constraints_manager.get_inferred_constraint(\n                        data_ID1=data_ID1,\n                        data_ID2=data_ID2,\n                    )\n                    == \"MUST_LINK\"\n                ):\n                    self.pairwise_similarity_matrix[ID1, ID2] = 1.0\n                    self.pairwise_similarity_matrix[ID2, ID1] = 1.0\n\n                # For each `\"CANNOT_LINK\"` constraint, set the similarity to 0.0.\n                elif (\n                    self.constraints_manager.get_inferred_constraint(\n                        data_ID1=data_ID1,\n                        data_ID2=data_ID2,\n                    )\n                    == \"CANNOT_LINK\"\n                ):\n                    self.pairwise_similarity_matrix[ID1, ID2] = 0.0\n                    self.pairwise_similarity_matrix[ID2, ID1] = 0.0\n\n        ###\n        ### RUN SPECTRAL CONSTRAINED CLUSTERING\n        ###     | Define laplacian matrix\n        ###     | Compute eigen vectors\n        ###     | Cluster eigen vectors\n        ###     | Return labels based on eigen vectors clustering\n        ###\n\n        # Initialize spectral clustering model.\n        self.clustering_model = SpectralClustering(\n            n_clusters=self.nb_clusters,\n            # n_components=self.current_nb_components, #TODO Add if `scikit-learn&gt;=0.24.1`\n            affinity=\"precomputed\",\n            random_state=self.random_seed,\n            **self.kargs,\n        )\n\n        # Run spectral clustering model.\n        self.clustering_model.fit_predict(X=self.pairwise_similarity_matrix)\n\n        # Get prediction of spectral clustering model.\n        list_of_clusters: List[int] = self.clustering_model.labels_.tolist()\n\n        # Define the dictionary of predicted clusters.\n        predicted_clusters: Dict[str, int] = {\n            data_ID: list_of_clusters[ID] for ID, data_ID in enumerate(self.list_of_data_IDs)\n        }\n\n        # Rename cluster IDs by order.\n        predicted_clusters = rename_clusters_by_order(clusters=predicted_clusters)\n\n        # Return predicted clusters\n        return predicted_clusters\n\n    # ==============================================================================\n    # IMPLEMENTATION - CCSR SPECTRAL CLUSTERING\n    # ==============================================================================\n    \"\"\" #TODO : do not work. check if 1) wrong implementation ? 2) cvxopt better ?\n    def clustering_spectral_model_CCSR(\n        self,\n        verbose: bool = False,\n    ) -&gt; Dict[str, int]:\n        \\\"\"\"\n        Implementation of Constrained Clustering with Spectral Regularization algorithm, based on spectral semidefinite programming interpretation.\n        - Source : `Li, Z., Liu, J., &amp; Tang, X. (2009). Constrained clustering via spectral regularization. 2009 IEEE Conference on Computer Vision and Pattern Recognition. https://doi.org/10.1109/cvpr.2009.5206852`\n        - MATLAB Implementation : https://icube-forge.unistra.fr/lampert/TSCC/-/tree/master/methods/Li09\n        - SOLVERS comparison : https://pypi.org/project/PICOS/\n        - CVXPY solver : https://www.cvxpy.org/index.html\n\n        Args:\n            verbose (bool, optional): Enable verbose output. Defaults to `False`.\n\n        Returns:\n            Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n        \\\"\"\"\n\n        ###\n        ### COMPUTE NORMALIZED LAPLACIAN\n        ###\n\n        # Compute the normalized Laplacian.\n        normalized_laplacian, diagonal = csgraph_laplacian(\n            csgraph=self.pairwise_similarity_matrix,\n            normed=True,\n            return_diag=True\n        )\n\n        ###\n        ### COMPUTE EIGENVECTORS OF LAPLACIAN\n        ###\n\n        random.seed(self.random_seed)\n        v0 = np.random.rand(min(normalized_laplacian.shape))\n\n        # Compute the largest eigen vectors.\n        eigen_values, eigen_vectors = eigsh(\n            A=normalized_laplacian,\n            which=\"SM\",\n            k=self.current_nb_components + 1,\n            v0=v0,\n        )\n\n        # Ignore eigenvector of eigenvalue 0.\n        eigen_values = eigen_values[1:]\n        eigen_vectors = eigen_vectors.T[1:]\n\n        if verbose:  # pragma: no cover\n            print(\"EIGENVALUES / EIGENVECTORS\")\n\n            for k in range(len(eigen_values)):\n                print(\"    \", \"=============\")\n                print(\"    \", \"ID :         \", k)\n                print(\"    \", \"VAL :        \", eigen_values[k])\n                print(\"    \", \"VEC :        \", eigen_vectors[k])\n\n        ###\n        ### FORMALIZE SEMIDEFINITE PROBLEM\n        ###\n\n        ## Problem ::\n        ## Cost function to minimize : L(z) = 1/2 * z.T * B * z + b.T * z + c\n        ## z : variable to find with the SDP problem\n        ## z = vec(M)\n        ## M &gt;&gt; 0 (semi definite positive), i.e. M = M.T, M.shape=(nb_components, nb_components)\n        ## B = sum ( s_ij s_ij.T )\n        # for ij in MUST_LINK or i,j in CANNOT_LINK\n        ## b = -2 * sum ( t_ij * s_ij )\n        # for ij in MUST_LINK or i,j in CANNOT_LINK\n        ## c = sum ( t_ij^2 )\n        # for ij in MUST_LINK or i,j in CANNOT_LINK\n        ## s_ij = vec( eigen_vector_i.T * eigen_vector_j )\n        ## t_ij = 1 if MUST_LINK(i,j), 0 if CANNOT_LINK(i,j)\n\n        # Initialization of SDP variables.\n        B = np.zeros((self.current_nb_components ** 2, self.current_nb_components ** 2))\n        b = np.zeros((self.current_nb_components ** 2, 1))\n        c = 0\n\n        for ID1, data_ID1 in enumerate(self.list_of_data_IDs):\n            for ID2, data_ID2 in enumerate(self.list_of_data_IDs):\n\n                # Get eigenvectors.\n                eigen_vector_i = np.atleast_2d(eigen_vectors.T[ID1])\n                eigen_vector_j = np.atleast_2d(eigen_vectors.T[ID2])\n\n                # Compute eigenvectors similarity.\n                U = eigen_vector_j.T @ eigen_vector_i\n                s = np.atleast_2d(U.ravel())\n\n\n                # For each `\"MUST_LINK\"` constraint, ....\n                if self.constraints_manager.get_inferred_constraint(\n                    data_ID1=data_ID1,\n                    data_ID2=data_ID2,\n                ) == \"MUST_LINK\":\n\n                    # Add the value to SDP variables.\n                    B += s.T * s\n                    b += - 1 * s.T\n                    c += 1 * 1\n\n                # For each `\"CANNOT_LINK\"` constraint, ....\n                if self.constraints_manager.get_inferred_constraint(\n                    data_ID1=data_ID1,\n                    data_ID2=data_ID2,\n                ) == \"CANNOT_LINK\":\n\n                    # Add the value to SDP variables.\n                    B += s.T * s\n                    b += - 0 * s.T\n                    c += 0 * 0\n\n        ###\n        ### SOLVE SEMIDEFINITE PROBLEM\n        ###\n\n        # Create a symetric matrix variable.\n        M = cp.Variable((self.current_nb_components, self.current_nb_components))\n\n        ### Define constraints.\n        SDP_constraints = []\n\n        # The solution must be positive semidefinite.\n        SDP_constraints += [M &gt;&gt; 0]\n\n        # Define cost function to minimize : `( 1/2 * z.T * B * z + b.T * z + c )`.\n        self.SDP_problem = cp.Problem(\n            cp.Minimize(\n                cp.quad_form(cp.atoms.affine.vec.vec(M), B)  # `1/2 * z.T * B * z`.\n                + b.T @ cp.atoms.affine.vec.vec(M)  # `b.T * z`.\n                + c  # c\n            ),\n            SDP_constraints,\n        )\n\n        # Solve the SDP problem.\n        self.SDP_problem.solve(solver=\"MOSEK\")\n        if verbose:  # pragma: no cover\n            print(\"SEMIDEFINITE PROBLEM\")\n            print(\"    \", \"STATUS\", \":\", self.SDP_problem.status)\n            print(\"    \", \"COST FUNCTION VALUE\", \":\", self.SDP_problem.value)\n\n        ###\n        ### CLUSTER EIGEN VECTORS\n        ###\n\n        # Define square root of M, and force sqrtM to be symetric.\n        sqrtM = sqrtm(M.value).real\n        sqrtM = (sqrtM + sqrtM.T) / 2\n\n        # Compute new embeddings for spectral clustering.\n        new_vectors_to_clusters = eigen_vectors.T @ sqrtM\n\n        # Initialize kmeans klustering model.\n        self.clustering_model = KMeans(\n            n_clusters=self.nb_clusters,\n            max_iter=10000,\n            random_state=self.random_seed,\n        )\n\n        # Run kmeans clustering model.\n        self.clustering_model.fit_predict(\n            X=new_vectors_to_clusters\n        )\n\n        # Get prediction of kmeans clustering model.\n        list_of_clusters: List[int] = self.clustering_model.labels_.tolist()\n\n        # Define the dictionary of predicted clusters.\n        predicted_clusters: Dict[str, int] = {\n            data_ID: list_of_clusters[ID]\n            for ID, data_ID in enumerate(self.list_of_data_IDs)\n        }\n\n        ###\n        ### RENAME CLUSTERS BY ORDER\n        ###\n\n        # Define a map to be able to rename cluster IDs.\n        mapping_of_old_ID_to_new_ID: Dict[int, int] = {}\n        new_ID: int = 0\n        for data_ID in self.list_of_data_IDs:\n            if predicted_clusters[data_ID] not in mapping_of_old_ID_to_new_ID.keys():\n                mapping_of_old_ID_to_new_ID[predicted_clusters[data_ID]] = new_ID\n                new_ID += 1\n\n        # Rename cluster IDs.\n        predicted_clusters = {\n            data_ID: mapping_of_old_ID_to_new_ID[predicted_clusters[data_ID]]\n            for data_ID in self.list_of_data_IDs\n        }\n\n        # Return predicted clusters\n        return predicted_clusters\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/spectral/#cognitivefactory.interactive_clustering.clustering.spectral.SpectralConstrainedClustering.__init__","title":"<code>__init__(model='SPEC', nb_components=None, random_seed=None, **kargs)</code>","text":"<p>The constructor for Spectral Constrainted Clustering class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The spectral clustering model to use. Available spectral models are <code>\"SPEC\"</code> and <code>\"CCSR\"</code>. Defaults to <code>\"SPEC\"</code>.</p> <code>'SPEC'</code> <code>nb_components</code> <code>Optional[int]</code> <p>The number of eigenvectors to compute in the spectral clustering. If <code>None</code>, set the number of components to the number of clusters. Defaults to <code>None</code>.</p> <code>None</code> <code>random_seed</code> <code>Optional[int]</code> <p>The random seed to use to redo the same clustering. Defaults to <code>None</code>.</p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\spectral.py</code> <pre><code>def __init__(\n    self, model: str = \"SPEC\", nb_components: Optional[int] = None, random_seed: Optional[int] = None, **kargs\n) -&gt; None:\n    \"\"\"\n    The constructor for Spectral Constrainted Clustering class.\n\n    Args:\n        model (str, optional): The spectral clustering model to use. Available spectral models are `\"SPEC\"` and `\"CCSR\"`. Defaults to `\"SPEC\"`.\n        nb_components (Optional[int], optional): The number of eigenvectors to compute in the spectral clustering. If `None`, set the number of components to the number of clusters. Defaults to `None`.\n        random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set.\n    \"\"\"\n\n    # Store `self.model`.\n    if model != \"SPEC\":  # TODO use `not in {\"SPEC\"}`. # TODO `\"CCSR\"` to add after correction.\n        raise ValueError(\"The `model` '\" + str(model) + \"' is not implemented.\")\n    self.model: str = model\n\n    # Store `self.nb_components`.\n    if (nb_components is not None) and (nb_components &lt; 2):\n        raise ValueError(\n            \"The `nb_components` '\" + str(nb_components) + \"' must be `None` or greater than or equal to 2.\"\n        )\n    self.nb_components: Optional[int] = nb_components\n\n    # Store `self.random_seed`.\n    self.random_seed: Optional[int] = random_seed\n\n    # Store `self.kargs` for kmeans clustering.\n    self.kargs = kargs\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters: Optional[Dict[str, int]] = None\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/spectral/#cognitivefactory.interactive_clustering.clustering.spectral.SpectralConstrainedClustering.cluster","title":"<code>cluster(constraints_manager, vectors, nb_clusters, verbose=False, **kargs)</code>","text":"<p>The main method used to cluster data with the Spectral model.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs that will force clustering to respect some conditions during computation.</p> required <code>vectors</code> <code>Dict[str, csr_matrix]</code> <p>The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data.</p> required <code>nb_clusters</code> <code>Optional[int]</code> <p>The number of clusters to compute.</p> required <code>verbose</code> <code>bool</code> <p>Enable verbose output. Defaults to <code>False</code>.</p> <code>False</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the clustering.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>vectors</code> and <code>constraints_manager</code> are incompatible, or if some parameters are incorrectly set.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\spectral.py</code> <pre><code>def cluster(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    vectors: Dict[str, csr_matrix],\n    nb_clusters: Optional[int],\n    verbose: bool = False,\n    **kargs,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    The main method used to cluster data with the Spectral model.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation.\n        vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data.\n        nb_clusters (Optional[int]): The number of clusters to compute.\n        verbose (bool, optional): Enable verbose output. Defaults to `False`.\n        **kargs (dict): Other parameters that can be used in the clustering.\n\n    Raises:\n        ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set.\n\n    Returns:\n        Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n\n    ###\n    ### GET PARAMETERS\n    ###\n\n    # Store `self.constraints_manager` and `self.list_of_data_IDs`.\n    if not isinstance(constraints_manager, AbstractConstraintsManager):\n        raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n    self.constraints_manager: AbstractConstraintsManager = constraints_manager\n    self.list_of_data_IDs: List[str] = self.constraints_manager.get_list_of_managed_data_IDs()\n\n    # Store `self.vectors`.\n    if not isinstance(vectors, dict):\n        raise ValueError(\"The `vectors` parameter has to be a `dict` type.\")\n    self.vectors: Dict[str, csr_matrix] = vectors\n\n    # Store `self.nb_clusters`.\n    if (nb_clusters is None) or (nb_clusters &lt; 2):\n        raise ValueError(\"The `nb_clusters` '\" + str(nb_clusters) + \"' must be greater than or equal to 2.\")\n    self.nb_clusters: int = min(nb_clusters, len(self.list_of_data_IDs))\n\n    # Define `self.current_nb_components`.\n    self.current_nb_components: int = (\n        self.nb_components\n        if ((self.nb_components is not None) and (self.nb_clusters &lt; self.nb_components))\n        else self.nb_clusters\n    )\n\n    # Compute `self.pairwise_similarity_matrix`.\n    self.pairwise_similarity_matrix: csr_matrix = pairwise_kernels(\n        X=vstack(self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()),\n        metric=\"rbf\",  # TODO get different pairwise_distances config in **kargs\n    )\n\n    ###\n    ### RUN SPECTRAL CONSTRAINED CLUSTERING\n    ###\n\n    # Initialize `self.dict_of_predicted_clusters`.\n    self.dict_of_predicted_clusters = None\n\n    # Case of `\"CCSR\"` spectral clustering.\n    # TODO Don't work.\n    ##if self.model == \"CCSR\":\n    ##    self.dict_of_predicted_clusters = self.clustering_spectral_model_CCSR(verbose=verbose)\n\n    # Case of `\"SPEC\"` spectral clustering.\n    ##### DEFAULTS : if self.model==\"SPEC\":\n    self.dict_of_predicted_clusters = self.clustering_spectral_model_SPEC(verbose=verbose)\n\n    ###\n    ### RETURN PREDICTED CLUSTERS\n    ###\n\n    return self.dict_of_predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/clustering/spectral/#cognitivefactory.interactive_clustering.clustering.spectral.SpectralConstrainedClustering.clustering_spectral_model_SPEC","title":"<code>clustering_spectral_model_SPEC(verbose=False)</code>","text":"<p>Implementation of a simple Spectral clustering algorithm, based affinity matrix modifications.</p> References <ul> <li>Constrained 'SPEC' Spectral Clustering: <code>Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566.</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Enable verbose output. Default is <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\clustering\\spectral.py</code> <pre><code>def clustering_spectral_model_SPEC(\n    self,\n    verbose: bool = False,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Implementation of a simple Spectral clustering algorithm, based affinity matrix modifications.\n\n    References :\n        - Constrained _'SPEC'_ Spectral Clustering: `Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566.`\n\n    Args:\n        verbose (bool, optional): Enable verbose output. Default is `False`.\n\n    Returns:\n        Dict[str,int]: A dictionary that contains the predicted cluster for each data ID.\n    \"\"\"\n\n    ###\n    ### MODIFY CONSTRAINTS MATRIX WITH CONSTRAINTS\n    ###\n\n    # Modify the similarity over data IDs.\n    for ID1, data_ID1 in enumerate(self.list_of_data_IDs):\n        for ID2, data_ID2 in enumerate(self.list_of_data_IDs):\n            # Symetry is already handled in next instructions.\n            if ID1 &gt; ID2:\n                continue\n\n            # For each `\"MUST_LINK\"` constraint, set the similarity to 1.0.\n            if (\n                self.constraints_manager.get_inferred_constraint(\n                    data_ID1=data_ID1,\n                    data_ID2=data_ID2,\n                )\n                == \"MUST_LINK\"\n            ):\n                self.pairwise_similarity_matrix[ID1, ID2] = 1.0\n                self.pairwise_similarity_matrix[ID2, ID1] = 1.0\n\n            # For each `\"CANNOT_LINK\"` constraint, set the similarity to 0.0.\n            elif (\n                self.constraints_manager.get_inferred_constraint(\n                    data_ID1=data_ID1,\n                    data_ID2=data_ID2,\n                )\n                == \"CANNOT_LINK\"\n            ):\n                self.pairwise_similarity_matrix[ID1, ID2] = 0.0\n                self.pairwise_similarity_matrix[ID2, ID1] = 0.0\n\n    ###\n    ### RUN SPECTRAL CONSTRAINED CLUSTERING\n    ###     | Define laplacian matrix\n    ###     | Compute eigen vectors\n    ###     | Cluster eigen vectors\n    ###     | Return labels based on eigen vectors clustering\n    ###\n\n    # Initialize spectral clustering model.\n    self.clustering_model = SpectralClustering(\n        n_clusters=self.nb_clusters,\n        # n_components=self.current_nb_components, #TODO Add if `scikit-learn&gt;=0.24.1`\n        affinity=\"precomputed\",\n        random_state=self.random_seed,\n        **self.kargs,\n    )\n\n    # Run spectral clustering model.\n    self.clustering_model.fit_predict(X=self.pairwise_similarity_matrix)\n\n    # Get prediction of spectral clustering model.\n    list_of_clusters: List[int] = self.clustering_model.labels_.tolist()\n\n    # Define the dictionary of predicted clusters.\n    predicted_clusters: Dict[str, int] = {\n        data_ID: list_of_clusters[ID] for ID, data_ID in enumerate(self.list_of_data_IDs)\n    }\n\n    # Rename cluster IDs by order.\n    predicted_clusters = rename_clusters_by_order(clusters=predicted_clusters)\n\n    # Return predicted clusters\n    return predicted_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/","title":"constraints","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.constraints</li> <li>Description:  Constraints managing module of the Interactive Clustering package.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul> <p>This module provides a constraints manager, that stores annotated constraints on data and gives some feedback on information deduced (such as the transitivity between constraints or the situation of inconsistency) :</p> <ul> <li><code>abstract</code>: an abstract class that defines constraints managers functionnalities. See interactive_clustering/constraints/abstract documentation ;</li> <li><code>factory</code>: a factory to easily instantiate constraints manager object. See interactive_clustering/constraints/factory documentation ;</li> <li><code>binary</code>: a constraints manager implementation that handles <code>MUST-LINK</code> and <code>CANNOT-LINK</code> constraints on pairs of data. See interactive_clustering/constraints/binary documentation.</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/","title":"abstract","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.constraints.abstract</li> <li>Description:  The abstract class used to define constraints managing algorithms.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager","title":"<code>AbstractConstraintsManager</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class that is used to define constraints manager. The main inherited methods are about data IDs management, constraints management and constraints exploration.</p> References <ul> <li>Constraints in clustering: <code>Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110.</code></li> </ul> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>class AbstractConstraintsManager(ABC):\n    \"\"\"\n    Abstract class that is used to define constraints manager.\n    The main inherited methods are about data IDs management, constraints management and constraints exploration.\n\n    References:\n        - Constraints in clustering: `Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110.`\n    \"\"\"\n\n    # ==============================================================================\n    # ABSTRACT METHOD - DATA_ID MANAGEMENT\n    # ==============================================================================\n    @abstractmethod\n    def add_data_ID(\n        self,\n        data_ID: str,\n    ) -&gt; bool:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to add a new data ID to manage.\n\n        Args:\n            data_ID (str): The data ID to manage.\n\n        Raises:\n            ValueError: if `data_ID` is already managed.\n\n        Returns:\n            bool: `True` if the addition is done.\n        \"\"\"\n\n    @abstractmethod\n    def delete_data_ID(\n        self,\n        data_ID: str,\n    ) -&gt; bool:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to delete a data ID to no longer manage.\n\n        Args:\n            data_ID (str): The data ID to no longer manage.\n\n        Raises:\n            ValueError: if `data_ID` is not managed.\n\n        Returns:\n            bool: `True` if the deletion is done.\n        \"\"\"\n\n    @abstractmethod\n    def get_list_of_managed_data_IDs(\n        self,\n    ) -&gt; List[str]:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to get the list of data IDs that are managed.\n\n        Returns:\n            List[str]: The list of data IDs that are managed.\n        \"\"\"\n\n    # ==============================================================================\n    # ABSTRACT METHOD - CONSTRAINTS MANAGEMENT\n    # ==============================================================================\n    @abstractmethod\n    def add_constraint(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n        constraint_type: str,\n        constraint_value: float = 1.0,\n    ) -&gt; bool:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to add a constraint between two data IDs.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint addition.\n            data_ID2 (str): The second data ID that is concerned for this constraint addition.\n            constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n            constraint_value (float, optional): The value of the constraint to add. The value have to be in range `[0.0, 1.0]`. Defaults to 1.0.\n\n        Raises:\n            ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed, or if a conflict is detected with constraints inference.\n\n        Returns:\n            bool: `True` if the addition is done, `False` is the constraint can't be added.\n        \"\"\"\n\n    @abstractmethod\n    def delete_constraint(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n    ) -&gt; bool:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to delete the constraint between two data IDs.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint deletion.\n            data_ID2 (str): The second data ID that is concerned for this constraint deletion.\n\n        Raises:\n            ValueError: if `data_ID1` or `data_ID2` are not managed.\n\n        Returns:\n            bool: `True` if the deletion is done.\n        \"\"\"\n\n    @abstractmethod\n    def get_added_constraint(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n    ) -&gt; Optional[Tuple[str, float]]:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to get the constraint added between the two data IDs.\n        Do not take into account the constraints transitivity, just look at constraints that are explicitly added.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint.\n            data_ID2 (str): The second data ID that is concerned for this constraint.\n\n        Raises:\n            ValueError: if `data_ID1` or `data_ID2` are not managed.\n\n        Returns:\n            Optional[Tuple[str, float]]: `None` if no constraint, `(constraint_type, constraint_value)` otherwise.\n        \"\"\"\n\n    # ==============================================================================\n    # ABSTRACT METHOD - CONSTRAINTS EXPLORATION\n    # ==============================================================================\n    @abstractmethod\n    def get_inferred_constraint(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n        threshold: float = 1.0,\n    ) -&gt; Optional[str]:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to check if the constraint inferred by transitivity between the two data IDs.\n        The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint.\n            data_ID2 (str): The second data ID that is concerned for this constraint.\n            threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n        Raises:\n            ValueError: if `data_ID1`, `data_ID2` or `threshold` are not managed.\n\n        Returns:\n            Optional[str]: The type of the inferred constraint. The type can be `None`, `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n        \"\"\"\n\n    @abstractmethod\n    def get_connected_components(\n        self,\n        threshold: float = 1.0,\n    ) -&gt; List[List[str]]:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to get the possible lists of data IDs that are connected by a `\"MUST_LINK\"` constraints.\n        Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs.\n        The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n        Args:\n            threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n        Raises:\n            ValueError: if `threshold` is not managed.\n\n        Returns:\n            List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph.\n        \"\"\"\n\n    @abstractmethod\n    def check_completude_of_constraints(\n        self,\n        threshold: float = 1.0,\n    ) -&gt; bool:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity).\n        The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n        Args:\n            threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n        Raises:\n            ValueError: if `threshold` is not managed.\n\n        Returns:\n            bool: Return `True` if all constraints are known, `False` otherwise.\n        \"\"\"\n\n    @abstractmethod\n    def get_min_and_max_number_of_clusters(\n        self,\n        threshold: float = 1.0,\n    ) -&gt; Tuple[int, int]:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number.\n        The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n        Args:\n            threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n        Raises:\n            ValueError: if `threshold` is not managed.\n\n        Returns:\n            Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints).\n        \"\"\"\n\n    # ==============================================================================\n    # ABSTRACT METHOD - CONSTRAINTS CONFLICT\n    # ==============================================================================\n\n    @abstractmethod\n    def get_list_of_involved_data_IDs_in_a_constraint_conflict(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n        constraint_type: str,\n    ) -&gt; Optional[List[str]]:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to get all data IDs involved in a constraints conflict.\n\n        Args:\n            data_ID1 (str): The first data ID involved in the constraint_conflit.\n            data_ID2 (str): The second data ID involved in the constraint_conflit.\n            constraint_type (str): The constraint that create a conflict. The constraints can be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n\n        Raises:\n            ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed.\n\n        Returns:\n            Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of `data_ID1` and `data_ID2`.\n        \"\"\"\n\n    # ==============================================================================\n    # ABSTRACT METHOD - SERIALIZATION\n    # ==============================================================================\n    @abstractmethod\n    def to_json(\n        self,\n        filepath: str,\n    ) -&gt; bool:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to serialize the constraints manager object into a JSON file.\n\n        Args:\n            filepath (str): The path where to serialize the constraints manager  object.\n\n        Returns:\n            bool: `True` if the serialization is done.\n        \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.add_constraint","title":"<code>add_constraint(data_ID1, data_ID2, constraint_type, constraint_value=1.0)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to add a constraint between two data IDs.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID that is concerned for this constraint addition.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID that is concerned for this constraint addition.</p> required <code>constraint_type</code> <code>str</code> <p>The type of the constraint to add. The type have to be <code>\"MUST_LINK\"</code> or <code>\"CANNOT_LINK\"</code>.</p> required <code>constraint_value</code> <code>float</code> <p>The value of the constraint to add. The value have to be in range <code>[0.0, 1.0]</code>. Defaults to 1.0.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code>, <code>data_ID2</code>, <code>constraint_type</code> are not managed, or if a conflict is detected with constraints inference.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the addition is done, <code>False</code> is the constraint can't be added.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef add_constraint(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n    constraint_type: str,\n    constraint_value: float = 1.0,\n) -&gt; bool:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to add a constraint between two data IDs.\n\n    Args:\n        data_ID1 (str): The first data ID that is concerned for this constraint addition.\n        data_ID2 (str): The second data ID that is concerned for this constraint addition.\n        constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n        constraint_value (float, optional): The value of the constraint to add. The value have to be in range `[0.0, 1.0]`. Defaults to 1.0.\n\n    Raises:\n        ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed, or if a conflict is detected with constraints inference.\n\n    Returns:\n        bool: `True` if the addition is done, `False` is the constraint can't be added.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.add_data_ID","title":"<code>add_data_ID(data_ID)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to add a new data ID to manage.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID</code> <code>str</code> <p>The data ID to manage.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID</code> is already managed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the addition is done.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef add_data_ID(\n    self,\n    data_ID: str,\n) -&gt; bool:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to add a new data ID to manage.\n\n    Args:\n        data_ID (str): The data ID to manage.\n\n    Raises:\n        ValueError: if `data_ID` is already managed.\n\n    Returns:\n        bool: `True` if the addition is done.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.check_completude_of_constraints","title":"<code>check_completude_of_constraints(threshold=1.0)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the <code>threshold</code> parameter is used to evaluate the impact of constraints transitivity.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold used to evaluate the impact of constraints transitivity link. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>threshold</code> is not managed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Return <code>True</code> if all constraints are known, <code>False</code> otherwise.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef check_completude_of_constraints(\n    self,\n    threshold: float = 1.0,\n) -&gt; bool:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity).\n    The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n    Args:\n        threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n    Raises:\n        ValueError: if `threshold` is not managed.\n\n    Returns:\n        bool: Return `True` if all constraints are known, `False` otherwise.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.delete_constraint","title":"<code>delete_constraint(data_ID1, data_ID2)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to delete the constraint between two data IDs.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID that is concerned for this constraint deletion.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID that is concerned for this constraint deletion.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code> or <code>data_ID2</code> are not managed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the deletion is done.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef delete_constraint(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n) -&gt; bool:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to delete the constraint between two data IDs.\n\n    Args:\n        data_ID1 (str): The first data ID that is concerned for this constraint deletion.\n        data_ID2 (str): The second data ID that is concerned for this constraint deletion.\n\n    Raises:\n        ValueError: if `data_ID1` or `data_ID2` are not managed.\n\n    Returns:\n        bool: `True` if the deletion is done.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.delete_data_ID","title":"<code>delete_data_ID(data_ID)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to delete a data ID to no longer manage.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID</code> <code>str</code> <p>The data ID to no longer manage.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID</code> is not managed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the deletion is done.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef delete_data_ID(\n    self,\n    data_ID: str,\n) -&gt; bool:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to delete a data ID to no longer manage.\n\n    Args:\n        data_ID (str): The data ID to no longer manage.\n\n    Raises:\n        ValueError: if `data_ID` is not managed.\n\n    Returns:\n        bool: `True` if the deletion is done.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_added_constraint","title":"<code>get_added_constraint(data_ID1, data_ID2)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID that is concerned for this constraint.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID that is concerned for this constraint.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code> or <code>data_ID2</code> are not managed.</p> <p>Returns:</p> Type Description <code>Optional[Tuple[str, float]]</code> <p>Optional[Tuple[str, float]]: <code>None</code> if no constraint, <code>(constraint_type, constraint_value)</code> otherwise.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef get_added_constraint(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n) -&gt; Optional[Tuple[str, float]]:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to get the constraint added between the two data IDs.\n    Do not take into account the constraints transitivity, just look at constraints that are explicitly added.\n\n    Args:\n        data_ID1 (str): The first data ID that is concerned for this constraint.\n        data_ID2 (str): The second data ID that is concerned for this constraint.\n\n    Raises:\n        ValueError: if `data_ID1` or `data_ID2` are not managed.\n\n    Returns:\n        Optional[Tuple[str, float]]: `None` if no constraint, `(constraint_type, constraint_value)` otherwise.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_connected_components","title":"<code>get_connected_components(threshold=1.0)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to get the possible lists of data IDs that are connected by a <code>\"MUST_LINK\"</code> constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the <code>threshold</code> parameter is used to evaluate the impact of constraints transitivity.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold used to evaluate the impact of constraints transitivity link. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>threshold</code> is not managed.</p> <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef get_connected_components(\n    self,\n    threshold: float = 1.0,\n) -&gt; List[List[str]]:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to get the possible lists of data IDs that are connected by a `\"MUST_LINK\"` constraints.\n    Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs.\n    The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n    Args:\n        threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n    Raises:\n        ValueError: if `threshold` is not managed.\n\n    Returns:\n        List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_inferred_constraint","title":"<code>get_inferred_constraint(data_ID1, data_ID2, threshold=1.0)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the <code>threshold</code> parameter is used to evaluate the impact of constraints transitivity.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID that is concerned for this constraint.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID that is concerned for this constraint.</p> required <code>threshold</code> <code>float</code> <p>The threshold used to evaluate the impact of constraints transitivity link. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code>, <code>data_ID2</code> or <code>threshold</code> are not managed.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The type of the inferred constraint. The type can be <code>None</code>, <code>\"MUST_LINK\"</code> or <code>\"CANNOT_LINK\"</code>.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef get_inferred_constraint(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n    threshold: float = 1.0,\n) -&gt; Optional[str]:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to check if the constraint inferred by transitivity between the two data IDs.\n    The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n    Args:\n        data_ID1 (str): The first data ID that is concerned for this constraint.\n        data_ID2 (str): The second data ID that is concerned for this constraint.\n        threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n    Raises:\n        ValueError: if `data_ID1`, `data_ID2` or `threshold` are not managed.\n\n    Returns:\n        Optional[str]: The type of the inferred constraint. The type can be `None`, `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_list_of_involved_data_IDs_in_a_constraint_conflict","title":"<code>get_list_of_involved_data_IDs_in_a_constraint_conflict(data_ID1, data_ID2, constraint_type)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to get all data IDs involved in a constraints conflict.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID involved in the constraint_conflit.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID involved in the constraint_conflit.</p> required <code>constraint_type</code> <code>str</code> <p>The constraint that create a conflict. The constraints can be <code>\"MUST_LINK\"</code> or <code>\"CANNOT_LINK\"</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code>, <code>data_ID2</code>, <code>constraint_type</code> are not managed.</p> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of <code>data_ID1</code> and <code>data_ID2</code>.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef get_list_of_involved_data_IDs_in_a_constraint_conflict(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n    constraint_type: str,\n) -&gt; Optional[List[str]]:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to get all data IDs involved in a constraints conflict.\n\n    Args:\n        data_ID1 (str): The first data ID involved in the constraint_conflit.\n        data_ID2 (str): The second data ID involved in the constraint_conflit.\n        constraint_type (str): The constraint that create a conflict. The constraints can be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n\n    Raises:\n        ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed.\n\n    Returns:\n        Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of `data_ID1` and `data_ID2`.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_list_of_managed_data_IDs","title":"<code>get_list_of_managed_data_IDs()</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to get the list of data IDs that are managed.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of data IDs that are managed.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef get_list_of_managed_data_IDs(\n    self,\n) -&gt; List[str]:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to get the list of data IDs that are managed.\n\n    Returns:\n        List[str]: The list of data IDs that are managed.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_min_and_max_number_of_clusters","title":"<code>get_min_and_max_number_of_clusters(threshold=1.0)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. The transitivity is taken into account, and the <code>threshold</code> parameter is used to evaluate the impact of constraints transitivity.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold used to evaluate the impact of constraints transitivity link. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>threshold</code> is not managed.</p> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints).</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef get_min_and_max_number_of_clusters(\n    self,\n    threshold: float = 1.0,\n) -&gt; Tuple[int, int]:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number.\n    The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n    Args:\n        threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n    Raises:\n        ValueError: if `threshold` is not managed.\n\n    Returns:\n        Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints).\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.to_json","title":"<code>to_json(filepath)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to serialize the constraints manager object into a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path where to serialize the constraints manager  object.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the serialization is done.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\abstract.py</code> <pre><code>@abstractmethod\ndef to_json(\n    self,\n    filepath: str,\n) -&gt; bool:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to serialize the constraints manager object into a JSON file.\n\n    Args:\n        filepath (str): The path where to serialize the constraints manager  object.\n\n    Returns:\n        bool: `True` if the serialization is done.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/","title":"binary","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.constraints.binary</li> <li>Description:  Implementation of binary constraints manager.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager","title":"<code>BinaryConstraintsManager</code>","text":"<p>             Bases: <code>AbstractConstraintsManager</code></p> <p>This class implements the binary constraints mangement. It inherits from <code>AbstractConstraintsManager</code>, and it takes into account the strong transitivity of constraints.</p> References <ul> <li>Binary constraints in clustering: <code>Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110.</code></li> </ul> Example <pre><code># Import.\nfrom cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\n\n# Create an instance of binary constraints manager.\nconstraints_manager = BinaryConstraintsManager(list_of_data_IDs=[\"0\", \"1\", \"2\", \"3\", \"4\"])\n\n# Add new data ID.\nconstraints_manager.add_data_ID(data_ID=\"99\")\n\n# Get list of data IDs.\nconstraints_manager.get_list_of_managed_data_IDs()\n\n# Delete an existing data ID.\nconstraints_manager.delete_data_ID(data_ID=\"99\")\n\n# Add constraints.\nconstraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"1\", data_ID2=\"2\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"3\", constraint_type=\"CANNOT_LINK\")\n\n# Get added constraint.\nconstraints_manager.get_added_constraint(data_ID1=\"0\", data_ID2=\"1\")  # expected (\"MUST_LINK\", 1.0)\nconstraints_manager.get_added_constraint(data_ID1=\"0\", data_ID2=\"2\")  # expected None\n\n# Get inferred constraint.\nconstraints_manager.get_inferred_constraint(data_ID1=\"0\", data_ID2=\"2\")  # expected \"MUST_LINK\"\nconstraints_manager.get_inferred_constraint(data_ID1=\"0\", data_ID2=\"3\")  # expected \"CANNOT_LINK\"\nconstraints_manager.get_inferred_constraint(data_ID1=\"0\", data_ID2=\"4\")  # expected None\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>class BinaryConstraintsManager(AbstractConstraintsManager):\n    \"\"\"\n    This class implements the binary constraints mangement.\n    It inherits from `AbstractConstraintsManager`, and it takes into account the strong transitivity of constraints.\n\n    References:\n        - Binary constraints in clustering: `Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110.`\n\n    Example:\n        ```python\n        # Import.\n        from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\n\n        # Create an instance of binary constraints manager.\n        constraints_manager = BinaryConstraintsManager(list_of_data_IDs=[\"0\", \"1\", \"2\", \"3\", \"4\"])\n\n        # Add new data ID.\n        constraints_manager.add_data_ID(data_ID=\"99\")\n\n        # Get list of data IDs.\n        constraints_manager.get_list_of_managed_data_IDs()\n\n        # Delete an existing data ID.\n        constraints_manager.delete_data_ID(data_ID=\"99\")\n\n        # Add constraints.\n        constraints_manager.add_constraint(data_ID1=\"0\", data_ID2=\"1\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"1\", data_ID2=\"2\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"2\", data_ID2=\"3\", constraint_type=\"CANNOT_LINK\")\n\n        # Get added constraint.\n        constraints_manager.get_added_constraint(data_ID1=\"0\", data_ID2=\"1\")  # expected (\"MUST_LINK\", 1.0)\n        constraints_manager.get_added_constraint(data_ID1=\"0\", data_ID2=\"2\")  # expected None\n\n        # Get inferred constraint.\n        constraints_manager.get_inferred_constraint(data_ID1=\"0\", data_ID2=\"2\")  # expected \"MUST_LINK\"\n        constraints_manager.get_inferred_constraint(data_ID1=\"0\", data_ID2=\"3\")  # expected \"CANNOT_LINK\"\n        constraints_manager.get_inferred_constraint(data_ID1=\"0\", data_ID2=\"4\")  # expected None\n        ```\n    \"\"\"\n\n    # ==============================================================================\n    # INITIALIZATION\n    # ==============================================================================\n    def __init__(self, list_of_data_IDs: List[str], **kargs) -&gt; None:\n        \"\"\"\n        The constructor for Binary Constraints Manager class.\n        This class use the strong transitivity to infer on constraints, so constraints values are not taken into account.\n\n        Args:\n            list_of_data_IDs (List[str]): The list of data IDs to manage.\n            **kargs (dict): Other parameters that can be used in the instantiation.\n        \"\"\"\n\n        # Define `self._allowed_constraint_types`.\n        self._allowed_constraint_types: Set[str] = {\n            \"MUST_LINK\",\n            \"CANNOT_LINK\",\n        }\n        # Define `self._allowed_constraint_value_range`.\n        self._allowed_constraint_value_range: Dict[str, float] = {\n            \"min\": 1.0,\n            \"max\": 1.0,\n        }\n\n        # Store `self.kargs` for binary constraints managing.\n        self.kargs = kargs\n\n        # Initialize `self._constraints_dictionary`.\n        self._constraints_dictionary: Dict[str, Dict[str, Optional[Tuple[str, float]]]] = {\n            data_ID1: {\n                data_ID2: (\n                    (\"MUST_LINK\", 1.0)\n                    if (data_ID1 == data_ID2)\n                    else None  # Unknwon constraints if `data_ID1` != `data_ID2`.\n                )\n                for data_ID2 in list_of_data_IDs\n                if (data_ID1 &lt;= data_ID2)\n            }\n            for data_ID1 in list_of_data_IDs\n        }\n\n        # Define `self._constraints_transitivity`.\n        # `Equivalent to `self._generate_constraints_transitivity()`\n        self._constraints_transitivity: Dict[str, Dict[str, Dict[str, None]]] = {\n            data_ID: {\n                \"MUST_LINK\": {data_ID: None},  # Initialize MUST_LINK clusters constraints.\n                \"CANNOT_LINK\": {},  # Initialize CANNOT_LINK clusters constraints.\n            }\n            for data_ID in list_of_data_IDs\n        }\n\n    # ==============================================================================\n    # DATA_ID MANAGEMENT - ADDITION\n    # ==============================================================================\n    def add_data_ID(\n        self,\n        data_ID: str,\n    ) -&gt; bool:\n        \"\"\"\n        The main method used to add a new data ID to manage.\n\n        Args:\n            data_ID (str): The data ID to manage.\n\n        Raises:\n            ValueError: if `data_ID` is already managed.\n\n        Returns:\n            bool: `True` if the addition is done.\n        \"\"\"\n\n        # If `data_ID` is in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID` `'\" + str(data_ID) + \"'` is already managed.\")\n\n        # Add `data_ID` to `self._constraints_dictionary.keys()`.\n        self._constraints_dictionary[data_ID] = {}\n\n        # Define constraint for `data_ID` and all other data IDs.\n        for other_data_ID in self._constraints_dictionary.keys():\n            if data_ID == other_data_ID:\n                self._constraints_dictionary[data_ID][data_ID] = (\"MUST_LINK\", 1.0)\n            elif data_ID &lt; other_data_ID:\n                self._constraints_dictionary[data_ID][other_data_ID] = None\n            else:  # elif data_ID &gt; other_data_ID:\n                self._constraints_dictionary[other_data_ID][data_ID] = None\n\n        # Regenerate `self._constraints_transitivity`.\n        # `Equivalent to `self._generate_constraints_transitivity()`\n        self._constraints_transitivity[data_ID] = {\n            \"MUST_LINK\": {data_ID: None},\n            \"CANNOT_LINK\": {},\n        }\n\n        # Return `True`.\n        return True\n\n    # ==============================================================================\n    # DATA_ID MANAGEMENT - DELETION\n    # ==============================================================================\n    def delete_data_ID(\n        self,\n        data_ID: str,\n    ) -&gt; bool:\n        \"\"\"\n        The main method used to delete a data ID to no longer manage.\n\n        Args:\n            data_ID (str): The data ID to no longer manage.\n\n        Raises:\n            ValueError: if `data_ID` is not managed.\n\n        Returns:\n            bool: `True` if the deletion is done.\n        \"\"\"\n\n        # If `data_ID` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID` `'\" + str(data_ID) + \"'` is not managed.\")\n\n        # Remove `data_ID` from `self._constraints_dictionary.keys()`.\n        self._constraints_dictionary.pop(data_ID, None)\n\n        # Remove `data_ID` from all `self._constraints_dictionary[other_data_ID].keys()`.\n        for other_data_ID in self._constraints_dictionary.keys():\n            self._constraints_dictionary[other_data_ID].pop(data_ID, None)\n\n        # Regenerate `self._constraints_transitivity`\n        self._generate_constraints_transitivity()\n\n        # Return `True`.\n        return True\n\n    # ==============================================================================\n    # DATA_ID MANAGEMENT - LISTING\n    # ==============================================================================\n    def get_list_of_managed_data_IDs(\n        self,\n    ) -&gt; List[str]:\n        \"\"\"\n        The main method used to get the list of data IDs that are managed.\n\n        Returns:\n            List[str]: The list of data IDs that are managed.\n        \"\"\"\n\n        # Return the possible keys of `self._constraints_dictionary`.\n        return list(self._constraints_dictionary.keys())\n\n    # ==============================================================================\n    # CONSTRAINTS MANAGEMENT - ADDITION\n    # ==============================================================================\n    def add_constraint(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n        constraint_type: str,\n        constraint_value: float = 1.0,\n    ) -&gt; bool:\n        \"\"\"\n        The main method used to add a constraint between two data IDs.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint addition.\n            data_ID2 (str): The second data ID that is concerned for this constraint addition.\n            constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n            constraint_value (float, optional): The value of the constraint to add. The value have to be in range `[0.0, 1.0]`. Defaults to `1.0`.\n\n        Raises:\n            ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed, or if a conflict is detected with constraints inference.\n\n        Returns:\n            bool: `True` if the addition is done, `False` is the constraint can't be added.\n        \"\"\"\n\n        # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID1 not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n        # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID2 not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n        # If the `constraint_type` is not in `self._allowed_constraint_types`, then raises a `ValueError`.\n        if constraint_type not in self._allowed_constraint_types:\n            raise ValueError(\n                \"The `constraint_type` `'\"\n                + str(constraint_type)\n                + \"'` is not managed. Allowed constraints types are : `\"\n                + str(self._allowed_constraint_types)\n                + \"`.\"\n            )\n\n        # Get current added constraint between `data_ID1` and `data_ID2`.\n        inferred_constraint: Optional[str] = self.get_inferred_constraint(\n            data_ID1=data_ID1,\n            data_ID2=data_ID2,\n        )\n\n        # Case of conflict with constraints inference.\n        if (inferred_constraint is not None) and (inferred_constraint != constraint_type):\n            raise ValueError(\n                \"The `constraint_type` `'\"\n                + str(constraint_type)\n                + \"'` is incompatible with the inferred constraint `'\"\n                + str(inferred_constraint)\n                + \"'` between data IDs `'\"\n                + data_ID1\n                + \"'` and `'\"\n                + data_ID2\n                + \"'`.\"\n            )\n\n        # Get current added constraint between `data_ID1` and `data_ID2`.\n        added_constraint: Optional[Tuple[str, float]] = self.get_added_constraint(\n            data_ID1=data_ID1,\n            data_ID2=data_ID2,\n        )\n\n        # If the constraint has already be added, ...\n        if added_constraint is not None:\n            # ... do nothing.\n            return True  # `added_constraint[0] == constraint_type`.\n        # Otherwise, the constraint has to be added.\n\n        # Add the direct constraint between `data_ID1` and `data_ID2`.\n        if data_ID1 &lt;= data_ID2:\n            self._constraints_dictionary[data_ID1][data_ID2] = (constraint_type, 1.0)\n        else:\n            self._constraints_dictionary[data_ID2][data_ID1] = (constraint_type, 1.0)\n\n        # Add the transitivity constraint between `data_ID1` and `data_ID2`.\n        self._add_constraint_transitivity(\n            data_ID1=data_ID1,\n            data_ID2=data_ID2,\n            constraint_type=constraint_type,\n        )\n\n        return True\n\n    # ==============================================================================\n    # CONSTRAINTS MANAGEMENT - DELETION\n    # ==============================================================================\n    def delete_constraint(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n    ) -&gt; bool:\n        \"\"\"\n        The main method used to delete a constraint between two data IDs.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint deletion.\n            data_ID2 (str): The second data ID that is concerned for this constraint deletion.\n\n        Raises:\n            ValueError: if `data_ID1` or `data_ID2` are not managed.\n\n        Returns:\n            bool: `True` if the deletion is done, `False` if the constraint can't be deleted.\n        \"\"\"\n\n        # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID1 not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n        # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID2 not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n        # Delete the constraint between `data_ID1` and `data_ID2`.\n        if data_ID1 &lt;= data_ID2:\n            self._constraints_dictionary[data_ID1][data_ID2] = None\n        else:\n            self._constraints_dictionary[data_ID2][data_ID1] = None\n\n        # Regenerate `self._constraints_transitivity`.\n        self._generate_constraints_transitivity()\n\n        # Return `True`\n        return True\n\n    # ==============================================================================\n    # CONSTRAINTS MANAGEMENT - GETTER\n    # ==============================================================================\n    def get_added_constraint(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n    ) -&gt; Optional[Tuple[str, float]]:\n        \"\"\"\n        The main method used to get the constraint added between the two data IDs.\n        Do not take into account the constraints transitivity, just look at constraints that are explicitly added.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint.\n            data_ID2 (str): The second data ID that is concerned for this constraint.\n\n        Raises:\n            ValueError: if `data_ID1` or `data_ID2` are not managed.\n\n        Returns:\n            Optional[Tuple[str, float]]: `None` if no constraint, `(constraint_type, constraint_value)` otherwise.\n        \"\"\"\n\n        # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID1 not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n        # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID2 not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n        # Retrun the current added constraint type and value.\n        return (\n            self._constraints_dictionary[data_ID1][data_ID2]\n            if (data_ID1 &lt;= data_ID2)\n            else self._constraints_dictionary[data_ID2][data_ID1]\n        )\n\n    # ==============================================================================\n    # CONSTRAINTS EXPLORATION - GETTER\n    # ==============================================================================\n    def get_inferred_constraint(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n        threshold: float = 1.0,\n    ) -&gt; Optional[str]:\n        \"\"\"\n        The main method used to check if the constraint inferred by transitivity between the two data IDs.\n        The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint.\n            data_ID2 (str): The second data ID that is concerned for this constraint.\n            threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n        Raises:\n            ValueError: if `data_ID1`, `data_ID2` or `threshold` are not managed.\n\n        Returns:\n            Optional[str]: The type of the inferred constraint. The type can be `None`, `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n        \"\"\"\n\n        # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID1 not in self._constraints_transitivity.keys():\n            raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n        # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID2 not in self._constraints_transitivity.keys():\n            raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n        # Case of `\"MUST_LINK\"`.\n        if data_ID1 in self._constraints_transitivity[data_ID2][\"MUST_LINK\"].keys():\n            return \"MUST_LINK\"\n\n        # Case of `\"CANNOT_LINK\"`.\n        if data_ID1 in self._constraints_transitivity[data_ID2][\"CANNOT_LINK\"].keys():\n            return \"CANNOT_LINK\"\n\n        # Case of `None`.\n        return None\n\n    # ==============================================================================\n    # CONSTRAINTS EXPLORATION - LIST OF COMPONENTS GETTER\n    # ==============================================================================\n    def get_connected_components(\n        self,\n        threshold: float = 1.0,\n    ) -&gt; List[List[str]]:\n        \"\"\"\n        The main method used to get the possible lists of data IDs that are linked by a `\"MUST_LINK\"` constraints.\n        Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs.\n        The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity.\n\n        Args:\n            threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`.\n\n        Returns:\n            List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph.\n        \"\"\"\n\n        # Initialize the list of connected components.\n        list_of_connected_components: List[List[str]] = []\n\n        # For each data ID...\n        for data_ID in self._constraints_transitivity.keys():\n            # ... get the list of `\"MUST_LINK\"` data IDs linked by transitivity with `data_ID` ...\n            connected_component_of_a_data_ID = list(self._constraints_transitivity[data_ID][\"MUST_LINK\"].keys())\n\n            # ... and if the connected component is not already get...\n            if connected_component_of_a_data_ID not in list_of_connected_components:\n                # ... then add it to the list of connected components.\n                list_of_connected_components.append(connected_component_of_a_data_ID)\n\n        # Return the list of connected components.\n        return list_of_connected_components\n\n    # ==============================================================================\n    # CONSTRAINTS EXPLORATION - CHECK COMPLETUDE OF CONSTRAINTS\n    # ==============================================================================\n    def check_completude_of_constraints(\n        self,\n        threshold: float = 1.0,\n    ) -&gt; bool:\n        \"\"\"\n        The main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity).\n        The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity.\n\n        Args:\n            threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`.\n\n        Returns:\n            bool: Return `True` if all constraints are known, `False` otherwise.\n        \"\"\"\n\n        # For each data ID...\n        for data_ID in self._constraints_transitivity.keys():\n            # ... if some data IDs are not linked by transitivity to this `data_ID` with a `\"MUST_LINK\"` or `\"CANNOT_LINK\"` constraints...\n            if (\n                len(self._constraints_transitivity[data_ID][\"MUST_LINK\"].keys())\n                + len(self._constraints_transitivity[data_ID][\"CANNOT_LINK\"].keys())\n            ) != len(self._constraints_transitivity.keys()):\n                # ... then return `False`.\n                return False\n\n        # Otherwise, return `True`.\n        return True\n\n    # ==============================================================================\n    # CONSTRAINTS EXPLORATION - GET MIN AND MAX NUMBER OF CLUSTERS\n    # ==============================================================================\n    def get_min_and_max_number_of_clusters(\n        self,\n        threshold: float = 1.0,\n    ) -&gt; Tuple[int, int]:\n        \"\"\"\n        The main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number.\n        Minimum number of cluster is estimated by the coloration of the `\"CANNOT_LINK\"` constraints graph.\n        Maximum number of cluster is defined by the number of `\"MUST_LINK\"` connected components.\n        The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity.\n\n        Args:\n            threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`.\n\n        Returns:\n            Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints).\n        \"\"\"\n\n        # Get the `\"MUST_LINK\"` connected components.\n        list_of_connected_components: List[List[str]] = self.get_connected_components()\n\n        ###\n        ### 1. Estimation of minimum clusters number.\n        ###\n\n        # Get connected component ids.\n        list_of_connected_component_ids: List[str] = [component[0] for component in list_of_connected_components]\n\n        # Keep only components that have more that one `\"CANNOT_LINK\"` constraints.\n        list_of_linked_connected_components_ids: List[str] = [\n            component_id\n            for component_id in list_of_connected_component_ids\n            if len(self._constraints_transitivity[component_id][\"CANNOT_LINK\"].keys()) &gt; 1  # noqa: WPS507\n        ]\n\n        # Get the `\"CANNOT_LINK\"` constraints.\n        list_of_cannot_link_constraints: List[Tuple[int, int]] = [\n            (i1, i2)\n            for i1, data_ID1 in enumerate(list_of_linked_connected_components_ids)\n            for i2, data_ID2 in enumerate(list_of_linked_connected_components_ids)\n            if (i1 &lt; i2)\n            and (  # To get the complement, get all possible link that are not a `\"CANNOT_LINK\"`.\n                data_ID2 in self._constraints_transitivity[data_ID1][\"CANNOT_LINK\"].keys()\n            )\n        ]\n\n        # Create a networkx graph.\n        cannot_link_graph: nx.Graph = nx.Graph()\n        cannot_link_graph.add_nodes_from(list_of_connected_component_ids)  # Add components id as nodes in the graph.\n        cannot_link_graph.add_edges_from(\n            list_of_cannot_link_constraints\n        )  # Add cannot link constraints as edges in the graph.\n\n        # Estimate the minimum clusters number by trying to colorate the `\"CANNOT_LINK\"` constraints graph.\n        # The lower bound has to be greater than 2.\n        estimation_of_minimum_clusters_number: int = max(\n            2,\n            1\n            + min(\n                max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"largest_first\").values()),\n                max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"smallest_last\").values()),\n                max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"random_sequential\").values()),\n                max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"random_sequential\").values()),\n                max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"random_sequential\").values()),\n            ),\n        )\n\n        ###\n        ### 2. Computation of maximum clusters number.\n        ###\n\n        # Determine the maximum clusters number with the number of `\"MUST_LINK\"` connected components.\n        maximum_clusters_number: int = len(list_of_connected_components)\n\n        # Return minimum and maximum.\n        return (estimation_of_minimum_clusters_number, maximum_clusters_number)\n\n    # ==============================================================================\n    # CONSTRAINTS TRANSITIVITY MANAGEMENT - GENERATE CONSTRAINTS TRANSITIVITY GRAPH\n    # ==============================================================================\n    def _generate_constraints_transitivity(\n        self,\n    ) -&gt; None:\n        \"\"\"\n        Generate `self._constraints_transitivity`, a constraints dictionary that takes into account the transitivity of constraints.\n        Suppose there is no inconsistency in `self._constraints_dictionary`.\n        It uses `Dict[str, None]` to simulate ordered sets.\n        \"\"\"\n\n        # Reset constraints transitivity.\n        self._constraints_transitivity = {\n            data_ID: {\n                \"MUST_LINK\": {data_ID: None},  # Initialize MUST_LINK clusters constraints.\n                \"CANNOT_LINK\": {},  # Initialize CANNOT_LINK clusters constraints.\n            }\n            for data_ID in self._constraints_dictionary.keys()\n        }\n\n        for data_ID1 in self._constraints_dictionary.keys():\n            for data_ID2 in self._constraints_dictionary[data_ID1].keys():\n                # Get the constraint between `data_ID1` and `data_ID2`.\n                constraint = self._constraints_dictionary[data_ID1][data_ID2]\n\n                # Add the constraint transitivity if the constraint is not `None`.\n                if constraint is not None:\n                    self._add_constraint_transitivity(\n                        data_ID1=data_ID1,\n                        data_ID2=data_ID2,\n                        constraint_type=constraint[0],\n                    )\n\n    # ==============================================================================\n    # CONSTRAINTS TRANSITIVITY MANAGEMENT - ADD CONSTRAINT TRANSITIVITY\n    # ==============================================================================\n    def _add_constraint_transitivity(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n        constraint_type: str,\n    ) -&gt; bool:\n        \"\"\"\n        Add constraint transitivity in `self._constraints_transitivity` between `data_ID1` and `data_ID2` for constraint type `constraint_type`.\n        Suppose there is no inconsistency in `self._constraints_dictionary`.\n\n        Args:\n            data_ID1 (str): The first data ID that is concerned for this constraint transitivity addition.\n            data_ID2 (str): The second data ID that is concerned for this constraint transitivity addition.\n            constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n\n        Returns:\n            bool: `True` when the transitivity addition is done.\n        \"\"\"\n\n        ###\n        ### Case 1 : `constraint_type` is `\"MUST_LINK\"`.\n        ###\n        if constraint_type == \"MUST_LINK\":\n            # Define new common set of `\"MUST_LINK\"` data IDs,\n            # by merging the sets of `\"MUST_LINK\"` data IDs for `data_ID1` and `data_ID2`.\n            new_MUST_LINK_common_set: Dict[str, None] = {\n                **self._constraints_transitivity[data_ID1][\"MUST_LINK\"],\n                **self._constraints_transitivity[data_ID2][\"MUST_LINK\"],\n            }\n\n            # Define new common set of `\"CANNOT_LINK\"` data IDs,\n            # by merging the sets of `\"CANNOT_LINK\"` data IDs for `data_ID1` and `data_ID2`.\n            new_CANNOT_LINK_common_set: Dict[str, None] = {\n                **self._constraints_transitivity[data_ID1][\"CANNOT_LINK\"],\n                **self._constraints_transitivity[data_ID2][\"CANNOT_LINK\"],\n            }\n\n            # For each data that are now similar to `data_ID1` and `data_ID2`...\n            for data_ID_ML in new_MUST_LINK_common_set.keys():\n                # ... affect the new set of `\"MUST_LINK\"` constraints...\n                self._constraints_transitivity[data_ID_ML][\"MUST_LINK\"] = new_MUST_LINK_common_set\n                # ... and affect the new set of `\"CANNOT_LINK\"` constraints.\n                self._constraints_transitivity[data_ID_ML][\"CANNOT_LINK\"] = new_CANNOT_LINK_common_set\n\n            # For each data that are now different to `data_ID1` and `data_ID2`...\n            for data_ID_CL in new_CANNOT_LINK_common_set.keys():\n                # ... affect the new set of `\"CANNOT_LINK\"` constraints.\n                self._constraints_transitivity[data_ID_CL][\"CANNOT_LINK\"] = {\n                    **self._constraints_transitivity[data_ID_CL][\"CANNOT_LINK\"],\n                    **new_MUST_LINK_common_set,\n                }\n\n        ###\n        ### Case 2 : `constraint_type` is `\"CANNOT_LINK\"`.\n        ###\n        else:  # if constraint_type == \"CANNOT_LINK\":\n            # Define new common set of `\"CANNOT_LINK\"` data IDs for data IDs that are similar to `data_ID1`.\n            new_CANNOT_LINK_set_for_data_ID1: Dict[str, None] = {\n                **self._constraints_transitivity[data_ID1][\"CANNOT_LINK\"],\n                **self._constraints_transitivity[data_ID2][\"MUST_LINK\"],\n            }\n\n            # Define new common set of `\"CANNOT_LINK\"` data IDs for data IDs that are similar to `data_ID2`.\n            new_CANNOT_LINK_set_for_data_ID2: Dict[str, None] = {\n                **self._constraints_transitivity[data_ID2][\"CANNOT_LINK\"],\n                **self._constraints_transitivity[data_ID1][\"MUST_LINK\"],\n            }\n\n            # For each data that are similar to `data_ID1`...\n            for data_ID_like_data_ID1 in self._constraints_transitivity[data_ID1][\"MUST_LINK\"].keys():\n                # ... affect the new list of `\"CANNOT_LINK\"` constraints.\n                self._constraints_transitivity[data_ID_like_data_ID1][\"CANNOT_LINK\"] = new_CANNOT_LINK_set_for_data_ID1\n\n            # For each data that are similar to `data_ID2`...\n            for data_ID_like_data_ID2 in self._constraints_transitivity[data_ID2][\"MUST_LINK\"].keys():\n                # ... affect the new list of `\"CANNOT_LINK\"` constraints.\n                self._constraints_transitivity[data_ID_like_data_ID2][\"CANNOT_LINK\"] = new_CANNOT_LINK_set_for_data_ID2\n\n        # Return `True`\n        return True\n\n    # ==============================================================================\n    # CONSTRAINTS CONFLICT - GET INVOLVED DATA IDS IN A CONFLICT\n    # ==============================================================================\n    def get_list_of_involved_data_IDs_in_a_constraint_conflict(\n        self,\n        data_ID1: str,\n        data_ID2: str,\n        constraint_type: str,\n    ) -&gt; Optional[List[str]]:\n        \"\"\"\n        Get all data IDs involved in a constraints conflict.\n\n        Args:\n            data_ID1 (str): The first data ID involved in the constraint_conflit.\n            data_ID2 (str): The second data ID involved in the constraint_conflit.\n            constraint_type (str): The constraint that create a conflict. The constraints can be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n\n        Raises:\n            ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed.\n\n        Returns:\n            Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of `data_ID1` and `data_ID2`.\n        \"\"\"\n\n        # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID1 not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n        # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n        if data_ID2 not in self._constraints_dictionary.keys():\n            raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n        # If the `constraint_conflict` is not in `self._allowed_constraint_types`, then raises a `ValueError`.\n        if constraint_type not in self._allowed_constraint_types:\n            raise ValueError(\n                \"The `constraint_type` `'\"\n                + str(constraint_type)\n                + \"'` is not managed. Allowed constraints types are : `\"\n                + str(self._allowed_constraint_types)\n                + \"`.\"\n            )\n\n        # Case of conflict (after trying to add a constraint different from the inferred constraint).\n        if self.get_inferred_constraint(\n            data_ID1=data_ID1, data_ID2=data_ID2\n        ) is not None and constraint_type != self.get_inferred_constraint(data_ID1=data_ID1, data_ID2=data_ID2):\n            return [\n                data_ID\n                for connected_component in self.get_connected_components()  # Get involved components.\n                for data_ID in connected_component  # Get data IDs from these components.\n                if (data_ID1 in connected_component or data_ID2 in connected_component)\n            ]\n\n        # Case of no conflict.\n        return None\n\n    # ==============================================================================\n    # SERIALIZATION - TO JSON\n    # ==============================================================================\n    def to_json(\n        self,\n        filepath: str = \"./constraint_manager.json\",\n    ) -&gt; bool:\n        \"\"\"\n        The main method used to serialize the constraints manager object into a JSON file.\n\n        Args:\n            filepath (str): The path where to serialize the constraints manager  object.\n\n        Returns:\n            bool: `True` if the serialization is done.\n        \"\"\"\n\n        # Serialize constraints manager.\n        with open(filepath, \"w\") as fileobject:\n            json.dump(\n                {\n                    \"list_of_managed_data_IDs\": self.get_list_of_managed_data_IDs(),\n                    \"list_of_added_constraints\": [\n                        {\n                            \"data_ID1\": data_ID1,\n                            \"data_ID2\": data_ID2,\n                            \"constraint_type\": constraint[0],\n                            \"constraints_value\": 1.0,  # Binary constraints manager, so force 1.0.\n                        }\n                        for data_ID1 in self._constraints_dictionary.keys()\n                        for data_ID2, constraint in self._constraints_dictionary[data_ID1].items()\n                        if (constraint is not None)\n                    ],\n                },\n                fileobject,\n                indent=1,\n            )\n\n        # Return.\n        return True\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.__init__","title":"<code>__init__(list_of_data_IDs, **kargs)</code>","text":"<p>The constructor for Binary Constraints Manager class. This class use the strong transitivity to infer on constraints, so constraints values are not taken into account.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_data_IDs</code> <code>List[str]</code> <p>The list of data IDs to manage.</p> required <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def __init__(self, list_of_data_IDs: List[str], **kargs) -&gt; None:\n    \"\"\"\n    The constructor for Binary Constraints Manager class.\n    This class use the strong transitivity to infer on constraints, so constraints values are not taken into account.\n\n    Args:\n        list_of_data_IDs (List[str]): The list of data IDs to manage.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n    \"\"\"\n\n    # Define `self._allowed_constraint_types`.\n    self._allowed_constraint_types: Set[str] = {\n        \"MUST_LINK\",\n        \"CANNOT_LINK\",\n    }\n    # Define `self._allowed_constraint_value_range`.\n    self._allowed_constraint_value_range: Dict[str, float] = {\n        \"min\": 1.0,\n        \"max\": 1.0,\n    }\n\n    # Store `self.kargs` for binary constraints managing.\n    self.kargs = kargs\n\n    # Initialize `self._constraints_dictionary`.\n    self._constraints_dictionary: Dict[str, Dict[str, Optional[Tuple[str, float]]]] = {\n        data_ID1: {\n            data_ID2: (\n                (\"MUST_LINK\", 1.0)\n                if (data_ID1 == data_ID2)\n                else None  # Unknwon constraints if `data_ID1` != `data_ID2`.\n            )\n            for data_ID2 in list_of_data_IDs\n            if (data_ID1 &lt;= data_ID2)\n        }\n        for data_ID1 in list_of_data_IDs\n    }\n\n    # Define `self._constraints_transitivity`.\n    # `Equivalent to `self._generate_constraints_transitivity()`\n    self._constraints_transitivity: Dict[str, Dict[str, Dict[str, None]]] = {\n        data_ID: {\n            \"MUST_LINK\": {data_ID: None},  # Initialize MUST_LINK clusters constraints.\n            \"CANNOT_LINK\": {},  # Initialize CANNOT_LINK clusters constraints.\n        }\n        for data_ID in list_of_data_IDs\n    }\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.add_constraint","title":"<code>add_constraint(data_ID1, data_ID2, constraint_type, constraint_value=1.0)</code>","text":"<p>The main method used to add a constraint between two data IDs.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID that is concerned for this constraint addition.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID that is concerned for this constraint addition.</p> required <code>constraint_type</code> <code>str</code> <p>The type of the constraint to add. The type have to be <code>\"MUST_LINK\"</code> or <code>\"CANNOT_LINK\"</code>.</p> required <code>constraint_value</code> <code>float</code> <p>The value of the constraint to add. The value have to be in range <code>[0.0, 1.0]</code>. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code>, <code>data_ID2</code>, <code>constraint_type</code> are not managed, or if a conflict is detected with constraints inference.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the addition is done, <code>False</code> is the constraint can't be added.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def add_constraint(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n    constraint_type: str,\n    constraint_value: float = 1.0,\n) -&gt; bool:\n    \"\"\"\n    The main method used to add a constraint between two data IDs.\n\n    Args:\n        data_ID1 (str): The first data ID that is concerned for this constraint addition.\n        data_ID2 (str): The second data ID that is concerned for this constraint addition.\n        constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n        constraint_value (float, optional): The value of the constraint to add. The value have to be in range `[0.0, 1.0]`. Defaults to `1.0`.\n\n    Raises:\n        ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed, or if a conflict is detected with constraints inference.\n\n    Returns:\n        bool: `True` if the addition is done, `False` is the constraint can't be added.\n    \"\"\"\n\n    # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID1 not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n    # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID2 not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n    # If the `constraint_type` is not in `self._allowed_constraint_types`, then raises a `ValueError`.\n    if constraint_type not in self._allowed_constraint_types:\n        raise ValueError(\n            \"The `constraint_type` `'\"\n            + str(constraint_type)\n            + \"'` is not managed. Allowed constraints types are : `\"\n            + str(self._allowed_constraint_types)\n            + \"`.\"\n        )\n\n    # Get current added constraint between `data_ID1` and `data_ID2`.\n    inferred_constraint: Optional[str] = self.get_inferred_constraint(\n        data_ID1=data_ID1,\n        data_ID2=data_ID2,\n    )\n\n    # Case of conflict with constraints inference.\n    if (inferred_constraint is not None) and (inferred_constraint != constraint_type):\n        raise ValueError(\n            \"The `constraint_type` `'\"\n            + str(constraint_type)\n            + \"'` is incompatible with the inferred constraint `'\"\n            + str(inferred_constraint)\n            + \"'` between data IDs `'\"\n            + data_ID1\n            + \"'` and `'\"\n            + data_ID2\n            + \"'`.\"\n        )\n\n    # Get current added constraint between `data_ID1` and `data_ID2`.\n    added_constraint: Optional[Tuple[str, float]] = self.get_added_constraint(\n        data_ID1=data_ID1,\n        data_ID2=data_ID2,\n    )\n\n    # If the constraint has already be added, ...\n    if added_constraint is not None:\n        # ... do nothing.\n        return True  # `added_constraint[0] == constraint_type`.\n    # Otherwise, the constraint has to be added.\n\n    # Add the direct constraint between `data_ID1` and `data_ID2`.\n    if data_ID1 &lt;= data_ID2:\n        self._constraints_dictionary[data_ID1][data_ID2] = (constraint_type, 1.0)\n    else:\n        self._constraints_dictionary[data_ID2][data_ID1] = (constraint_type, 1.0)\n\n    # Add the transitivity constraint between `data_ID1` and `data_ID2`.\n    self._add_constraint_transitivity(\n        data_ID1=data_ID1,\n        data_ID2=data_ID2,\n        constraint_type=constraint_type,\n    )\n\n    return True\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.add_data_ID","title":"<code>add_data_ID(data_ID)</code>","text":"<p>The main method used to add a new data ID to manage.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID</code> <code>str</code> <p>The data ID to manage.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID</code> is already managed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the addition is done.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def add_data_ID(\n    self,\n    data_ID: str,\n) -&gt; bool:\n    \"\"\"\n    The main method used to add a new data ID to manage.\n\n    Args:\n        data_ID (str): The data ID to manage.\n\n    Raises:\n        ValueError: if `data_ID` is already managed.\n\n    Returns:\n        bool: `True` if the addition is done.\n    \"\"\"\n\n    # If `data_ID` is in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID` `'\" + str(data_ID) + \"'` is already managed.\")\n\n    # Add `data_ID` to `self._constraints_dictionary.keys()`.\n    self._constraints_dictionary[data_ID] = {}\n\n    # Define constraint for `data_ID` and all other data IDs.\n    for other_data_ID in self._constraints_dictionary.keys():\n        if data_ID == other_data_ID:\n            self._constraints_dictionary[data_ID][data_ID] = (\"MUST_LINK\", 1.0)\n        elif data_ID &lt; other_data_ID:\n            self._constraints_dictionary[data_ID][other_data_ID] = None\n        else:  # elif data_ID &gt; other_data_ID:\n            self._constraints_dictionary[other_data_ID][data_ID] = None\n\n    # Regenerate `self._constraints_transitivity`.\n    # `Equivalent to `self._generate_constraints_transitivity()`\n    self._constraints_transitivity[data_ID] = {\n        \"MUST_LINK\": {data_ID: None},\n        \"CANNOT_LINK\": {},\n    }\n\n    # Return `True`.\n    return True\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.check_completude_of_constraints","title":"<code>check_completude_of_constraints(threshold=1.0)</code>","text":"<p>The main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the <code>threshold</code> parameters is used if constraints values are used in the constraints transitivity.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold used to define the transitivity link. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Return <code>True</code> if all constraints are known, <code>False</code> otherwise.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def check_completude_of_constraints(\n    self,\n    threshold: float = 1.0,\n) -&gt; bool:\n    \"\"\"\n    The main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity).\n    The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity.\n\n    Args:\n        threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`.\n\n    Returns:\n        bool: Return `True` if all constraints are known, `False` otherwise.\n    \"\"\"\n\n    # For each data ID...\n    for data_ID in self._constraints_transitivity.keys():\n        # ... if some data IDs are not linked by transitivity to this `data_ID` with a `\"MUST_LINK\"` or `\"CANNOT_LINK\"` constraints...\n        if (\n            len(self._constraints_transitivity[data_ID][\"MUST_LINK\"].keys())\n            + len(self._constraints_transitivity[data_ID][\"CANNOT_LINK\"].keys())\n        ) != len(self._constraints_transitivity.keys()):\n            # ... then return `False`.\n            return False\n\n    # Otherwise, return `True`.\n    return True\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.delete_constraint","title":"<code>delete_constraint(data_ID1, data_ID2)</code>","text":"<p>The main method used to delete a constraint between two data IDs.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID that is concerned for this constraint deletion.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID that is concerned for this constraint deletion.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code> or <code>data_ID2</code> are not managed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the deletion is done, <code>False</code> if the constraint can't be deleted.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def delete_constraint(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n) -&gt; bool:\n    \"\"\"\n    The main method used to delete a constraint between two data IDs.\n\n    Args:\n        data_ID1 (str): The first data ID that is concerned for this constraint deletion.\n        data_ID2 (str): The second data ID that is concerned for this constraint deletion.\n\n    Raises:\n        ValueError: if `data_ID1` or `data_ID2` are not managed.\n\n    Returns:\n        bool: `True` if the deletion is done, `False` if the constraint can't be deleted.\n    \"\"\"\n\n    # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID1 not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n    # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID2 not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n    # Delete the constraint between `data_ID1` and `data_ID2`.\n    if data_ID1 &lt;= data_ID2:\n        self._constraints_dictionary[data_ID1][data_ID2] = None\n    else:\n        self._constraints_dictionary[data_ID2][data_ID1] = None\n\n    # Regenerate `self._constraints_transitivity`.\n    self._generate_constraints_transitivity()\n\n    # Return `True`\n    return True\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.delete_data_ID","title":"<code>delete_data_ID(data_ID)</code>","text":"<p>The main method used to delete a data ID to no longer manage.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID</code> <code>str</code> <p>The data ID to no longer manage.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID</code> is not managed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the deletion is done.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def delete_data_ID(\n    self,\n    data_ID: str,\n) -&gt; bool:\n    \"\"\"\n    The main method used to delete a data ID to no longer manage.\n\n    Args:\n        data_ID (str): The data ID to no longer manage.\n\n    Raises:\n        ValueError: if `data_ID` is not managed.\n\n    Returns:\n        bool: `True` if the deletion is done.\n    \"\"\"\n\n    # If `data_ID` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID` `'\" + str(data_ID) + \"'` is not managed.\")\n\n    # Remove `data_ID` from `self._constraints_dictionary.keys()`.\n    self._constraints_dictionary.pop(data_ID, None)\n\n    # Remove `data_ID` from all `self._constraints_dictionary[other_data_ID].keys()`.\n    for other_data_ID in self._constraints_dictionary.keys():\n        self._constraints_dictionary[other_data_ID].pop(data_ID, None)\n\n    # Regenerate `self._constraints_transitivity`\n    self._generate_constraints_transitivity()\n\n    # Return `True`.\n    return True\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_added_constraint","title":"<code>get_added_constraint(data_ID1, data_ID2)</code>","text":"<p>The main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID that is concerned for this constraint.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID that is concerned for this constraint.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code> or <code>data_ID2</code> are not managed.</p> <p>Returns:</p> Type Description <code>Optional[Tuple[str, float]]</code> <p>Optional[Tuple[str, float]]: <code>None</code> if no constraint, <code>(constraint_type, constraint_value)</code> otherwise.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def get_added_constraint(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n) -&gt; Optional[Tuple[str, float]]:\n    \"\"\"\n    The main method used to get the constraint added between the two data IDs.\n    Do not take into account the constraints transitivity, just look at constraints that are explicitly added.\n\n    Args:\n        data_ID1 (str): The first data ID that is concerned for this constraint.\n        data_ID2 (str): The second data ID that is concerned for this constraint.\n\n    Raises:\n        ValueError: if `data_ID1` or `data_ID2` are not managed.\n\n    Returns:\n        Optional[Tuple[str, float]]: `None` if no constraint, `(constraint_type, constraint_value)` otherwise.\n    \"\"\"\n\n    # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID1 not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n    # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID2 not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n    # Retrun the current added constraint type and value.\n    return (\n        self._constraints_dictionary[data_ID1][data_ID2]\n        if (data_ID1 &lt;= data_ID2)\n        else self._constraints_dictionary[data_ID2][data_ID1]\n    )\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_connected_components","title":"<code>get_connected_components(threshold=1.0)</code>","text":"<p>The main method used to get the possible lists of data IDs that are linked by a <code>\"MUST_LINK\"</code> constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the <code>threshold</code> parameters is used if constraints values are used in the constraints transitivity.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold used to define the transitivity link. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def get_connected_components(\n    self,\n    threshold: float = 1.0,\n) -&gt; List[List[str]]:\n    \"\"\"\n    The main method used to get the possible lists of data IDs that are linked by a `\"MUST_LINK\"` constraints.\n    Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs.\n    The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity.\n\n    Args:\n        threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`.\n\n    Returns:\n        List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph.\n    \"\"\"\n\n    # Initialize the list of connected components.\n    list_of_connected_components: List[List[str]] = []\n\n    # For each data ID...\n    for data_ID in self._constraints_transitivity.keys():\n        # ... get the list of `\"MUST_LINK\"` data IDs linked by transitivity with `data_ID` ...\n        connected_component_of_a_data_ID = list(self._constraints_transitivity[data_ID][\"MUST_LINK\"].keys())\n\n        # ... and if the connected component is not already get...\n        if connected_component_of_a_data_ID not in list_of_connected_components:\n            # ... then add it to the list of connected components.\n            list_of_connected_components.append(connected_component_of_a_data_ID)\n\n    # Return the list of connected components.\n    return list_of_connected_components\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_inferred_constraint","title":"<code>get_inferred_constraint(data_ID1, data_ID2, threshold=1.0)</code>","text":"<p>The main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the <code>threshold</code> parameter is used to evaluate the impact of constraints transitivity.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID that is concerned for this constraint.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID that is concerned for this constraint.</p> required <code>threshold</code> <code>float</code> <p>The threshold used to evaluate the impact of constraints transitivity link. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code>, <code>data_ID2</code> or <code>threshold</code> are not managed.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The type of the inferred constraint. The type can be <code>None</code>, <code>\"MUST_LINK\"</code> or <code>\"CANNOT_LINK\"</code>.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def get_inferred_constraint(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n    threshold: float = 1.0,\n) -&gt; Optional[str]:\n    \"\"\"\n    The main method used to check if the constraint inferred by transitivity between the two data IDs.\n    The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity.\n\n    Args:\n        data_ID1 (str): The first data ID that is concerned for this constraint.\n        data_ID2 (str): The second data ID that is concerned for this constraint.\n        threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`.\n\n    Raises:\n        ValueError: if `data_ID1`, `data_ID2` or `threshold` are not managed.\n\n    Returns:\n        Optional[str]: The type of the inferred constraint. The type can be `None`, `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n    \"\"\"\n\n    # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID1 not in self._constraints_transitivity.keys():\n        raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n    # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID2 not in self._constraints_transitivity.keys():\n        raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n    # Case of `\"MUST_LINK\"`.\n    if data_ID1 in self._constraints_transitivity[data_ID2][\"MUST_LINK\"].keys():\n        return \"MUST_LINK\"\n\n    # Case of `\"CANNOT_LINK\"`.\n    if data_ID1 in self._constraints_transitivity[data_ID2][\"CANNOT_LINK\"].keys():\n        return \"CANNOT_LINK\"\n\n    # Case of `None`.\n    return None\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_list_of_involved_data_IDs_in_a_constraint_conflict","title":"<code>get_list_of_involved_data_IDs_in_a_constraint_conflict(data_ID1, data_ID2, constraint_type)</code>","text":"<p>Get all data IDs involved in a constraints conflict.</p> <p>Parameters:</p> Name Type Description Default <code>data_ID1</code> <code>str</code> <p>The first data ID involved in the constraint_conflit.</p> required <code>data_ID2</code> <code>str</code> <p>The second data ID involved in the constraint_conflit.</p> required <code>constraint_type</code> <code>str</code> <p>The constraint that create a conflict. The constraints can be <code>\"MUST_LINK\"</code> or <code>\"CANNOT_LINK\"</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>data_ID1</code>, <code>data_ID2</code>, <code>constraint_type</code> are not managed.</p> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of <code>data_ID1</code> and <code>data_ID2</code>.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def get_list_of_involved_data_IDs_in_a_constraint_conflict(\n    self,\n    data_ID1: str,\n    data_ID2: str,\n    constraint_type: str,\n) -&gt; Optional[List[str]]:\n    \"\"\"\n    Get all data IDs involved in a constraints conflict.\n\n    Args:\n        data_ID1 (str): The first data ID involved in the constraint_conflit.\n        data_ID2 (str): The second data ID involved in the constraint_conflit.\n        constraint_type (str): The constraint that create a conflict. The constraints can be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`.\n\n    Raises:\n        ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed.\n\n    Returns:\n        Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of `data_ID1` and `data_ID2`.\n    \"\"\"\n\n    # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID1 not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID1` `'\" + str(data_ID1) + \"'` is not managed.\")\n\n    # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`.\n    if data_ID2 not in self._constraints_dictionary.keys():\n        raise ValueError(\"The `data_ID2` `'\" + str(data_ID2) + \"'` is not managed.\")\n\n    # If the `constraint_conflict` is not in `self._allowed_constraint_types`, then raises a `ValueError`.\n    if constraint_type not in self._allowed_constraint_types:\n        raise ValueError(\n            \"The `constraint_type` `'\"\n            + str(constraint_type)\n            + \"'` is not managed. Allowed constraints types are : `\"\n            + str(self._allowed_constraint_types)\n            + \"`.\"\n        )\n\n    # Case of conflict (after trying to add a constraint different from the inferred constraint).\n    if self.get_inferred_constraint(\n        data_ID1=data_ID1, data_ID2=data_ID2\n    ) is not None and constraint_type != self.get_inferred_constraint(data_ID1=data_ID1, data_ID2=data_ID2):\n        return [\n            data_ID\n            for connected_component in self.get_connected_components()  # Get involved components.\n            for data_ID in connected_component  # Get data IDs from these components.\n            if (data_ID1 in connected_component or data_ID2 in connected_component)\n        ]\n\n    # Case of no conflict.\n    return None\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_list_of_managed_data_IDs","title":"<code>get_list_of_managed_data_IDs()</code>","text":"<p>The main method used to get the list of data IDs that are managed.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of data IDs that are managed.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def get_list_of_managed_data_IDs(\n    self,\n) -&gt; List[str]:\n    \"\"\"\n    The main method used to get the list of data IDs that are managed.\n\n    Returns:\n        List[str]: The list of data IDs that are managed.\n    \"\"\"\n\n    # Return the possible keys of `self._constraints_dictionary`.\n    return list(self._constraints_dictionary.keys())\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_min_and_max_number_of_clusters","title":"<code>get_min_and_max_number_of_clusters(threshold=1.0)</code>","text":"<p>The main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. Minimum number of cluster is estimated by the coloration of the <code>\"CANNOT_LINK\"</code> constraints graph. Maximum number of cluster is defined by the number of <code>\"MUST_LINK\"</code> connected components. The transitivity is taken into account, and the <code>threshold</code> parameters is used if constraints values are used in the constraints transitivity.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold used to define the transitivity link. Defaults to <code>1.0</code>.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints).</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def get_min_and_max_number_of_clusters(\n    self,\n    threshold: float = 1.0,\n) -&gt; Tuple[int, int]:\n    \"\"\"\n    The main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number.\n    Minimum number of cluster is estimated by the coloration of the `\"CANNOT_LINK\"` constraints graph.\n    Maximum number of cluster is defined by the number of `\"MUST_LINK\"` connected components.\n    The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity.\n\n    Args:\n        threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`.\n\n    Returns:\n        Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints).\n    \"\"\"\n\n    # Get the `\"MUST_LINK\"` connected components.\n    list_of_connected_components: List[List[str]] = self.get_connected_components()\n\n    ###\n    ### 1. Estimation of minimum clusters number.\n    ###\n\n    # Get connected component ids.\n    list_of_connected_component_ids: List[str] = [component[0] for component in list_of_connected_components]\n\n    # Keep only components that have more that one `\"CANNOT_LINK\"` constraints.\n    list_of_linked_connected_components_ids: List[str] = [\n        component_id\n        for component_id in list_of_connected_component_ids\n        if len(self._constraints_transitivity[component_id][\"CANNOT_LINK\"].keys()) &gt; 1  # noqa: WPS507\n    ]\n\n    # Get the `\"CANNOT_LINK\"` constraints.\n    list_of_cannot_link_constraints: List[Tuple[int, int]] = [\n        (i1, i2)\n        for i1, data_ID1 in enumerate(list_of_linked_connected_components_ids)\n        for i2, data_ID2 in enumerate(list_of_linked_connected_components_ids)\n        if (i1 &lt; i2)\n        and (  # To get the complement, get all possible link that are not a `\"CANNOT_LINK\"`.\n            data_ID2 in self._constraints_transitivity[data_ID1][\"CANNOT_LINK\"].keys()\n        )\n    ]\n\n    # Create a networkx graph.\n    cannot_link_graph: nx.Graph = nx.Graph()\n    cannot_link_graph.add_nodes_from(list_of_connected_component_ids)  # Add components id as nodes in the graph.\n    cannot_link_graph.add_edges_from(\n        list_of_cannot_link_constraints\n    )  # Add cannot link constraints as edges in the graph.\n\n    # Estimate the minimum clusters number by trying to colorate the `\"CANNOT_LINK\"` constraints graph.\n    # The lower bound has to be greater than 2.\n    estimation_of_minimum_clusters_number: int = max(\n        2,\n        1\n        + min(\n            max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"largest_first\").values()),\n            max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"smallest_last\").values()),\n            max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"random_sequential\").values()),\n            max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"random_sequential\").values()),\n            max(nx.coloring.greedy_color(cannot_link_graph, strategy=\"random_sequential\").values()),\n        ),\n    )\n\n    ###\n    ### 2. Computation of maximum clusters number.\n    ###\n\n    # Determine the maximum clusters number with the number of `\"MUST_LINK\"` connected components.\n    maximum_clusters_number: int = len(list_of_connected_components)\n\n    # Return minimum and maximum.\n    return (estimation_of_minimum_clusters_number, maximum_clusters_number)\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.to_json","title":"<code>to_json(filepath='./constraint_manager.json')</code>","text":"<p>The main method used to serialize the constraints manager object into a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path where to serialize the constraints manager  object.</p> <code>'./constraint_manager.json'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p><code>True</code> if the serialization is done.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def to_json(\n    self,\n    filepath: str = \"./constraint_manager.json\",\n) -&gt; bool:\n    \"\"\"\n    The main method used to serialize the constraints manager object into a JSON file.\n\n    Args:\n        filepath (str): The path where to serialize the constraints manager  object.\n\n    Returns:\n        bool: `True` if the serialization is done.\n    \"\"\"\n\n    # Serialize constraints manager.\n    with open(filepath, \"w\") as fileobject:\n        json.dump(\n            {\n                \"list_of_managed_data_IDs\": self.get_list_of_managed_data_IDs(),\n                \"list_of_added_constraints\": [\n                    {\n                        \"data_ID1\": data_ID1,\n                        \"data_ID2\": data_ID2,\n                        \"constraint_type\": constraint[0],\n                        \"constraints_value\": 1.0,  # Binary constraints manager, so force 1.0.\n                    }\n                    for data_ID1 in self._constraints_dictionary.keys()\n                    for data_ID2, constraint in self._constraints_dictionary[data_ID1].items()\n                    if (constraint is not None)\n                ],\n            },\n            fileobject,\n            indent=1,\n        )\n\n    # Return.\n    return True\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.load_constraints_manager_from_json","title":"<code>load_constraints_manager_from_json(filepath)</code>","text":"<p>The main method used initialize a constraints manager from a deserialized one.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path where is the deserialized constraints manager object.</p> required <p>Returns:</p> Name Type Description <code>BinaryConstraintsManager</code> <code>BinaryConstraintsManager</code> <p>The deserialized constraints manager.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\binary.py</code> <pre><code>def load_constraints_manager_from_json(\n    filepath: str,\n) -&gt; BinaryConstraintsManager:\n    \"\"\"\n    The main method used initialize a constraints manager from a deserialized one.\n\n    Args:\n        filepath (str): The path where is the deserialized constraints manager object.\n\n    Returns:\n        BinaryConstraintsManager: The deserialized constraints manager.\n    \"\"\"\n\n    # Deserialize constraints manager attributes.\n    with open(filepath, \"r\") as fileobject:\n        attributes_from_json: Dict[str, Any] = json.load(fileobject)\n    # list_of_managed_data_IDs: List[str] = attributes_from_json[\"list_of_managed_data_IDs\"]\n    # list_of_added_constraints: List[Dict[str, Any]] = attributes_from_json[\"list_of_added_constraints\"]\n\n    # Initialize blank constraints manager.\n    constraints_manager: BinaryConstraintsManager = BinaryConstraintsManager(\n        list_of_data_IDs=attributes_from_json[\"list_of_managed_data_IDs\"],\n    )\n\n    # Load from json.\n    for constraint in attributes_from_json[\"list_of_added_constraints\"]:\n        constraints_manager.add_constraint(\n            data_ID1=constraint[\"data_ID1\"],\n            data_ID2=constraint[\"data_ID2\"],\n            constraint_type=constraint[\"constraint_type\"],\n            # constraint_value=constraint[\"constraint_value\"],  # Binary constraints manager, so force 1.0.\n        )\n\n    # Return the constraints manager.\n    return constraints_manager\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/factory/","title":"factory","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.constraints.factory</li> <li>Description:  The factory method used to easily initialize a constraints manager.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL-C License v1.0 (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/constraints/factory/#cognitivefactory.interactive_clustering.constraints.factory.managing_factory","title":"<code>managing_factory(list_of_data_IDs, manager='binary', **kargs)</code>","text":"<p>A factory to create a new instance of a constraints manager.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_data_IDs</code> <code>List[str]</code> <p>The list of data IDs to manage.</p> required <code>manager</code> <code>str</code> <p>The identification of constraints manager to instantiate. Can be \"binary\". Defaults to <code>\"binary\"</code>.</p> <code>'binary'</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>manager</code> is not implemented.</p> <p>Returns:</p> Name Type Description <code>AbstractConstraintsManager</code> <code>AbstractConstraintsManager</code> <p>An instance of constraints manager.</p> Example <pre><code># Import.\nfrom cognitivefactory.interactive_clustering.constraints.factory import managing_factory\n\n# Create an instance of binary constraints manager.\nconstraints_manager = managing_factory(\n    list_of_data_IDs=[\"0\", \"1\", \"2\", \"3\", \"4\"],\n    manager=\"binary\",\n)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\constraints\\factory.py</code> <pre><code>def managing_factory(list_of_data_IDs: List[str], manager: str = \"binary\", **kargs) -&gt; AbstractConstraintsManager:\n    \"\"\"\n    A factory to create a new instance of a constraints manager.\n\n    Args:\n        list_of_data_IDs (List[str]): The list of data IDs to manage.\n        manager (str, optional): The identification of constraints manager to instantiate. Can be \"binary\". Defaults to `\"binary\"`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Raises:\n        ValueError: if `manager` is not implemented.\n\n    Returns:\n        AbstractConstraintsManager : An instance of constraints manager.\n\n    Example:\n        ```python\n        # Import.\n        from cognitivefactory.interactive_clustering.constraints.factory import managing_factory\n\n        # Create an instance of binary constraints manager.\n        constraints_manager = managing_factory(\n            list_of_data_IDs=[\"0\", \"1\", \"2\", \"3\", \"4\"],\n            manager=\"binary\",\n        )\n        ```\n    \"\"\"\n\n    # Check that the requested algorithm is implemented.\n    if manager != \"binary\":  # TODO use `not in {\"binary\"}`.\n        raise ValueError(\"The `manager` '\" + str(manager) + \"' is not implemented.\")\n\n    # Case of Binary Constraints Manager\n    ## if manager==\"binary\":\n\n    return BinaryConstraintsManager(list_of_data_IDs=list_of_data_IDs, **kargs)\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/","title":"sampling","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.sampling</li> <li>Description:  Constraints sampling module of the Interactive Clustering package.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul> <p>This module provides several constraints sampling algorithm, that selecte relevant contraints to annotate by an expert :</p> <ul> <li><code>abstract</code>: an abstract class that defines constraints sampling algorithms functionnalities. See interactive_clustering/sampling/abstract documentation ;</li> <li><code>factory</code>: a factory to easily instantiate constraints sampling algorithm object. See interactive_clustering/sampling/factory documentation ;</li> <li><code>clusters_based</code>: a constraints sampling algorithm implementation that uses constraints annotation, data similarity and clustering results. See interactive_clustering/sampling/clusters_based documentation.</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/abstract/","title":"abstract","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.sampling.abstract</li> <li>Description:  The abstract class used to define constraints sampling algorithms.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/abstract/#cognitivefactory.interactive_clustering.sampling.abstract.AbstractConstraintsSampling","title":"<code>AbstractConstraintsSampling</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class that is used to define constraints sampling algorithms. The main inherited method is <code>sample</code>.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\sampling\\abstract.py</code> <pre><code>class AbstractConstraintsSampling(ABC):\n    \"\"\"\n    Abstract class that is used to define constraints sampling algorithms.\n    The main inherited method is `sample`.\n    \"\"\"\n\n    # ==============================================================================\n    # ABSTRACT METHOD - SAMPLE\n    # ==============================================================================\n    @abstractmethod\n    def sample(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        nb_to_select: int,\n        clustering_result: Optional[Dict[str, int]] = None,\n        vectors: Optional[Dict[str, csr_matrix]] = None,\n        **kargs,\n    ) -&gt; List[Tuple[str, str]]:\n        \"\"\"\n        (ABSTRACT METHOD)\n        An abstract method that represents the main method used to sample couple of data IDs for constraints annotation.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs.\n            nb_to_select (int): The number of couple of data IDs to select.\n            clustering_result (Optional[Dict[str,int]], optional): A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If `None`, no clustering result are used during the sampling. Defaults to `None`.\n            vectors (Optional[Dict[str, csr_matrix]], optional): vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. If `None`, no vectors are used during the sampling. Defaults to `None`\n            **kargs (dict): Other parameters that can be used in the sampling.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set or incompatible.\n\n        Returns:\n            List[Tuple[str,str]]: A list of couple of data IDs.\n        \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/abstract/#cognitivefactory.interactive_clustering.sampling.abstract.AbstractConstraintsSampling.sample","title":"<code>sample(constraints_manager, nb_to_select, clustering_result=None, vectors=None, **kargs)</code>  <code>abstractmethod</code>","text":"<p>(ABSTRACT METHOD) An abstract method that represents the main method used to sample couple of data IDs for constraints annotation.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs.</p> required <code>nb_to_select</code> <code>int</code> <p>The number of couple of data IDs to select.</p> required <code>clustering_result</code> <code>Optional[Dict[str, int]]</code> <p>A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If <code>None</code>, no clustering result are used during the sampling. Defaults to <code>None</code>.</p> <code>None</code> <code>vectors</code> <code>Optional[Dict[str, csr_matrix]]</code> <p>vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data. If <code>None</code>, no vectors are used during the sampling. Defaults to <code>None</code></p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the sampling.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set or incompatible.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>List[Tuple[str,str]]: A list of couple of data IDs.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\sampling\\abstract.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    nb_to_select: int,\n    clustering_result: Optional[Dict[str, int]] = None,\n    vectors: Optional[Dict[str, csr_matrix]] = None,\n    **kargs,\n) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    (ABSTRACT METHOD)\n    An abstract method that represents the main method used to sample couple of data IDs for constraints annotation.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs.\n        nb_to_select (int): The number of couple of data IDs to select.\n        clustering_result (Optional[Dict[str,int]], optional): A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If `None`, no clustering result are used during the sampling. Defaults to `None`.\n        vectors (Optional[Dict[str, csr_matrix]], optional): vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. If `None`, no vectors are used during the sampling. Defaults to `None`\n        **kargs (dict): Other parameters that can be used in the sampling.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set or incompatible.\n\n    Returns:\n        List[Tuple[str,str]]: A list of couple of data IDs.\n    \"\"\"\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/clusters_based/","title":"clusters_based","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.sampling.clusters_based</li> <li>Description:  Implementation of constraints sampling based on clusters information.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      04/10/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/clusters_based/#cognitivefactory.interactive_clustering.sampling.clusters_based.ClustersBasedConstraintsSampling","title":"<code>ClustersBasedConstraintsSampling</code>","text":"<p>             Bases: <code>AbstractConstraintsSampling</code></p> <p>This class implements the sampling of data IDs based on clusters information in order to annotate constraints. It inherits from <code>AbstractConstraintsSampling</code>.</p> Example <pre><code># Import.\nfrom cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\nfrom cognitivefactory.interactive_clustering.sampling.clusters_based import ClustersBasedConstraintsSampling\n\n# Create an instance of random sampling.\nsampler = ClustersBasedConstraintsSampling(random_seed=1)\n\n# Define list of data IDs.\nlist_of_data_IDs = [\"bonjour\", \"salut\", \"coucou\", \"au revoir\", \"a bient\u00f4t\",]\n\n# Define constraints manager.\nconstraints_manager = BinaryConstraintsManager(\n    list_of_data_IDs=list_of_data_IDs,\n)\nconstraints_manager.add_constraint(data_ID1=\"bonjour\", data_ID2=\"salut\", constraint_type=\"MUST_LINK\")\nconstraints_manager.add_constraint(data_ID1=\"au revoir\", data_ID2=\"a bient\u00f4t\", constraint_type=\"MUST_LINK\")\n\n# Run sampling.\nselection = sampler.sample(\n    constraints_manager=constraints_manager,\n    nb_to_select=3,\n)\n\n# Print results.\nprint(\"Expected results\", \";\", [(\"au revoir\", \"bonjour\"), (\"bonjour\", \"coucou\"), (\"a bient\u00f4t\", \"coucou\"),])\nprint(\"Computed results\", \":\", selection)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\sampling\\clusters_based.py</code> <pre><code>class ClustersBasedConstraintsSampling(AbstractConstraintsSampling):\n    \"\"\"\n    This class implements the sampling of data IDs based on clusters information in order to annotate constraints.\n    It inherits from `AbstractConstraintsSampling`.\n\n    Example:\n        ```python\n        # Import.\n        from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager\n        from cognitivefactory.interactive_clustering.sampling.clusters_based import ClustersBasedConstraintsSampling\n\n        # Create an instance of random sampling.\n        sampler = ClustersBasedConstraintsSampling(random_seed=1)\n\n        # Define list of data IDs.\n        list_of_data_IDs = [\"bonjour\", \"salut\", \"coucou\", \"au revoir\", \"a bient\u00f4t\",]\n\n        # Define constraints manager.\n        constraints_manager = BinaryConstraintsManager(\n            list_of_data_IDs=list_of_data_IDs,\n        )\n        constraints_manager.add_constraint(data_ID1=\"bonjour\", data_ID2=\"salut\", constraint_type=\"MUST_LINK\")\n        constraints_manager.add_constraint(data_ID1=\"au revoir\", data_ID2=\"a bient\u00f4t\", constraint_type=\"MUST_LINK\")\n\n        # Run sampling.\n        selection = sampler.sample(\n            constraints_manager=constraints_manager,\n            nb_to_select=3,\n        )\n\n        # Print results.\n        print(\"Expected results\", \";\", [(\"au revoir\", \"bonjour\"), (\"bonjour\", \"coucou\"), (\"a bient\u00f4t\", \"coucou\"),])\n        print(\"Computed results\", \":\", selection)\n        ```\n    \"\"\"\n\n    # ==============================================================================\n    # INITIALIZATION\n    # ==============================================================================\n    def __init__(\n        self,\n        random_seed: Optional[int] = None,\n        clusters_restriction: Optional[str] = None,\n        distance_restriction: Optional[str] = None,\n        without_added_constraints: bool = True,\n        without_inferred_constraints: bool = True,\n        **kargs,\n    ) -&gt; None:\n        \"\"\"\n        The constructor for Clusters Based Constraints Sampling class.\n\n        Args:\n            random_seed (Optional[int]): The random seed to use to redo the same sampling. Defaults to `None`.\n            clusters_restriction (Optional[str]): Restrict the sampling with a cluster constraints. Can impose data IDs to be in `\"same_cluster\"` or `\"different_clusters\"`. Defaults to `None`.  # TODO: `\"specific_clusters\"`\n            distance_restriction (Optional[str]): Restrict the sampling with a distance constraints. Can impose data IDs to be `\"closest_neighbors\"` or `\"farthest_neighbors\"`. Defaults to `None`.\n            without_added_constraints (bool): Option to not sample the already added constraints. Defaults to `True`.\n            without_inferred_constraints (bool): Option to not sample the deduced constraints from already added one. Defaults to `True`.\n            **kargs (dict): Other parameters that can be used in the instantiation.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set.\n        \"\"\"\n\n        # Store `self.random_seed`.\n        self.random_seed: Optional[int] = random_seed\n\n        # Store clusters restriction.\n        if clusters_restriction not in {None, \"same_cluster\", \"different_clusters\"}:\n            raise ValueError(\"The `clusters_restriction` '\" + str(clusters_restriction) + \"' is not implemented.\")\n        self.clusters_restriction: Optional[str] = clusters_restriction\n\n        # Store distance restriction.\n        if distance_restriction not in {None, \"closest_neighbors\", \"farthest_neighbors\"}:\n            raise ValueError(\"The `distance_restriction` '\" + str(distance_restriction) + \"' is not implemented.\")\n        self.distance_restriction: Optional[str] = distance_restriction\n\n        # Store constraints restrictions.\n        if not isinstance(without_added_constraints, bool):\n            raise ValueError(\"The `without_added_constraints` must be boolean\")\n        self.without_added_constraints: bool = without_added_constraints\n        if not isinstance(without_inferred_constraints, bool):\n            raise ValueError(\"The `without_inferred_constraints` must be boolean\")\n        self.without_inferred_constraints: bool = without_inferred_constraints\n\n    # ==============================================================================\n    # MAIN - SAMPLE\n    # ==============================================================================\n    def sample(\n        self,\n        constraints_manager: AbstractConstraintsManager,\n        nb_to_select: int,\n        clustering_result: Optional[Dict[str, int]] = None,\n        vectors: Optional[Dict[str, csr_matrix]] = None,\n        **kargs,\n    ) -&gt; List[Tuple[str, str]]:\n        \"\"\"\n        The main method used to sample pairs of data IDs for constraints annotation.\n\n        Args:\n            constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs.\n            nb_to_select (int): The number of pairs of data IDs to sample.\n            clustering_result (Optional[Dict[str,int]], optional): A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If `None`, no clustering result are used during the sampling. Defaults to `None`.\n            vectors (Optional[Dict[str, csr_matrix]], optional): vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. If `None`, no vectors are used during the sampling. Defaults to `None`\n            **kargs (dict): Other parameters that can be used in the sampling.\n\n        Raises:\n            ValueError: if some parameters are incorrectly set or incompatible.\n\n        Returns:\n            List[Tuple[str,str]]: A list of couple of data IDs.\n        \"\"\"\n\n        ###\n        ### GET PARAMETERS\n        ###\n\n        # Check `constraints_manager`.\n        if not isinstance(constraints_manager, AbstractConstraintsManager):\n            raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n        self.constraints_manager: AbstractConstraintsManager = constraints_manager\n\n        # Check `nb_to_select`.\n        if not isinstance(nb_to_select, int) or (nb_to_select &lt; 0):\n            raise ValueError(\"The `nb_to_select` '\" + str(nb_to_select) + \"' must be greater than or equal to 0.\")\n        elif nb_to_select == 0:\n            return []\n\n        # If `self.cluster_restriction` is set, check `clustering_result` parameters.\n        if self.clusters_restriction is not None:\n            if not isinstance(clustering_result, dict):\n                raise ValueError(\"The `clustering_result` parameter has to be a `Dict[str, int]` type.\")\n            self.clustering_result: Dict[str, int] = clustering_result\n\n        # If `self.distance_restriction` is set, check `vectors` parameters.\n        if self.distance_restriction is not None:\n            if not isinstance(vectors, dict):\n                raise ValueError(\"The `vectors` parameter has to be a `Dict[str, csr_matrix]` type.\")\n            self.vectors: Dict[str, csr_matrix] = vectors\n\n        ###\n        ### DEFINE POSSIBLE PAIRS OF DATA IDS\n        ###\n\n        # Initialize possible pairs of data IDs\n        list_of_possible_pairs_of_data_IDs: List[Tuple[str, str]] = []\n\n        # Loop over pairs of data IDs.\n        for data_ID1 in self.constraints_manager.get_list_of_managed_data_IDs():\n            for data_ID2 in self.constraints_manager.get_list_of_managed_data_IDs():\n                # Select ordered pairs.\n                if data_ID1 &gt;= data_ID2:\n                    continue\n\n                # Check clusters restriction.\n                if (\n                    self.clusters_restriction == \"same_cluster\"\n                    and self.clustering_result[data_ID1] != self.clustering_result[data_ID2]\n                ) or (\n                    self.clusters_restriction == \"different_clusters\"\n                    and self.clustering_result[data_ID1] == self.clustering_result[data_ID2]\n                ):\n                    continue\n\n                # Check known constraints.\n                if (\n                    self.without_added_constraints is True\n                    and self.constraints_manager.get_added_constraint(data_ID1=data_ID1, data_ID2=data_ID2) is not None\n                ) or (\n                    self.without_inferred_constraints is True\n                    and self.constraints_manager.get_inferred_constraint(data_ID1=data_ID1, data_ID2=data_ID2)\n                    is not None\n                ):\n                    continue\n\n                # Add the pair of data IDs.\n                list_of_possible_pairs_of_data_IDs.append((data_ID1, data_ID2))\n\n        ###\n        ### SAMPLING\n        ###\n\n        # Precompute pairwise distances.\n        if self.distance_restriction is not None:\n            # Compute pairwise distances.\n            matrix_of_pairwise_distances: csr_matrix = pairwise_distances(\n                X=vstack(self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()),\n                metric=\"euclidean\",  # TODO get different pairwise_distances config in **kargs\n            )\n\n            # Format pairwise distances in a dictionary.\n            self.dict_of_pairwise_distances: Dict[str, Dict[str, float]] = {\n                vector_ID1: {\n                    vector_ID2: float(matrix_of_pairwise_distances[i1, i2])\n                    for i2, vector_ID2 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n                }\n                for i1, vector_ID1 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n            }\n\n        # Set random seed.\n        random.seed(self.random_seed)\n\n        # Case of closest neightbors selection.\n        if self.distance_restriction == \"closest_neighbors\":\n            return sorted(\n                list_of_possible_pairs_of_data_IDs,\n                key=lambda combination: self.dict_of_pairwise_distances[combination[0]][combination[1]],\n            )[:nb_to_select]\n\n        # Case of farthest neightbors selection.\n        if self.distance_restriction == \"farthest_neighbors\":\n            return sorted(\n                list_of_possible_pairs_of_data_IDs,\n                key=lambda combination: self.dict_of_pairwise_distances[combination[0]][combination[1]],\n                reverse=True,\n            )[:nb_to_select]\n\n        # (default) Case of random selection.\n        return random.sample(\n            list_of_possible_pairs_of_data_IDs, k=min(nb_to_select, len(list_of_possible_pairs_of_data_IDs))\n        )\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/clusters_based/#cognitivefactory.interactive_clustering.sampling.clusters_based.ClustersBasedConstraintsSampling.__init__","title":"<code>__init__(random_seed=None, clusters_restriction=None, distance_restriction=None, without_added_constraints=True, without_inferred_constraints=True, **kargs)</code>","text":"<p>The constructor for Clusters Based Constraints Sampling class.</p> <p>Parameters:</p> Name Type Description Default <code>random_seed</code> <code>Optional[int]</code> <p>The random seed to use to redo the same sampling. Defaults to <code>None</code>.</p> <code>None</code> <code>clusters_restriction</code> <code>Optional[str]</code> <p>Restrict the sampling with a cluster constraints. Can impose data IDs to be in <code>\"same_cluster\"</code> or <code>\"different_clusters\"</code>. Defaults to <code>None</code>.  # TODO: <code>\"specific_clusters\"</code></p> <code>None</code> <code>distance_restriction</code> <code>Optional[str]</code> <p>Restrict the sampling with a distance constraints. Can impose data IDs to be <code>\"closest_neighbors\"</code> or <code>\"farthest_neighbors\"</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>without_added_constraints</code> <code>bool</code> <p>Option to not sample the already added constraints. Defaults to <code>True</code>.</p> <code>True</code> <code>without_inferred_constraints</code> <code>bool</code> <p>Option to not sample the deduced constraints from already added one. Defaults to <code>True</code>.</p> <code>True</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\sampling\\clusters_based.py</code> <pre><code>def __init__(\n    self,\n    random_seed: Optional[int] = None,\n    clusters_restriction: Optional[str] = None,\n    distance_restriction: Optional[str] = None,\n    without_added_constraints: bool = True,\n    without_inferred_constraints: bool = True,\n    **kargs,\n) -&gt; None:\n    \"\"\"\n    The constructor for Clusters Based Constraints Sampling class.\n\n    Args:\n        random_seed (Optional[int]): The random seed to use to redo the same sampling. Defaults to `None`.\n        clusters_restriction (Optional[str]): Restrict the sampling with a cluster constraints. Can impose data IDs to be in `\"same_cluster\"` or `\"different_clusters\"`. Defaults to `None`.  # TODO: `\"specific_clusters\"`\n        distance_restriction (Optional[str]): Restrict the sampling with a distance constraints. Can impose data IDs to be `\"closest_neighbors\"` or `\"farthest_neighbors\"`. Defaults to `None`.\n        without_added_constraints (bool): Option to not sample the already added constraints. Defaults to `True`.\n        without_inferred_constraints (bool): Option to not sample the deduced constraints from already added one. Defaults to `True`.\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set.\n    \"\"\"\n\n    # Store `self.random_seed`.\n    self.random_seed: Optional[int] = random_seed\n\n    # Store clusters restriction.\n    if clusters_restriction not in {None, \"same_cluster\", \"different_clusters\"}:\n        raise ValueError(\"The `clusters_restriction` '\" + str(clusters_restriction) + \"' is not implemented.\")\n    self.clusters_restriction: Optional[str] = clusters_restriction\n\n    # Store distance restriction.\n    if distance_restriction not in {None, \"closest_neighbors\", \"farthest_neighbors\"}:\n        raise ValueError(\"The `distance_restriction` '\" + str(distance_restriction) + \"' is not implemented.\")\n    self.distance_restriction: Optional[str] = distance_restriction\n\n    # Store constraints restrictions.\n    if not isinstance(without_added_constraints, bool):\n        raise ValueError(\"The `without_added_constraints` must be boolean\")\n    self.without_added_constraints: bool = without_added_constraints\n    if not isinstance(without_inferred_constraints, bool):\n        raise ValueError(\"The `without_inferred_constraints` must be boolean\")\n    self.without_inferred_constraints: bool = without_inferred_constraints\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/clusters_based/#cognitivefactory.interactive_clustering.sampling.clusters_based.ClustersBasedConstraintsSampling.sample","title":"<code>sample(constraints_manager, nb_to_select, clustering_result=None, vectors=None, **kargs)</code>","text":"<p>The main method used to sample pairs of data IDs for constraints annotation.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_manager</code> <code>AbstractConstraintsManager</code> <p>A constraints manager over data IDs.</p> required <code>nb_to_select</code> <code>int</code> <p>The number of pairs of data IDs to sample.</p> required <code>clustering_result</code> <code>Optional[Dict[str, int]]</code> <p>A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If <code>None</code>, no clustering result are used during the sampling. Defaults to <code>None</code>.</p> <code>None</code> <code>vectors</code> <code>Optional[Dict[str, csr_matrix]]</code> <p>vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the <code>constraints_manager</code>. The value of the dictionary represent the vector of each data. If <code>None</code>, no vectors are used during the sampling. Defaults to <code>None</code></p> <code>None</code> <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the sampling.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if some parameters are incorrectly set or incompatible.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>List[Tuple[str,str]]: A list of couple of data IDs.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\sampling\\clusters_based.py</code> <pre><code>def sample(\n    self,\n    constraints_manager: AbstractConstraintsManager,\n    nb_to_select: int,\n    clustering_result: Optional[Dict[str, int]] = None,\n    vectors: Optional[Dict[str, csr_matrix]] = None,\n    **kargs,\n) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    The main method used to sample pairs of data IDs for constraints annotation.\n\n    Args:\n        constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs.\n        nb_to_select (int): The number of pairs of data IDs to sample.\n        clustering_result (Optional[Dict[str,int]], optional): A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If `None`, no clustering result are used during the sampling. Defaults to `None`.\n        vectors (Optional[Dict[str, csr_matrix]], optional): vectors (Dict[str, csr_matrix]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. If `None`, no vectors are used during the sampling. Defaults to `None`\n        **kargs (dict): Other parameters that can be used in the sampling.\n\n    Raises:\n        ValueError: if some parameters are incorrectly set or incompatible.\n\n    Returns:\n        List[Tuple[str,str]]: A list of couple of data IDs.\n    \"\"\"\n\n    ###\n    ### GET PARAMETERS\n    ###\n\n    # Check `constraints_manager`.\n    if not isinstance(constraints_manager, AbstractConstraintsManager):\n        raise ValueError(\"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\")\n    self.constraints_manager: AbstractConstraintsManager = constraints_manager\n\n    # Check `nb_to_select`.\n    if not isinstance(nb_to_select, int) or (nb_to_select &lt; 0):\n        raise ValueError(\"The `nb_to_select` '\" + str(nb_to_select) + \"' must be greater than or equal to 0.\")\n    elif nb_to_select == 0:\n        return []\n\n    # If `self.cluster_restriction` is set, check `clustering_result` parameters.\n    if self.clusters_restriction is not None:\n        if not isinstance(clustering_result, dict):\n            raise ValueError(\"The `clustering_result` parameter has to be a `Dict[str, int]` type.\")\n        self.clustering_result: Dict[str, int] = clustering_result\n\n    # If `self.distance_restriction` is set, check `vectors` parameters.\n    if self.distance_restriction is not None:\n        if not isinstance(vectors, dict):\n            raise ValueError(\"The `vectors` parameter has to be a `Dict[str, csr_matrix]` type.\")\n        self.vectors: Dict[str, csr_matrix] = vectors\n\n    ###\n    ### DEFINE POSSIBLE PAIRS OF DATA IDS\n    ###\n\n    # Initialize possible pairs of data IDs\n    list_of_possible_pairs_of_data_IDs: List[Tuple[str, str]] = []\n\n    # Loop over pairs of data IDs.\n    for data_ID1 in self.constraints_manager.get_list_of_managed_data_IDs():\n        for data_ID2 in self.constraints_manager.get_list_of_managed_data_IDs():\n            # Select ordered pairs.\n            if data_ID1 &gt;= data_ID2:\n                continue\n\n            # Check clusters restriction.\n            if (\n                self.clusters_restriction == \"same_cluster\"\n                and self.clustering_result[data_ID1] != self.clustering_result[data_ID2]\n            ) or (\n                self.clusters_restriction == \"different_clusters\"\n                and self.clustering_result[data_ID1] == self.clustering_result[data_ID2]\n            ):\n                continue\n\n            # Check known constraints.\n            if (\n                self.without_added_constraints is True\n                and self.constraints_manager.get_added_constraint(data_ID1=data_ID1, data_ID2=data_ID2) is not None\n            ) or (\n                self.without_inferred_constraints is True\n                and self.constraints_manager.get_inferred_constraint(data_ID1=data_ID1, data_ID2=data_ID2)\n                is not None\n            ):\n                continue\n\n            # Add the pair of data IDs.\n            list_of_possible_pairs_of_data_IDs.append((data_ID1, data_ID2))\n\n    ###\n    ### SAMPLING\n    ###\n\n    # Precompute pairwise distances.\n    if self.distance_restriction is not None:\n        # Compute pairwise distances.\n        matrix_of_pairwise_distances: csr_matrix = pairwise_distances(\n            X=vstack(self.vectors[data_ID] for data_ID in self.constraints_manager.get_list_of_managed_data_IDs()),\n            metric=\"euclidean\",  # TODO get different pairwise_distances config in **kargs\n        )\n\n        # Format pairwise distances in a dictionary.\n        self.dict_of_pairwise_distances: Dict[str, Dict[str, float]] = {\n            vector_ID1: {\n                vector_ID2: float(matrix_of_pairwise_distances[i1, i2])\n                for i2, vector_ID2 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n            }\n            for i1, vector_ID1 in enumerate(self.constraints_manager.get_list_of_managed_data_IDs())\n        }\n\n    # Set random seed.\n    random.seed(self.random_seed)\n\n    # Case of closest neightbors selection.\n    if self.distance_restriction == \"closest_neighbors\":\n        return sorted(\n            list_of_possible_pairs_of_data_IDs,\n            key=lambda combination: self.dict_of_pairwise_distances[combination[0]][combination[1]],\n        )[:nb_to_select]\n\n    # Case of farthest neightbors selection.\n    if self.distance_restriction == \"farthest_neighbors\":\n        return sorted(\n            list_of_possible_pairs_of_data_IDs,\n            key=lambda combination: self.dict_of_pairwise_distances[combination[0]][combination[1]],\n            reverse=True,\n        )[:nb_to_select]\n\n    # (default) Case of random selection.\n    return random.sample(\n        list_of_possible_pairs_of_data_IDs, k=min(nb_to_select, len(list_of_possible_pairs_of_data_IDs))\n    )\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/factory/","title":"factory","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.sampling.factory</li> <li>Description:  The factory method used to easily initialize a constraints sampling algorithm.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/sampling/factory/#cognitivefactory.interactive_clustering.sampling.factory.sampling_factory","title":"<code>sampling_factory(algorithm, **kargs)</code>","text":"<p>A factory to create a new instance of a constraints sampling model.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>str</code> <p>The identification of model to instantiate. Can be <code>\"random\"</code> or <code>\"random_in_same_cluster\"</code> or <code>\"farthest_in_same_cluster\"</code> or <code>\"closest_in_different_clusters\"</code>. Defaults to <code>\"random\"</code></p> required <code>**kargs</code> <code>dict</code> <p>Other parameters that can be used in the instantiation.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>algorithm</code> is not implemented.</p> <p>Returns:</p> Name Type Description <code>AbstractConstraintsSampling</code> <code>AbstractConstraintsSampling</code> <p>An instance of constraints sampling model.</p> Example <pre><code># Import.\nfrom cognitivefactory.interactive_clustering.sampling.factory import sampling_factory\n\n# Create an instance of random sampler.\nsampler = sampling_factory(\n    algorithm=\"random\",\n)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\sampling\\factory.py</code> <pre><code>def sampling_factory(algorithm: str, **kargs) -&gt; \"AbstractConstraintsSampling\":\n    \"\"\"\n    A factory to create a new instance of a constraints sampling model.\n\n    Args:\n        algorithm (str): The identification of model to instantiate. Can be `\"random\"` or `\"random_in_same_cluster\"` or `\"farthest_in_same_cluster\"` or `\"closest_in_different_clusters\"`. Defaults to `\"random\"`\n        **kargs (dict): Other parameters that can be used in the instantiation.\n\n    Raises:\n        ValueError: if `algorithm` is not implemented.\n\n    Returns:\n        AbstractConstraintsSampling: An instance of constraints sampling model.\n\n    Example:\n        ```python\n        # Import.\n        from cognitivefactory.interactive_clustering.sampling.factory import sampling_factory\n\n        # Create an instance of random sampler.\n        sampler = sampling_factory(\n            algorithm=\"random\",\n        )\n        ```\n    \"\"\"\n\n    # Check that the requested algorithm is implemented.\n    if algorithm not in {\n        \"random\",\n        \"random_in_same_cluster\",\n        \"farthest_in_same_cluster\",\n        \"closest_in_different_clusters\",\n    }:\n        raise ValueError(\"The `algorithm` '\" + str(algorithm) + \"' is not implemented.\")\n\n    # Case of Random In Same Cluster Constraints Sampling.\n    if algorithm == \"random_in_same_cluster\":\n        return ClustersBasedConstraintsSampling(clusters_restriction=\"same_cluster\", **kargs)\n\n    # Case of Farthest In Same Cluster Constraints Sampling.\n    if algorithm == \"farthest_in_same_cluster\":\n        return ClustersBasedConstraintsSampling(\n            distance_restriction=\"farthest_neighbors\", clusters_restriction=\"same_cluster\", **kargs\n        )\n\n    # Case of Closest In Different Clusters Constraints Sampling.\n    if algorithm == \"closest_in_different_clusters\":\n        return ClustersBasedConstraintsSampling(\n            distance_restriction=\"closest_neighbors\", clusters_restriction=\"different_clusters\", **kargs\n        )\n\n    # Case of Random Constraints Sampling.\n    ##if algorithm == \"random\":\n    return ClustersBasedConstraintsSampling(**kargs)\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/utils/","title":"utils","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.utils</li> <li>Description:  utilities module of the Interactive Clustering package.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul> <p>This module provides several basic functionnalities, like data preprocessing and data vectorization :</p> <ul> <li><code>preprocessing</code>: a method to preprocess data. See interactive_clustering/utils/preprocessing documentation ;</li> <li><code>vectorization</code>: a method to vectoize data. See interactive_clustering/utils/vectorization documentation.</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/utils/frequency/","title":"frequency","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.utils.frequency</li> <li>Description:  Utilities methods for frequency analysis.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/utils/frequency/#cognitivefactory.interactive_clustering.utils.frequency.compute_clusters_frequency","title":"<code>compute_clusters_frequency(clustering_result)</code>","text":"<p>Get the frequency of each cluster present in a clustering result.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_result</code> <code>Dict[str, int]</code> <p>The dictionary that contains the predicted cluster for each data ID.</p> required <p>Returns:</p> Type Description <code>Dict[int, float]</code> <p>Dict[int,float] : Frequency fo each predicted intent.</p> Source code in <code>src\\cognitivefactory\\interactive_clustering\\utils\\frequency.py</code> <pre><code>def compute_clusters_frequency(clustering_result: Dict[str, int]) -&gt; Dict[int, float]:\n    \"\"\"\n    Get the frequency of each cluster present in a clustering result.\n\n    Args:\n        clustering_result (Dict[str,int]): The dictionary that contains the predicted cluster for each data ID.\n\n    Returns:\n        Dict[int,float] : Frequency fo each predicted intent.\n    \"\"\"\n\n    # Get the total number of data IDs.\n    nb_of_data_IDs = len(clustering_result.keys())\n\n    # Default case : No data, so no cluster.\n    if nb_of_data_IDs == 0:\n        return {}\n\n    # Get possible clusters IDs.\n    list_of_possible_cluster_IDs: List[int] = sorted(\n        {clustering_result[data_ID] for data_ID in clustering_result.keys()}\n    )\n\n    # Compute frequency of clusters in `clustering_result`.\n    frequence_of_clusters: Dict[int, float] = {\n        cluster_ID: len([data_ID for data_ID in clustering_result if clustering_result[data_ID] == cluster_ID])\n        / nb_of_data_IDs\n        for cluster_ID in list_of_possible_cluster_IDs\n    }\n\n    # Return the frequence of clusters.\n    return frequence_of_clusters\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/utils/preprocessing/","title":"preprocessing","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.utils.preprocessing</li> <li>Description:  Utilities methods to apply NLP preprocessing.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/utils/preprocessing/#cognitivefactory.interactive_clustering.utils.preprocessing.preprocess","title":"<code>preprocess(dict_of_texts, apply_stopwords_deletion=False, apply_parsing_filter=False, apply_lemmatization=False, spacy_language_model='fr_core_news_md')</code>","text":"<p>A method used to preprocess texts. It applies simple preprocessing (lowercasing, punctuations deletion, accents replacement, whitespace deletion). Some options are available to delete stopwords, apply lemmatization, and delete tokens according to their depth in the denpendency tree.</p> References <ul> <li>spaCy: <code>Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.</code></li> <li>spaCy language models: <code>https://spacy.io/usage/models</code></li> <li>NLTK: <code>Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc.</code></li> <li>NLTK 'SnowballStemmer': <code>https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>dict_of_texts</code> <code>Dict[str, str]</code> <p>A dictionary that contains the texts to preprocess.</p> required <code>apply_stopwords_deletion</code> <code>bool</code> <p>The option to delete stopwords. Defaults to <code>False</code>.</p> <code>False</code> <code>apply_parsing_filter</code> <code>bool</code> <p>The option to filter tokens based on dependency parsing results. If set, it only keeps <code>\"ROOT\"</code> tokens and their direct children. Defaults to <code>False</code>.</p> <code>False</code> <code>apply_lemmatization</code> <code>bool</code> <p>The option to lemmatize tokens. Defaults to <code>False</code>.</p> <code>False</code> <code>spacy_language_model</code> <code>str</code> <p>The spaCy language model to use if vectorizer is spacy. The model has to be installed. Defaults to <code>\"fr_core_news_md\"</code>.</p> <code>'fr_core_news_md'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises error if the <code>spacy_language_model</code> is not installed.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str,str]: A dictionary that contains the preprocessed texts.</p> Example <pre><code># Import.\nfrom cognitivefactory.interactive_clustering.utils.preprocessing import preprocess\n\n# Define data.\ndict_of_texts={\n    \"0\": \"Comment signaler une perte de carte de paiement ?\",\n    \"1\": \"Quelle est la proc\u00e9dure pour chercher une carte de cr\u00e9dit aval\u00e9e ?\",\n    \"2\": \"Ma carte Visa a un plafond de paiment trop bas, puis-je l'augmenter ?\",\n}\n\n# Apply preprocessing.\ndict_of_preprocessed_texts = preprocess(\n    dict_of_texts=dict_of_texts,\n    apply_stopwords_deletion=True,\n    apply_parsing_filter=False,\n    apply_lemmatization=False,\n    spacy_language_model=\"fr_core_news_md\",\n)\n\n# Print results.\nprint(\"Expected results\", \";\", {\n    \"0\": \"signaler perte carte paiement\",\n    \"1\": \"procedure chercher carte credit avalee\",\n    \"2\": \"carte visa plafond paiment l augmenter\",\n})\nprint(\"Computed results\", \":\", dict_of_preprocessed_texts)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\utils\\preprocessing.py</code> <pre><code>def preprocess(\n    dict_of_texts: Dict[str, str],\n    apply_stopwords_deletion: bool = False,\n    apply_parsing_filter: bool = False,\n    apply_lemmatization: bool = False,\n    spacy_language_model: str = \"fr_core_news_md\",\n) -&gt; Dict[str, str]:\n    \"\"\"\n    A method used to preprocess texts.\n    It applies simple preprocessing (lowercasing, punctuations deletion, accents replacement, whitespace deletion).\n    Some options are available to delete stopwords, apply lemmatization, and delete tokens according to their depth in the denpendency tree.\n\n    References:\n        - _spaCy_: `Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.`\n        - _spaCy_ language models: `https://spacy.io/usage/models`\n        - _NLTK_: `Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc.`\n        - _NLTK_ _'SnowballStemmer'_: `https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball`\n\n    Args:\n        dict_of_texts (Dict[str,str]): A dictionary that contains the texts to preprocess.\n        apply_stopwords_deletion (bool, optional): The option to delete stopwords. Defaults to `False`.\n        apply_parsing_filter (bool, optional): The option to filter tokens based on dependency parsing results. If set, it only keeps `\"ROOT\"` tokens and their direct children. Defaults to `False`.\n        apply_lemmatization (bool, optional): The option to lemmatize tokens. Defaults to `False`.\n        spacy_language_model (str, optional): The spaCy language model to use if vectorizer is spacy. The model has to be installed. Defaults to `\"fr_core_news_md\"`.\n\n    Raises:\n        ValueError: Raises error if the `spacy_language_model` is not installed.\n\n    Returns:\n        Dict[str,str]: A dictionary that contains the preprocessed texts.\n\n    Example:\n        ```python\n        # Import.\n        from cognitivefactory.interactive_clustering.utils.preprocessing import preprocess\n\n        # Define data.\n        dict_of_texts={\n            \"0\": \"Comment signaler une perte de carte de paiement ?\",\n            \"1\": \"Quelle est la proc\u00e9dure pour chercher une carte de cr\u00e9dit aval\u00e9e ?\",\n            \"2\": \"Ma carte Visa a un plafond de paiment trop bas, puis-je l'augmenter ?\",\n        }\n\n        # Apply preprocessing.\n        dict_of_preprocessed_texts = preprocess(\n            dict_of_texts=dict_of_texts,\n            apply_stopwords_deletion=True,\n            apply_parsing_filter=False,\n            apply_lemmatization=False,\n            spacy_language_model=\"fr_core_news_md\",\n        )\n\n        # Print results.\n        print(\"Expected results\", \";\", {\n            \"0\": \"signaler perte carte paiement\",\n            \"1\": \"procedure chercher carte credit avalee\",\n            \"2\": \"carte visa plafond paiment l augmenter\",\n        })\n        print(\"Computed results\", \":\", dict_of_preprocessed_texts)\n        ```\n    \"\"\"\n\n    # Initialize dictionary of preprocessed texts.\n    dict_of_preprocessed_texts: Dict[str, str] = {}\n\n    # Initialize punctuation translator.\n    punctuation_translator = str.maketrans(\n        {\n            punct: \" \"\n            for punct in (\n                \".\",\n                \",\",\n                \";\",\n                \":\",\n                \"!\",\n                \"\u00a1\",\n                \"?\",\n                \"\u00bf\",\n                \"\u2026\",\n                \"\u2022\",\n                \"(\",\n                \")\",\n                \"{\",\n                \"}\",\n                \"[\",\n                \"]\",\n                \"\u00ab\",\n                \"\u00bb\",\n                \"^\",\n                \"`\",\n                \"'\",\n                '\"',\n                \"\\\\\",\n                \"/\",\n                \"|\",\n                \"-\",\n                \"_\",\n                \"#\",\n                \"&amp;\",\n                \"~\",\n                \"@\",\n            )\n        }\n    )\n\n    # Load vectorizer (spacy language model).\n    try:\n        spacy_nlp = spacy.load(\n            name=spacy_language_model,\n            disable=[\n                # \"morphologizer\", # Needed for lemmatization.\n                # \"parser\", # Needed for filtering on dependency parsing.\n                # \"attribute_ruler\",  # Need for pos tagging.\n                # \"lemmatizer\", # Needed for lemmatization.\n                \"ner\",  # Not needed\n            ],\n        )\n    except OSError as err:  # `spacy_language_model` is not installed.\n        raise ValueError(\"The `spacy_language_model` '\" + str(spacy_language_model) + \"' is not installed.\") from err\n\n    # Initialize stemmer.\n    ####stemmer = SnowballStemmer(language=\"french\")\n\n    # For each text...\n    for key, text in dict_of_texts.items():\n        # Force string type.\n        preprocessed_text: str = str(text)\n\n        # Apply lowercasing.\n        preprocessed_text = text.lower()\n\n        # Apply punctuation deletion (before tokenization).\n        preprocessed_text = preprocessed_text.translate(punctuation_translator)\n\n        # Apply tokenization and spaCy pipeline.\n        tokens = [\n            token\n            for token in spacy_nlp(preprocessed_text)\n            if (\n                # Spaces are not allowed.\n                not token.is_space\n            )\n            and (\n                # Punctuation are not allowed.\n                not token.is_punct\n                and not token.is_quote\n            )\n            and (\n                # If set, stopwords are not allowed.\n                (not apply_stopwords_deletion)\n                or (not token.is_stop)\n            )\n            and (\n                # If set, stopwords are not allowed.\n                (not apply_parsing_filter)\n                or (len(list(token.ancestors)) &lt;= 1)\n            )\n        ]\n\n        # Apply retokenization with lemmatization.\n        if apply_lemmatization:\n            preprocessed_text = \" \".join([token.lemma_.strip() for token in tokens])\n\n        # Apply retokenization without lemmatization.\n        else:\n            preprocessed_text = \" \".join([token.text.strip() for token in tokens])\n\n        # Apply accents deletion (after lemmatization).\n        preprocessed_text = \"\".join(\n            [char for char in unicodedata.normalize(\"NFKD\", preprocessed_text) if not unicodedata.combining(char)]\n        )\n\n        # Store preprocessed text.\n        dict_of_preprocessed_texts[key] = preprocessed_text\n\n    return dict_of_preprocessed_texts\n</code></pre>"},{"location":"reference/cognitivefactory/interactive_clustering/utils/vectorization/","title":"vectorization","text":"<ul> <li>Name:         cognitivefactory.interactive_clustering.utils.vectorization</li> <li>Description:  Utilities methods to apply NLP vectorization.</li> <li>Author:       Erwan SCHILD</li> <li>Created:      17/03/2021</li> <li>Licence:      CeCILL (https://cecill.info/licences.fr.html)</li> </ul>"},{"location":"reference/cognitivefactory/interactive_clustering/utils/vectorization/#cognitivefactory.interactive_clustering.utils.vectorization.vectorize","title":"<code>vectorize(dict_of_texts, vectorizer_type='tfidf', spacy_language_model='fr_core_news_md')</code>","text":"<p>A method used to vectorize texts. Severals vectorizer are available : TFIDF, spaCy language model.</p> References <ul> <li>Scikit-learn: <code>Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830.</code></li> <li>Scikit-learn 'TfidfVectorizer': <code>https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</code></li> <li>spaCy: <code>Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.</code></li> <li>spaCy language models: <code>https://spacy.io/usage/models</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>dict_of_texts</code> <code>Dict[str, str]</code> <p>A dictionary that contains the texts to vectorize.</p> required <code>vectorizer_type</code> <code>str</code> <p>The vectorizer type to use. The type can be <code>\"tfidf\"</code> or <code>\"spacy\"</code>. Defaults to <code>\"tfidf\"</code>.</p> <code>'tfidf'</code> <code>spacy_language_model</code> <code>str</code> <p>The spaCy language model to use if vectorizer is spacy. Defaults to <code>\"fr_core_news_md\"</code>.</p> <code>'fr_core_news_md'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises error if <code>vectorizer_type</code> is not implemented or if the <code>spacy_language_model</code> is not installed.</p> <p>Returns:</p> Type Description <code>Dict[str, csr_matrix]</code> <p>Dict[str, csr_matrix]: A dictionary that contains the computed vectors.</p> Example <pre><code># Import.\nfrom cognitivefactory.interactive_clustering.utils.vectorization import vectorize\n\n# Define data.\ndict_of_texts={\n    \"0\": \"comment signaler une perte de carte de paiement\",\n    \"1\": \"quelle est la procedure pour chercher une carte de credit avalee\",\n    \"2\": \"ma carte visa a un plafond de paiment trop bas puis je l augmenter\",\n}\n\n# Apply vectorization.\ndict_of_vectors = vectorize(\n    dict_of_texts=dict_of_texts,\n    vectorizer_type=\"spacy\",\n    spacy_language_model=\"fr_core_news_md\",\n)\n\n# Print results.\nprint(\"Computed results\", \":\", dict_of_vectors)\n</code></pre> Source code in <code>src\\cognitivefactory\\interactive_clustering\\utils\\vectorization.py</code> <pre><code>def vectorize(\n    dict_of_texts: Dict[str, str],\n    vectorizer_type: str = \"tfidf\",\n    spacy_language_model: str = \"fr_core_news_md\",\n) -&gt; Dict[str, csr_matrix]:\n    \"\"\"\n    A method used to vectorize texts.\n    Severals vectorizer are available : TFIDF, spaCy language model.\n\n    References:\n        - _Scikit-learn_: `Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830.`\n        - _Scikit-learn_ _'TfidfVectorizer'_: `https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html`\n        - _spaCy_: `Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.`\n        - _spaCy_ language models: `https://spacy.io/usage/models`\n\n    Args:\n        dict_of_texts (Dict[str,str]): A dictionary that contains the texts to vectorize.\n        vectorizer_type (str, optional): The vectorizer type to use. The type can be `\"tfidf\"` or `\"spacy\"`. Defaults to `\"tfidf\"`.\n        spacy_language_model (str, optional): The spaCy language model to use if vectorizer is spacy. Defaults to `\"fr_core_news_md\"`.\n\n    Raises:\n        ValueError: Raises error if `vectorizer_type` is not implemented or if the `spacy_language_model` is not installed.\n\n    Returns:\n        Dict[str, csr_matrix]: A dictionary that contains the computed vectors.\n\n    Example:\n        ```python\n        # Import.\n        from cognitivefactory.interactive_clustering.utils.vectorization import vectorize\n\n        # Define data.\n        dict_of_texts={\n            \"0\": \"comment signaler une perte de carte de paiement\",\n            \"1\": \"quelle est la procedure pour chercher une carte de credit avalee\",\n            \"2\": \"ma carte visa a un plafond de paiment trop bas puis je l augmenter\",\n        }\n\n        # Apply vectorization.\n        dict_of_vectors = vectorize(\n            dict_of_texts=dict_of_texts,\n            vectorizer_type=\"spacy\",\n            spacy_language_model=\"fr_core_news_md\",\n        )\n\n        # Print results.\n        print(\"Computed results\", \":\", dict_of_vectors)\n        ```\n    \"\"\"\n\n    # Initialize dictionary of vectors.\n    dict_of_vectors: Dict[str, csr_matrix] = {}\n\n    ###\n    ### Case of TFIDF vectorization.\n    ###\n    if vectorizer_type == \"tfidf\":\n        # Initialize vectorizer.\n        vectorizer = TfidfVectorizer(\n            analyzer=\"word\",\n            ngram_range=(1, 3),\n            min_df=2,\n            ####min_df=0.0, max_df=0.95, max_features=20000,\n            ####ngram_range=(1,5), analyzer=\"char_wb\", sublinear_tf=True,\n        )\n\n        # Apply vectorization.\n        tfidf_vectorization: csr_matrix = vectorizer.fit_transform(\n            [str(dict_of_texts[data_ID]) for data_ID in dict_of_texts.keys()]\n        )\n\n        # Format dictionary of vectors to return.\n        dict_of_vectors = {data_ID: tfidf_vectorization[i] for i, data_ID in enumerate(dict_of_texts.keys())}\n\n        # Return the dictionary of vectors.\n        return dict_of_vectors\n\n    ###\n    ### Case of SPACY vectorization.\n    ###\n    if vectorizer_type == \"spacy\":\n        # Load vectorizer (spaCy language model).\n        try:\n            spacy_nlp = spacy.load(\n                name=spacy_language_model,\n                disable=[\n                    \"morphologizer\",  # Not needed\n                    \"parser\",  # Not needed\n                    \"attribute_ruler\",  # Not needed\n                    \"lemmatizer\",  # Not needed\n                    \"ner\",  # Not needed\n                ],\n            )\n        except OSError as err:  # `spacy_language_model` is not installed.\n            raise ValueError(\n                \"The `spacy_language_model` '\" + str(spacy_language_model) + \"' is not installed.\"\n            ) from err\n\n        # Apply vectorization.\n        dict_of_vectors = {data_ID: csr_matrix(spacy_nlp(str(text)).vector) for data_ID, text in dict_of_texts.items()}\n\n        # Return the dictionary of vectors.\n        return dict_of_vectors\n\n    ###\n    ### Other case : Raise a `ValueError`.\n    ###\n    raise ValueError(\"The `vectorizer_type` '\" + str(vectorizer_type) + \"' is not implemented.\")\n</code></pre>"},{"location":"coverage/","title":"Coverage report","text":""}]}