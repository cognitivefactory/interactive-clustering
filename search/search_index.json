{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Interactive Clustering \u00b6 Python package used to apply NLP interactive clustering methods. Quick description \u00b6 Interactive clustering is a method intended to assist in the design of a training data set. This iterative process begins with an unlabeled dataset, and it uses a sequence of two substeps : the user defines constraints on the data ; the machine performs data partitioning using a constrained clustering algorithm. Thus, at each step of the process : the user corrects the clustering of the previous steps using constraints, and the machine offers a corrected and more relevant data partitioning for the next step. The process use severals objects : a constraints manager : its role is to manage the constraints annotated by the user and to feed back the information deduced (such as the transitivity between constraints or the situation of inconsistency) ; a constraints sampler : its role is to select the most relevant data during the annotation of constraints by the user ; a constrained clustering algorithm : its role is to partition the data while respecting the constraints provided by the user. NB : This python library does not contain integration into a graphic interface. For more details, read the Documentation and the articles in the References section. Documentation \u00b6 Main documentation Requirements \u00b6 Interactive Clustering requires Python 3.6 or above. To install Python 3.6, I recommend using pyenv . # install pyenv git clone https://github.com/pyenv/pyenv ~/.pyenv # setup pyenv (you should also put these three lines in .bashrc or similar) export PATH = \" ${ HOME } /.pyenv/bin: ${ PATH } \" export PYENV_ROOT = \" ${ HOME } /.pyenv\" eval \" $( pyenv init - ) \" # install Python 3.6 pyenv install 3 .6.12 # make it available globally pyenv global system 3 .6.12 Installation \u00b6 With pip : # install package python3 -m pip install cognitivefactory-interactive-clustering # install spacy language model dependencies (the one you want, with version \"^2.3\") python3 -m spacy download fr_core_news_sm-2.3.0 --direct With pipx : # install pipx python3 -m pip install --user pipx # install package pipx install --python python3 cognitivefactory-interactive-clustering # install spacy language model dependencies (the one you want, with version \"^2.3\") python3 -m spacy download fr_core_news_sm-2.3.0 --direct NB : Other spaCy language models can be downloaded here : spaCy - Models & Languages . Use spacy version \"^2.3\" . Development \u00b6 To work on this project or contribute to it, please read the Copier PDM documentation . Quick setup and help \u00b6 Get the code and prepare the environment: git clone https://github.com/cognitivefactory/interactive-clustering/ cd interactive-clustering make setup Show the help: make help # or just make For more details, read the Contributing documentation. References \u00b6 Interactive Clustering : Theory and Implementation: Schild, E., Durantin, G., Lamirel, J.C., & Miconi, F. (2021). Conception it\u00e9rative et semi-supervis\u00e9e d'assistants conversationnels par regroupement interactif des questions. In EGC 2021 - 21\u00e8mes Journ\u00e9es Francophones Extraction et Gestion des Connaissances. Edition RNTI. \u27e8hal-03133007\u27e9 Methodological instructions: Schild, E., Durantin, G., & Lamirel, J.C. (2021). Concevoir un assistant conversationnel de mani\u00e8re it\u00e9rative et semi-supervis\u00e9e avec le clustering interactif. In Atelier - Fouille de Textes - Text Mine 2021 - En conjonction avec EGC 2021. \u27e8hal-03133060\u27e9 Constraints and Constrained Clustering : Constraints in clustering: Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110. Survey on Constrained Clustering: Lampert, T., T.-B.-H. Dao, B. Lafabregue, N. Serrette, G. Forestier, B. Cremilleux, C. Vrain, et P. Gancarski (2018). Constrained distance based clustering for time-series : a comparative and experimental study. Data Mining and Knowledge Discovery 32(6), 1663\u20131707. KMeans Clustering: KMeans Clustering: MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297. Constrained 'COP' KMeans Clustering: Wagstaff, K., C. Cardie, S. Rogers, et S. Schroedl (2001). Constrained K-means Clustering with Background Knowledge. International Conference on Machine Learning Hierarchical Clustering: Hierarchical Clustering: Murtagh, F. et P. Contreras (2012). Algorithms for hierarchical clustering : An overview. Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery 2, 86\u201397. Constrained Hierarchical Clustering: Davidson, I. et S. S. Ravi (2005). Agglomerative Hierarchical Clustering with Constraints : Theoretical and Empirical Results. Springer, Berlin, Heidelberg 3721, 12. Spectral Clustering: Spectral Clustering: Ng, A. Y., M. I. Jordan, et Y.Weiss (2002). On Spectral Clustering: Analysis and an algorithm. In T. G. Dietterich, S. Becker, et Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press. Constrained 'SPEC' Spectral Clustering: Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566. Preprocessing and Vectorization : spaCy : Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. spaCy language models: https://spacy.io/usage/models NLTK : Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc. NLTK 'SnowballStemmer' : https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball Scikit-learn : Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830. Scikit-learn 'TfidfVectorizer' : https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html","title":"Overview"},{"location":"#interactive-clustering","text":"Python package used to apply NLP interactive clustering methods.","title":"Interactive Clustering"},{"location":"#quick-description","text":"Interactive clustering is a method intended to assist in the design of a training data set. This iterative process begins with an unlabeled dataset, and it uses a sequence of two substeps : the user defines constraints on the data ; the machine performs data partitioning using a constrained clustering algorithm. Thus, at each step of the process : the user corrects the clustering of the previous steps using constraints, and the machine offers a corrected and more relevant data partitioning for the next step. The process use severals objects : a constraints manager : its role is to manage the constraints annotated by the user and to feed back the information deduced (such as the transitivity between constraints or the situation of inconsistency) ; a constraints sampler : its role is to select the most relevant data during the annotation of constraints by the user ; a constrained clustering algorithm : its role is to partition the data while respecting the constraints provided by the user. NB : This python library does not contain integration into a graphic interface. For more details, read the Documentation and the articles in the References section.","title":" Quick description"},{"location":"#documentation","text":"Main documentation","title":" Documentation"},{"location":"#requirements","text":"Interactive Clustering requires Python 3.6 or above. To install Python 3.6, I recommend using pyenv . # install pyenv git clone https://github.com/pyenv/pyenv ~/.pyenv # setup pyenv (you should also put these three lines in .bashrc or similar) export PATH = \" ${ HOME } /.pyenv/bin: ${ PATH } \" export PYENV_ROOT = \" ${ HOME } /.pyenv\" eval \" $( pyenv init - ) \" # install Python 3.6 pyenv install 3 .6.12 # make it available globally pyenv global system 3 .6.12","title":" Requirements"},{"location":"#installation","text":"With pip : # install package python3 -m pip install cognitivefactory-interactive-clustering # install spacy language model dependencies (the one you want, with version \"^2.3\") python3 -m spacy download fr_core_news_sm-2.3.0 --direct With pipx : # install pipx python3 -m pip install --user pipx # install package pipx install --python python3 cognitivefactory-interactive-clustering # install spacy language model dependencies (the one you want, with version \"^2.3\") python3 -m spacy download fr_core_news_sm-2.3.0 --direct NB : Other spaCy language models can be downloaded here : spaCy - Models & Languages . Use spacy version \"^2.3\" .","title":" Installation"},{"location":"#development","text":"To work on this project or contribute to it, please read the Copier PDM documentation .","title":" Development"},{"location":"#quick-setup-and-help","text":"Get the code and prepare the environment: git clone https://github.com/cognitivefactory/interactive-clustering/ cd interactive-clustering make setup Show the help: make help # or just make For more details, read the Contributing documentation.","title":"Quick setup and help"},{"location":"#references","text":"Interactive Clustering : Theory and Implementation: Schild, E., Durantin, G., Lamirel, J.C., & Miconi, F. (2021). Conception it\u00e9rative et semi-supervis\u00e9e d'assistants conversationnels par regroupement interactif des questions. In EGC 2021 - 21\u00e8mes Journ\u00e9es Francophones Extraction et Gestion des Connaissances. Edition RNTI. \u27e8hal-03133007\u27e9 Methodological instructions: Schild, E., Durantin, G., & Lamirel, J.C. (2021). Concevoir un assistant conversationnel de mani\u00e8re it\u00e9rative et semi-supervis\u00e9e avec le clustering interactif. In Atelier - Fouille de Textes - Text Mine 2021 - En conjonction avec EGC 2021. \u27e8hal-03133060\u27e9 Constraints and Constrained Clustering : Constraints in clustering: Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110. Survey on Constrained Clustering: Lampert, T., T.-B.-H. Dao, B. Lafabregue, N. Serrette, G. Forestier, B. Cremilleux, C. Vrain, et P. Gancarski (2018). Constrained distance based clustering for time-series : a comparative and experimental study. Data Mining and Knowledge Discovery 32(6), 1663\u20131707. KMeans Clustering: KMeans Clustering: MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297. Constrained 'COP' KMeans Clustering: Wagstaff, K., C. Cardie, S. Rogers, et S. Schroedl (2001). Constrained K-means Clustering with Background Knowledge. International Conference on Machine Learning Hierarchical Clustering: Hierarchical Clustering: Murtagh, F. et P. Contreras (2012). Algorithms for hierarchical clustering : An overview. Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery 2, 86\u201397. Constrained Hierarchical Clustering: Davidson, I. et S. S. Ravi (2005). Agglomerative Hierarchical Clustering with Constraints : Theoretical and Empirical Results. Springer, Berlin, Heidelberg 3721, 12. Spectral Clustering: Spectral Clustering: Ng, A. Y., M. I. Jordan, et Y.Weiss (2002). On Spectral Clustering: Analysis and an algorithm. In T. G. Dietterich, S. Becker, et Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press. Constrained 'SPEC' Spectral Clustering: Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566. Preprocessing and Vectorization : spaCy : Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. spaCy language models: https://spacy.io/usage/models NLTK : Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc. NLTK 'SnowballStemmer' : https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball Scikit-learn : Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830. Scikit-learn 'TfidfVectorizer' : https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html","title":" References"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning . 0.4.0 - 2021-10-12 \u00b6 Compare with 0.3.0 Bug Fixes \u00b6 correct networkx dependency requirement ( 4ec4587 by Erwan SCHILD). correct networkx import in sampling ( 521a4ff by Erwan Schild). speed up computation of sampling.clusters_based.sampling for distance restrictions ( 5ab6821 by Erwan Schild). speed up computation of constraints.binary.get_min_and_max_number_of_clusters ( 1e50f7c by Erwan Schild). Code Refactoring \u00b6 update template with copier update ( e0a7c77 by Erwan Schild). fix black dependenciy installation ( eef88c5 by Erwan Schild). delete old random sampler ( 6cd0a06 by Erwan Schild). Features \u00b6 implementation of getter of data IDs involved in a constraint conflict ( 6eace0d by Erwan Schild). 0.3.0 - 2021-10-04 \u00b6 Compare with 0.2.1 Features \u00b6 update Constraints Sampling with clusters/distance/known_constraints restrictions ( 34c5747 by Erwan Schild). 0.2.1 - 2021-09-20 \u00b6 Compare with 0.2.0 Bug Fixes \u00b6 correct constraints transitivity inconsistencies in BinaryConstraintsManager.add_constraint ( 98f162e by Erwan Schild). Code Refactoring \u00b6 fix code quality errors ( 02c03ee by Erwan Schild). update exception message ( 2003a1e by Erwan Schild). 0.2.0 - 2021-09-01 \u00b6 Compare with 0.1.3 Bug Fixes \u00b6 change constraints storage from sorted lists to sets ( 47d3528 by Erwan Schild). Code Refactoring \u00b6 delete utils.checking ( a9a1f50 by Erwan Schild). remove checks and force usage of constraints_manager ( 4cdb0bb by Erwan Schild). improve sampling speed ( 9d6ed5c by Erwan Schild). add py.typed file ( 25c7be3 by Erwan Schild). 0.1.3 - 2021-05-20 \u00b6 Compare with 0.1.2 0.1.2 - 2021-05-19 \u00b6 Compare with 0.1.1 Code Refactoring \u00b6 correct format and tests ( e3245f3 by Erwan SCHILD). 0.1.1 - 2021-05-18 \u00b6 Compare with 0.1.0 0.1.0 - 2021-05-17 \u00b6 Compare with first commit Bug Fixes \u00b6 fix encoding error on fr_core_news_sm-2.3.0/meta.json ? ( 98acb42 by Erwan SCHILD). correct installation sources ( de4c727 by Erwan SCHILD). Code Refactoring \u00b6 order import and update documentation ( 70e8780 by Erwan SCHILD). remove local fr_core_news_sm model ( 1f9da8f by Erwan SCHILD). test fr_core_news_sm installation like a pip package ( b249159 by Erwan SCHILD). Features \u00b6 implement Interactive Clustering ( d678d87 by Erwan SCHILD).","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#040-2021-10-12","text":"Compare with 0.3.0","title":"0.4.0 - 2021-10-12"},{"location":"changelog/#bug-fixes","text":"correct networkx dependency requirement ( 4ec4587 by Erwan SCHILD). correct networkx import in sampling ( 521a4ff by Erwan Schild). speed up computation of sampling.clusters_based.sampling for distance restrictions ( 5ab6821 by Erwan Schild). speed up computation of constraints.binary.get_min_and_max_number_of_clusters ( 1e50f7c by Erwan Schild).","title":"Bug Fixes"},{"location":"changelog/#code-refactoring","text":"update template with copier update ( e0a7c77 by Erwan Schild). fix black dependenciy installation ( eef88c5 by Erwan Schild). delete old random sampler ( 6cd0a06 by Erwan Schild).","title":"Code Refactoring"},{"location":"changelog/#features","text":"implementation of getter of data IDs involved in a constraint conflict ( 6eace0d by Erwan Schild).","title":"Features"},{"location":"changelog/#030-2021-10-04","text":"Compare with 0.2.1","title":"0.3.0 - 2021-10-04"},{"location":"changelog/#features_1","text":"update Constraints Sampling with clusters/distance/known_constraints restrictions ( 34c5747 by Erwan Schild).","title":"Features"},{"location":"changelog/#021-2021-09-20","text":"Compare with 0.2.0","title":"0.2.1 - 2021-09-20"},{"location":"changelog/#bug-fixes_1","text":"correct constraints transitivity inconsistencies in BinaryConstraintsManager.add_constraint ( 98f162e by Erwan Schild).","title":"Bug Fixes"},{"location":"changelog/#code-refactoring_1","text":"fix code quality errors ( 02c03ee by Erwan Schild). update exception message ( 2003a1e by Erwan Schild).","title":"Code Refactoring"},{"location":"changelog/#020-2021-09-01","text":"Compare with 0.1.3","title":"0.2.0 - 2021-09-01"},{"location":"changelog/#bug-fixes_2","text":"change constraints storage from sorted lists to sets ( 47d3528 by Erwan Schild).","title":"Bug Fixes"},{"location":"changelog/#code-refactoring_2","text":"delete utils.checking ( a9a1f50 by Erwan Schild). remove checks and force usage of constraints_manager ( 4cdb0bb by Erwan Schild). improve sampling speed ( 9d6ed5c by Erwan Schild). add py.typed file ( 25c7be3 by Erwan Schild).","title":"Code Refactoring"},{"location":"changelog/#013-2021-05-20","text":"Compare with 0.1.2","title":"0.1.3 - 2021-05-20"},{"location":"changelog/#012-2021-05-19","text":"Compare with 0.1.1","title":"0.1.2 - 2021-05-19"},{"location":"changelog/#code-refactoring_3","text":"correct format and tests ( e3245f3 by Erwan SCHILD).","title":"Code Refactoring"},{"location":"changelog/#011-2021-05-18","text":"Compare with 0.1.0","title":"0.1.1 - 2021-05-18"},{"location":"changelog/#010-2021-05-17","text":"Compare with first commit","title":"0.1.0 - 2021-05-17"},{"location":"changelog/#bug-fixes_3","text":"fix encoding error on fr_core_news_sm-2.3.0/meta.json ? ( 98acb42 by Erwan SCHILD). correct installation sources ( de4c727 by Erwan SCHILD).","title":"Bug Fixes"},{"location":"changelog/#code-refactoring_4","text":"order import and update documentation ( 70e8780 by Erwan SCHILD). remove local fr_core_news_sm model ( 1f9da8f by Erwan SCHILD). test fr_core_news_sm installation like a pip package ( b249159 by Erwan SCHILD).","title":"Code Refactoring"},{"location":"changelog/#features_2","text":"implement Interactive Clustering ( d678d87 by Erwan SCHILD).","title":"Features"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at erwan.schild@e-i.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at erwan.schild@e-i.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Attribution"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Environment setup \u00b6 Nothing easier! Fork and clone the repository, then: cd interactive-clustering make setup Note If it fails for some reason, you'll need to install PDM manually. You can install it with: python3 -m pip install --user pipx pipx install pdm Now you can try running make setup again, or simply pdm install . You now have the dependencies installed. Run make help to see all the available actions! Tasks \u00b6 This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with pdm run duty TASK The Makefile detects if a virtual environment is activated, so make will work the same with the virtualenv activated or not. Development \u00b6 As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation If you updated the documentation or the project dependencies: run make docs-regen run make docs-serve , go to http://localhost:8000 and check that everything looks good Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this. Commit message convention \u00b6 Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15. Pull requests guidelines \u00b6 Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Contributing"},{"location":"contributing/#environment-setup","text":"Nothing easier! Fork and clone the repository, then: cd interactive-clustering make setup Note If it fails for some reason, you'll need to install PDM manually. You can install it with: python3 -m pip install --user pipx pipx install pdm Now you can try running make setup again, or simply pdm install . You now have the dependencies installed. Run make help to see all the available actions!","title":"Environment setup"},{"location":"contributing/#tasks","text":"This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with pdm run duty TASK The Makefile detects if a virtual environment is activated, so make will work the same with the virtualenv activated or not.","title":"Tasks"},{"location":"contributing/#development","text":"As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation If you updated the documentation or the project dependencies: run make docs-regen run make docs-serve , go to http://localhost:8000 and check that everything looks good Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this.","title":"Development"},{"location":"contributing/#commit-message-convention","text":"Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15.","title":"Commit message convention"},{"location":"contributing/#pull-requests-guidelines","text":"Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Pull requests guidelines"},{"location":"credits/","text":"Credits \u00b6 These projects were used to build cognitivefactory-interactive-clustering . Thank you! python | pdm | copier-pdm Direct dependencies \u00b6 autoflake | black | darglint | duty | flake8-bandit | flake8-black | flake8-bugbear | flake8-builtins | flake8-comprehensions | flake8-docstrings | flake8-pytest-style | flake8-string-format | flake8-tidy-imports | flake8-variables-names | git-changelog | https | isort | mkdocs | mkdocs-coverage | mkdocs-gen-files | mkdocs-literate-nav | mkdocs-material | mkdocstrings | mypy | networkx | numpy | pep8-naming | pytest | pytest-cov | pytest-randomly | pytest-sugar | pytest-xdist | regex | scikit-learn | scipy | spacy | toml | wps-light Indirect dependencies \u00b6 ansimarkup | appdirs | astor | astunparse | atomicwrites | attrs | bandit | blis | cached-property | catalogue | certifi | charset-normalizer | click | colorama | coverage | cymem | dataclasses | decorator | en-core-web-sm | execnet | failprint | flake8 | flake8-plugin-utils | flake8-polyfill | fr-core-news-sm | ghp-import | gitdb | gitpython | idna | importlib-metadata | iniconfig | jinja2 | joblib | markdown | markupsafe | mccabe | mergedeep | mkdocs-autorefs | mkdocs-material-extensions | murmurhash | mypy-extensions | packaging | pathspec | pbr | plac | pluggy | preshed | ptyprocess | py | pycodestyle | pydocstyle | pyflakes | pygments | pymdown-extensions | pyparsing | pytest-forked | python-dateutil | pytkdocs | pyyaml | pyyaml-env-tag | requests | setuptools | six | smmap | snowballstemmer | srsly | stevedore | termcolor | thinc | threadpoolctl | tqdm | typed-ast | typing-extensions | urllib3 | wasabi | watchdog | wheel | zipp","title":"Credits"},{"location":"credits/#credits","text":"These projects were used to build cognitivefactory-interactive-clustering . Thank you! python | pdm | copier-pdm","title":"Credits"},{"location":"credits/#direct-dependencies","text":"autoflake | black | darglint | duty | flake8-bandit | flake8-black | flake8-bugbear | flake8-builtins | flake8-comprehensions | flake8-docstrings | flake8-pytest-style | flake8-string-format | flake8-tidy-imports | flake8-variables-names | git-changelog | https | isort | mkdocs | mkdocs-coverage | mkdocs-gen-files | mkdocs-literate-nav | mkdocs-material | mkdocstrings | mypy | networkx | numpy | pep8-naming | pytest | pytest-cov | pytest-randomly | pytest-sugar | pytest-xdist | regex | scikit-learn | scipy | spacy | toml | wps-light","title":"Direct dependencies"},{"location":"credits/#indirect-dependencies","text":"ansimarkup | appdirs | astor | astunparse | atomicwrites | attrs | bandit | blis | cached-property | catalogue | certifi | charset-normalizer | click | colorama | coverage | cymem | dataclasses | decorator | en-core-web-sm | execnet | failprint | flake8 | flake8-plugin-utils | flake8-polyfill | fr-core-news-sm | ghp-import | gitdb | gitpython | idna | importlib-metadata | iniconfig | jinja2 | joblib | markdown | markupsafe | mccabe | mergedeep | mkdocs-autorefs | mkdocs-material-extensions | murmurhash | mypy-extensions | packaging | pathspec | pbr | plac | pluggy | preshed | ptyprocess | py | pycodestyle | pydocstyle | pyflakes | pygments | pymdown-extensions | pyparsing | pytest-forked | python-dateutil | pytkdocs | pyyaml | pyyaml-env-tag | requests | setuptools | six | smmap | snowballstemmer | srsly | stevedore | termcolor | thinc | threadpoolctl | tqdm | typed-ast | typing-extensions | urllib3 | wasabi | watchdog | wheel | zipp","title":"Indirect dependencies"},{"location":"license/","text":"CeCILL-C FREE SOFTWARE LICENSE AGREEMENT \u00b6 Notice \u00b6 This Agreement is a Free Software license agreement that is the result of discussions between its authors in order to ensure compliance with the two main principles guiding its drafting: firstly, compliance with the principles governing the distribution of Free Software: access to source code, broad rights granted to users, secondly, the election of a governing law, French law, with which it is conformant, both as regards the law of torts and intellectual property law, and the protection that it offers to both authors and holders of the economic rights over software. The authors of the CeCILL-C license are: Commissariat \u00e0 l'Energie Atomique - CEA, a public scientific, technical and industrial research establishment, having its principal place of business at 25 rue Leblanc, immeuble Le Ponant D, 75015 Paris, France. Centre National de la Recherche Scientifique - CNRS, a public scientific and technological establishment, having its principal place of business at 3 rue Michel-Ange, 75794 Paris cedex 16, France. Institut National de Recherche en Informatique et en Automatique - INRIA, a public scientific and technological establishment, having its principal place of business at Domaine de Voluceau, Rocquencourt, BP 105, 78153 Le Chesnay cedex, France. CeCILL stands for Ce(a) C(nrs) I(nria) L(ogiciel) L(ibre) Preamble \u00b6 The purpose of this Free Software license agreement is to grant users the right to modify and re-use the software governed by this license. The exercising of this right is conditional upon the obligation to make available to the community the modifications made to the source code of the software so as to contribute to its evolution. In consideration of access to the source code and the rights to copy, modify and redistribute granted by the license, users are provided only with a limited warranty and the software's author, the holder of the economic rights, and the successive licensors only have limited liability. In this respect, the risks associated with loading, using, modifying and/or developing or reproducing the software by the user are brought to the user's attention, given its Free Software status, which may make it complicated to use, with the result that its use is reserved for developers and experienced professionals having in-depth computer knowledge. Users are therefore encouraged to load and test the suitability of the software as regards their requirements in conditions enabling the security of their systems and/or data to be ensured and, more generally, to use and operate it in the same conditions of security. This Agreement may be freely reproduced and published, provided it is not altered, and that no provisions are either added or removed herefrom. This Agreement may apply to any or all software for which the holder of the economic rights decides to submit the use thereof to its provisions. Article 1 - DEFINITIONS \u00b6 For the purpose of this Agreement, when the following expressions commence with a capital letter, they shall have the following meaning: Agreement: means this license agreement, and its possible subsequent versions and annexes. Software: means the software in its Object Code and/or Source Code form and, where applicable, its documentation, \"as is\" when the Licensee accepts the Agreement. Initial Software: means the Software in its Source Code and possibly its Object Code form and, where applicable, its documentation, \"as is\" when it is first distributed under the terms and conditions of the Agreement. Modified Software: means the Software modified by at least one Integrated Contribution. Source Code: means all the Software's instructions and program lines to which access is required so as to modify the Software. Object Code: means the binary files originating from the compilation of the Source Code. Holder: means the holder(s) of the economic rights over the Initial Software. Licensee: means the Software user(s) having accepted the Agreement. Contributor: means a Licensee having made at least one Integrated Contribution. Licensor: means the Holder, or any other individual or legal entity, who distributes the Software under the Agreement. Integrated Contribution: means any or all modifications, corrections, translations, adaptations and/or new functions integrated into the Source Code by any or all Contributors. Related Module: means a set of sources files including their documentation that, without modification to the Source Code, enables supplementary functions or services in addition to those offered by the Software. Derivative Software: means any combination of the Software, modified or not, and of a Related Module. Parties: mean both the Licensee and the Licensor. These expressions may be used both in singular and plural form. Article 2 - PURPOSE \u00b6 The purpose of the Agreement is the grant by the Licensor to the Licensee of a non-exclusive, transferable and worldwide license for the Software as set forth in Article 5 hereinafter for the whole term of the protection granted by the rights over said Software. Article 3 - ACCEPTANCE \u00b6 3.1 The Licensee shall be deemed as having accepted the terms and conditions of this Agreement upon the occurrence of the first of the following events: (i) loading the Software by any or all means, notably, by downloading from a remote server, or by loading from a physical medium; (ii) the first time the Licensee exercises any of the rights granted hereunder. 3.2 One copy of the Agreement, containing a notice relating to the characteristics of the Software, to the limited warranty, and to the fact that its use is restricted to experienced users has been provided to the Licensee prior to its acceptance as set forth in Article 3.1 hereinabove, and the Licensee hereby acknowledges that it has read and understood it. Article 4 - EFFECTIVE DATE AND TERM \u00b6 4.1 EFFECTIVE DATE \u00b6 The Agreement shall become effective on the date when it is accepted by the Licensee as set forth in Article 3.1 . 4.2 TERM \u00b6 The Agreement shall remain in force for the entire legal term of protection of the economic rights over the Software. Article 5 - SCOPE OF RIGHTS GRANTED \u00b6 The Licensor hereby grants to the Licensee, who accepts, the following rights over the Software for any or all use, and for the term of the Agreement, on the basis of the terms and conditions set forth hereinafter. Besides, if the Licensor owns or comes to own one or more patents protecting all or part of the functions of the Software or of its components, the Licensor undertakes not to enforce the rights granted by these patents against successive Licensees using, exploiting or modifying the Software. If these patents are transferred, the Licensor undertakes to have the transferees subscribe to the obligations set forth in this paragraph. 5.1 RIGHT OF USE \u00b6 The Licensee is authorized to use the Software, without any limitation as to its fields of application, with it being hereinafter specified that this comprises: permanent or temporary reproduction of all or part of the Software by any or all means and in any or all form. loading, displaying, running, or storing the Software on any or all medium. entitlement to observe, study or test its operation so as to determine the ideas and principles behind any or all constituent elements of said Software. This shall apply when the Licensee carries out any or all loading, displaying, running, transmission or storage operation as regards the Software, that it is entitled to carry out hereunder. 5.2 RIGHT OF MODIFICATION \u00b6 The right of modification includes the right to translate, adapt, arrange, or make any or all modifications to the Software, and the right to reproduce the resulting software. It includes, in particular, the right to create a Derivative Software. The Licensee is authorized to make any or all modification to the Software provided that it includes an explicit notice that it is the author of said modification and indicates the date of the creation thereof. 5.3 RIGHT OF DISTRIBUTION \u00b6 In particular, the right of distribution includes the right to publish, transmit and communicate the Software to the general public on any or all medium, and by any or all means, and the right to market, either in consideration of a fee, or free of charge, one or more copies of the Software by any means. The Licensee is further authorized to distribute copies of the modified or unmodified Software to third parties according to the terms and conditions set forth hereinafter. 5.3.1 DISTRIBUTION OF SOFTWARE WITHOUT MODIFICATION \u00b6 The Licensee is authorized to distribute true copies of the Software in Source Code or Object Code form, provided that said distribution complies with all the provisions of the Agreement and is accompanied by: a copy of the Agreement, a notice relating to the limitation of both the Licensor's warranty and liability as set forth in Article 8 and Article 9 , and that, in the event that only the Object Code of the Software is redistributed, the Licensee allows effective access to the full Source Code of the Software at a minimum during the entire period of its distribution of the Software, it being understood that the additional cost of acquiring the Source Code shall not exceed the cost of transferring the data. 5.3.2 DISTRIBUTION OF MODIFIED SOFTWARE \u00b6 When the Licensee makes an Integrated Contribution to the Software, the terms and conditions for the distribution of the resulting Modified Software become subject to all the provisions of this Agreement. The Licensee is authorized to distribute the Modified Software, in source code or object code form, provided that said distribution complies with all the provisions of the Agreement and is accompanied by: a copy of the Agreement, a notice relating to the limitation of both the Licensor's warranty and liability as set forth in Article 8 and Article 9 , and that, in the event that only the object code of the Modified Software is redistributed, the Licensee allows effective access to the full source code of the Modified Software at a minimum during the entire period of its distribution of the Modified Software, it being understood that the additional cost of acquiring the source code shall not exceed the cost of transferring the data. 5.3.3 DISTRIBUTION OF DERIVATIVE SOFTWARE \u00b6 When the Licensee creates Derivative Software, this Derivative Software may be distributed under a license agreement other than this Agreement, subject to compliance with the requirement to include a notice concerning the rights over the Software as defined in Article 6.4 . In the event the creation of the Derivative Software required modification of the Source Code, the Licensee undertakes that: the resulting Modified Software will be governed by this Agreement, the Integrated Contributions in the resulting Modified Software will be clearly identified and documented, the Licensee will allow effective access to the source code of the Modified Software, at a minimum during the entire period of distribution of the Derivative Software, such that such modifications may be carried over in a subsequent version of the Software; it being understood that the additional cost of purchasing the source code of the Modified Software shall not exceed the cost of transferring the data. 5.3.4 COMPATIBILITY WITH THE CeCILL LICENSE \u00b6 When a Modified Software contains an Integrated Contribution subject to the CeCILL license agreement, or when a Derivative Software contains a Related Module subject to the CeCILL license agreement, the provisions set forth in the third item of Article 6.4 are optional. Article 6 - INTELLECTUAL PROPERTY \u00b6 6.1 OVER THE INITIAL SOFTWARE \u00b6 The Holder owns the economic rights over the Initial Software. Any or all use of the Initial Software is subject to compliance with the terms and conditions under which the Holder has elected to distribute its work and no one shall be entitled to modify the terms and conditions for the distribution of said Initial Software. The Holder undertakes that the Initial Software will remain ruled at least by this Agreement, for the duration set forth in Article 4.2 . 6.2 OVER THE INTEGRATED CONTRIBUTIONS \u00b6 The Licensee who develops an Integrated Contribution is the owner of the intellectual property rights over this Contribution as defined by applicable law. 6.3 OVER THE RELATED MODULES \u00b6 The Licensee who develops a Related Module is the owner of the intellectual property rights over this Related Module as defined by applicable law and is free to choose the type of agreement that shall govern its distribution under the conditions defined in Article 5.3.3 . 6.4 NOTICE OF RIGHTS \u00b6 The Licensee expressly undertakes: not to remove, or modify, in any manner, the intellectual property notices attached to the Software; to reproduce said notices, in an identical manner, in the copies of the Software modified or not; to ensure that use of the Software, its intellectual property notices and the fact that it is governed by the Agreement is indicated in a text that is easily accessible, specifically from the interface of any Derivative Software. The Licensee undertakes not to directly or indirectly infringe the intellectual property rights of the Holder and/or Contributors on the Software and to take, where applicable, vis-\u00e0-vis its staff, any and all measures required to ensure respect of said intellectual property rights of the Holder and/or Contributors. Article 7 - RELATED SERVICES \u00b6 7.1 Under no circumstances shall the Agreement oblige the Licensor to provide technical assistance or maintenance services for the Software. However, the Licensor is entitled to offer this type of services. The terms and conditions of such technical assistance, and/or such maintenance, shall be set forth in a separate instrument. Only the Licensor offering said maintenance and/or technical assistance services shall incur liability therefor. 7.2 Similarly, any Licensor is entitled to offer to its licensees, under its sole responsibility, a warranty, that shall only be binding upon itself, for the redistribution of the Software and/or the Modified Software, under terms and conditions that it is free to decide. Said warranty, and the financial terms and conditions of its application, shall be subject of a separate instrument executed between the Licensor and the Licensee. Article 8 - LIABILITY \u00b6 8.1 Subject to the provisions of Article 8.2, the Licensee shall be entitled to claim compensation for any direct loss it may have suffered from the Software as a result of a fault on the part of the relevant Licensor, subject to providing evidence thereof. 8.2 The Licensor's liability is limited to the commitments made under this Agreement and shall not be incurred as a result of in particular: (i) loss due the Licensee's total or partial failure to fulfill its obligations, (ii) direct or consequential loss that is suffered by the Licensee due to the use or performance of the Software, and (iii) more generally, any consequential loss. In particular the Parties expressly agree that any or all pecuniary or business loss (i.e. loss of data, loss of profits, operating loss, loss of customers or orders, opportunity cost, any disturbance to business activities) or any or all legal proceedings instituted against the Licensee by a third party, shall constitute consequential loss and shall not provide entitlement to any or all compensation from the Licensor. Article 9 - WARRANTY \u00b6 9.1 The Licensee acknowledges that the scientific and technical state-of-the-art when the Software was distributed did not enable all possible uses to be tested and verified, nor for the presence of possible defects to be detected. In this respect, the Licensee's attention has been drawn to the risks associated with loading, using, modifying and/or developing and reproducing the Software which are reserved for experienced users. The Licensee shall be responsible for verifying, by any or all means, the suitability of the product for its requirements, its good working order, and for ensuring that it shall not cause damage to either persons or properties. 9.2 The Licensor hereby represents, in good faith, that it is entitled to grant all the rights over the Software (including in particular the rights set forth in Article 5 ). 9.3 The Licensee acknowledges that the Software is supplied \"as is\" by the Licensor without any other express or tacit warranty, other than that provided for in Article 9.2 and, in particular, without any warranty as to its commercial value, its secured, safe, innovative or relevant nature. Specifically, the Licensor does not warrant that the Software is free from any error, that it will operate without interruption, that it will be compatible with the Licensee's own equipment and software configuration, nor that it will meet the Licensee's requirements. 9.4 The Licensor does not either expressly or tacitly warrant that the Software does not infringe any third party intellectual property right relating to a patent, software or any other property right. Therefore, the Licensor disclaims any and all liability towards the Licensee arising out of any or all proceedings for infringement that may be instituted in respect of the use, modification and redistribution of the Software. Nevertheless, should such proceedings be instituted against the Licensee, the Licensor shall provide it with technical and legal assistance for its defense. Such technical and legal assistance shall be decided on a case-by-case basis between the relevant Licensor and the Licensee pursuant to a memorandum of understanding. The Licensor disclaims any and all liability as regards the Licensee's use of the name of the Software. No warranty is given as regards the existence of prior rights over the name of the Software or as regards the existence of a trademark. Article 10 - TERMINATION \u00b6 10.1 In the event of a breach by the Licensee of its obligations hereunder, the Licensor may automatically terminate this Agreement thirty (30) days after notice has been sent to the Licensee and has remained ineffective. 10.2 A Licensee whose Agreement is terminated shall no longer be authorized to use, modify or distribute the Software. However, any licenses that it may have granted prior to termination of the Agreement shall remain valid subject to their having been granted in compliance with the terms and conditions hereof. Article 11 - MISCELLANEOUS \u00b6 11.1 EXCUSABLE EVENTS Neither Party shall be liable for any or all delay, or failure to perform the Agreement, that may be attributable to an event of force majeure, an act of God or an outside cause, such as defective functioning or interruptions of the electricity or telecommunications networks, network paralysis following a virus attack, intervention by government authorities, natural disasters, water damage, earthquakes, fire, explosions, strikes and labor unrest, war, etc. 11.2 Any failure by either Party, on one or more occasions, to invoke one or more of the provisions hereof, shall under no circumstances be interpreted as being a waiver by the interested Party of its right to invoke said provision(s) subsequently. 11.3 The Agreement cancels and replaces any or all previous agreements, whether written or oral, between the Parties and having the same purpose, and constitutes the entirety of the agreement between said Parties concerning said purpose. No supplement or modification to the terms and conditions hereof shall be effective as between the Parties unless it is made in writing and signed by their duly authorized representatives. 11.4 In the event that one or more of the provisions hereof were to conflict with a current or future applicable act or legislative text, said act or legislative text shall prevail, and the Parties shall make the necessary amendments so as to comply with said act or legislative text. All other provisions shall remain effective. Similarly, invalidity of a provision of the Agreement, for any reason whatsoever, shall not cause the Agreement as a whole to be invalid. 11.5 LANGUAGE The Agreement is drafted in both French and English and both versions are deemed authentic. Article 12 - NEW VERSIONS OF THE AGREEMENT \u00b6 12.1 Any person is authorized to duplicate and distribute copies of this Agreement. 12.2 So as to ensure coherence, the wording of this Agreement is protected and may only be modified by the authors of the License, who reserve the right to periodically publish updates or new versions of the Agreement, each with a separate number. These subsequent versions may address new issues encountered by Free Software. 12.3 Any Software distributed under a given version of the Agreement may only be subsequently distributed under the same version of the Agreement or a subsequent version. Article 13 - GOVERNING LAW AND JURISDICTION \u00b6 13.1 The Agreement is governed by French law. The Parties agree to endeavor to seek an amicable solution to any disagreements or disputes that may arise during the performance of the Agreement. 13.2 Failing an amicable solution within two (2) months as from their occurrence, and unless emergency proceedings are necessary, the disagreements or disputes shall be referred to the Paris Courts having jurisdiction, by the more diligent Party. Version 1.0 dated 2006-09-05.","title":"License"},{"location":"license/#cecill-c-free-software-license-agreement","text":"","title":"CeCILL-C FREE SOFTWARE LICENSE AGREEMENT"},{"location":"license/#notice","text":"This Agreement is a Free Software license agreement that is the result of discussions between its authors in order to ensure compliance with the two main principles guiding its drafting: firstly, compliance with the principles governing the distribution of Free Software: access to source code, broad rights granted to users, secondly, the election of a governing law, French law, with which it is conformant, both as regards the law of torts and intellectual property law, and the protection that it offers to both authors and holders of the economic rights over software. The authors of the CeCILL-C license are: Commissariat \u00e0 l'Energie Atomique - CEA, a public scientific, technical and industrial research establishment, having its principal place of business at 25 rue Leblanc, immeuble Le Ponant D, 75015 Paris, France. Centre National de la Recherche Scientifique - CNRS, a public scientific and technological establishment, having its principal place of business at 3 rue Michel-Ange, 75794 Paris cedex 16, France. Institut National de Recherche en Informatique et en Automatique - INRIA, a public scientific and technological establishment, having its principal place of business at Domaine de Voluceau, Rocquencourt, BP 105, 78153 Le Chesnay cedex, France. CeCILL stands for Ce(a) C(nrs) I(nria) L(ogiciel) L(ibre)","title":"Notice"},{"location":"license/#preamble","text":"The purpose of this Free Software license agreement is to grant users the right to modify and re-use the software governed by this license. The exercising of this right is conditional upon the obligation to make available to the community the modifications made to the source code of the software so as to contribute to its evolution. In consideration of access to the source code and the rights to copy, modify and redistribute granted by the license, users are provided only with a limited warranty and the software's author, the holder of the economic rights, and the successive licensors only have limited liability. In this respect, the risks associated with loading, using, modifying and/or developing or reproducing the software by the user are brought to the user's attention, given its Free Software status, which may make it complicated to use, with the result that its use is reserved for developers and experienced professionals having in-depth computer knowledge. Users are therefore encouraged to load and test the suitability of the software as regards their requirements in conditions enabling the security of their systems and/or data to be ensured and, more generally, to use and operate it in the same conditions of security. This Agreement may be freely reproduced and published, provided it is not altered, and that no provisions are either added or removed herefrom. This Agreement may apply to any or all software for which the holder of the economic rights decides to submit the use thereof to its provisions.","title":"Preamble"},{"location":"license/#article-1-definitions","text":"For the purpose of this Agreement, when the following expressions commence with a capital letter, they shall have the following meaning: Agreement: means this license agreement, and its possible subsequent versions and annexes. Software: means the software in its Object Code and/or Source Code form and, where applicable, its documentation, \"as is\" when the Licensee accepts the Agreement. Initial Software: means the Software in its Source Code and possibly its Object Code form and, where applicable, its documentation, \"as is\" when it is first distributed under the terms and conditions of the Agreement. Modified Software: means the Software modified by at least one Integrated Contribution. Source Code: means all the Software's instructions and program lines to which access is required so as to modify the Software. Object Code: means the binary files originating from the compilation of the Source Code. Holder: means the holder(s) of the economic rights over the Initial Software. Licensee: means the Software user(s) having accepted the Agreement. Contributor: means a Licensee having made at least one Integrated Contribution. Licensor: means the Holder, or any other individual or legal entity, who distributes the Software under the Agreement. Integrated Contribution: means any or all modifications, corrections, translations, adaptations and/or new functions integrated into the Source Code by any or all Contributors. Related Module: means a set of sources files including their documentation that, without modification to the Source Code, enables supplementary functions or services in addition to those offered by the Software. Derivative Software: means any combination of the Software, modified or not, and of a Related Module. Parties: mean both the Licensee and the Licensor. These expressions may be used both in singular and plural form.","title":"Article 1 - DEFINITIONS"},{"location":"license/#article-2-purpose","text":"The purpose of the Agreement is the grant by the Licensor to the Licensee of a non-exclusive, transferable and worldwide license for the Software as set forth in Article 5 hereinafter for the whole term of the protection granted by the rights over said Software.","title":"Article 2 - PURPOSE"},{"location":"license/#article-3-acceptance","text":"3.1 The Licensee shall be deemed as having accepted the terms and conditions of this Agreement upon the occurrence of the first of the following events: (i) loading the Software by any or all means, notably, by downloading from a remote server, or by loading from a physical medium; (ii) the first time the Licensee exercises any of the rights granted hereunder. 3.2 One copy of the Agreement, containing a notice relating to the characteristics of the Software, to the limited warranty, and to the fact that its use is restricted to experienced users has been provided to the Licensee prior to its acceptance as set forth in Article 3.1 hereinabove, and the Licensee hereby acknowledges that it has read and understood it.","title":" Article 3 - ACCEPTANCE"},{"location":"license/#article-4-effective-date-and-term","text":"","title":" Article 4 - EFFECTIVE DATE AND TERM"},{"location":"license/#41-effective-date","text":"The Agreement shall become effective on the date when it is accepted by the Licensee as set forth in Article 3.1 .","title":" 4.1 EFFECTIVE DATE"},{"location":"license/#42-term","text":"The Agreement shall remain in force for the entire legal term of protection of the economic rights over the Software.","title":" 4.2 TERM"},{"location":"license/#article-5-scope-of-rights-granted","text":"The Licensor hereby grants to the Licensee, who accepts, the following rights over the Software for any or all use, and for the term of the Agreement, on the basis of the terms and conditions set forth hereinafter. Besides, if the Licensor owns or comes to own one or more patents protecting all or part of the functions of the Software or of its components, the Licensor undertakes not to enforce the rights granted by these patents against successive Licensees using, exploiting or modifying the Software. If these patents are transferred, the Licensor undertakes to have the transferees subscribe to the obligations set forth in this paragraph.","title":" Article 5 - SCOPE OF RIGHTS GRANTED"},{"location":"license/#51-right-of-use","text":"The Licensee is authorized to use the Software, without any limitation as to its fields of application, with it being hereinafter specified that this comprises: permanent or temporary reproduction of all or part of the Software by any or all means and in any or all form. loading, displaying, running, or storing the Software on any or all medium. entitlement to observe, study or test its operation so as to determine the ideas and principles behind any or all constituent elements of said Software. This shall apply when the Licensee carries out any or all loading, displaying, running, transmission or storage operation as regards the Software, that it is entitled to carry out hereunder.","title":" 5.1 RIGHT OF USE"},{"location":"license/#52-right-of-modification","text":"The right of modification includes the right to translate, adapt, arrange, or make any or all modifications to the Software, and the right to reproduce the resulting software. It includes, in particular, the right to create a Derivative Software. The Licensee is authorized to make any or all modification to the Software provided that it includes an explicit notice that it is the author of said modification and indicates the date of the creation thereof.","title":" 5.2 RIGHT OF MODIFICATION"},{"location":"license/#53-right-of-distribution","text":"In particular, the right of distribution includes the right to publish, transmit and communicate the Software to the general public on any or all medium, and by any or all means, and the right to market, either in consideration of a fee, or free of charge, one or more copies of the Software by any means. The Licensee is further authorized to distribute copies of the modified or unmodified Software to third parties according to the terms and conditions set forth hereinafter.","title":" 5.3 RIGHT OF DISTRIBUTION"},{"location":"license/#531-distribution-of-software-without-modification","text":"The Licensee is authorized to distribute true copies of the Software in Source Code or Object Code form, provided that said distribution complies with all the provisions of the Agreement and is accompanied by: a copy of the Agreement, a notice relating to the limitation of both the Licensor's warranty and liability as set forth in Article 8 and Article 9 , and that, in the event that only the Object Code of the Software is redistributed, the Licensee allows effective access to the full Source Code of the Software at a minimum during the entire period of its distribution of the Software, it being understood that the additional cost of acquiring the Source Code shall not exceed the cost of transferring the data.","title":" 5.3.1 DISTRIBUTION OF SOFTWARE WITHOUT MODIFICATION"},{"location":"license/#532-distribution-of-modified-software","text":"When the Licensee makes an Integrated Contribution to the Software, the terms and conditions for the distribution of the resulting Modified Software become subject to all the provisions of this Agreement. The Licensee is authorized to distribute the Modified Software, in source code or object code form, provided that said distribution complies with all the provisions of the Agreement and is accompanied by: a copy of the Agreement, a notice relating to the limitation of both the Licensor's warranty and liability as set forth in Article 8 and Article 9 , and that, in the event that only the object code of the Modified Software is redistributed, the Licensee allows effective access to the full source code of the Modified Software at a minimum during the entire period of its distribution of the Modified Software, it being understood that the additional cost of acquiring the source code shall not exceed the cost of transferring the data.","title":" 5.3.2 DISTRIBUTION OF MODIFIED SOFTWARE"},{"location":"license/#533-distribution-of-derivative-software","text":"When the Licensee creates Derivative Software, this Derivative Software may be distributed under a license agreement other than this Agreement, subject to compliance with the requirement to include a notice concerning the rights over the Software as defined in Article 6.4 . In the event the creation of the Derivative Software required modification of the Source Code, the Licensee undertakes that: the resulting Modified Software will be governed by this Agreement, the Integrated Contributions in the resulting Modified Software will be clearly identified and documented, the Licensee will allow effective access to the source code of the Modified Software, at a minimum during the entire period of distribution of the Derivative Software, such that such modifications may be carried over in a subsequent version of the Software; it being understood that the additional cost of purchasing the source code of the Modified Software shall not exceed the cost of transferring the data.","title":" 5.3.3 DISTRIBUTION OF DERIVATIVE SOFTWARE"},{"location":"license/#534-compatibility-with-the-cecill-license","text":"When a Modified Software contains an Integrated Contribution subject to the CeCILL license agreement, or when a Derivative Software contains a Related Module subject to the CeCILL license agreement, the provisions set forth in the third item of Article 6.4 are optional.","title":" 5.3.4 COMPATIBILITY WITH THE CeCILL LICENSE"},{"location":"license/#article-6-intellectual-property","text":"","title":" Article 6 - INTELLECTUAL PROPERTY"},{"location":"license/#61-over-the-initial-software","text":"The Holder owns the economic rights over the Initial Software. Any or all use of the Initial Software is subject to compliance with the terms and conditions under which the Holder has elected to distribute its work and no one shall be entitled to modify the terms and conditions for the distribution of said Initial Software. The Holder undertakes that the Initial Software will remain ruled at least by this Agreement, for the duration set forth in Article 4.2 .","title":" 6.1 OVER THE INITIAL SOFTWARE"},{"location":"license/#62-over-the-integrated-contributions","text":"The Licensee who develops an Integrated Contribution is the owner of the intellectual property rights over this Contribution as defined by applicable law.","title":" 6.2 OVER THE INTEGRATED CONTRIBUTIONS"},{"location":"license/#63-over-the-related-modules","text":"The Licensee who develops a Related Module is the owner of the intellectual property rights over this Related Module as defined by applicable law and is free to choose the type of agreement that shall govern its distribution under the conditions defined in Article 5.3.3 .","title":" 6.3 OVER THE RELATED MODULES"},{"location":"license/#64-notice-of-rights","text":"The Licensee expressly undertakes: not to remove, or modify, in any manner, the intellectual property notices attached to the Software; to reproduce said notices, in an identical manner, in the copies of the Software modified or not; to ensure that use of the Software, its intellectual property notices and the fact that it is governed by the Agreement is indicated in a text that is easily accessible, specifically from the interface of any Derivative Software. The Licensee undertakes not to directly or indirectly infringe the intellectual property rights of the Holder and/or Contributors on the Software and to take, where applicable, vis-\u00e0-vis its staff, any and all measures required to ensure respect of said intellectual property rights of the Holder and/or Contributors.","title":" 6.4 NOTICE OF RIGHTS"},{"location":"license/#article-7-related-services","text":"7.1 Under no circumstances shall the Agreement oblige the Licensor to provide technical assistance or maintenance services for the Software. However, the Licensor is entitled to offer this type of services. The terms and conditions of such technical assistance, and/or such maintenance, shall be set forth in a separate instrument. Only the Licensor offering said maintenance and/or technical assistance services shall incur liability therefor. 7.2 Similarly, any Licensor is entitled to offer to its licensees, under its sole responsibility, a warranty, that shall only be binding upon itself, for the redistribution of the Software and/or the Modified Software, under terms and conditions that it is free to decide. Said warranty, and the financial terms and conditions of its application, shall be subject of a separate instrument executed between the Licensor and the Licensee.","title":" Article 7 - RELATED SERVICES"},{"location":"license/#article-8-liability","text":"8.1 Subject to the provisions of Article 8.2, the Licensee shall be entitled to claim compensation for any direct loss it may have suffered from the Software as a result of a fault on the part of the relevant Licensor, subject to providing evidence thereof. 8.2 The Licensor's liability is limited to the commitments made under this Agreement and shall not be incurred as a result of in particular: (i) loss due the Licensee's total or partial failure to fulfill its obligations, (ii) direct or consequential loss that is suffered by the Licensee due to the use or performance of the Software, and (iii) more generally, any consequential loss. In particular the Parties expressly agree that any or all pecuniary or business loss (i.e. loss of data, loss of profits, operating loss, loss of customers or orders, opportunity cost, any disturbance to business activities) or any or all legal proceedings instituted against the Licensee by a third party, shall constitute consequential loss and shall not provide entitlement to any or all compensation from the Licensor.","title":" Article 8 - LIABILITY"},{"location":"license/#article-9-warranty","text":"9.1 The Licensee acknowledges that the scientific and technical state-of-the-art when the Software was distributed did not enable all possible uses to be tested and verified, nor for the presence of possible defects to be detected. In this respect, the Licensee's attention has been drawn to the risks associated with loading, using, modifying and/or developing and reproducing the Software which are reserved for experienced users. The Licensee shall be responsible for verifying, by any or all means, the suitability of the product for its requirements, its good working order, and for ensuring that it shall not cause damage to either persons or properties. 9.2 The Licensor hereby represents, in good faith, that it is entitled to grant all the rights over the Software (including in particular the rights set forth in Article 5 ). 9.3 The Licensee acknowledges that the Software is supplied \"as is\" by the Licensor without any other express or tacit warranty, other than that provided for in Article 9.2 and, in particular, without any warranty as to its commercial value, its secured, safe, innovative or relevant nature. Specifically, the Licensor does not warrant that the Software is free from any error, that it will operate without interruption, that it will be compatible with the Licensee's own equipment and software configuration, nor that it will meet the Licensee's requirements. 9.4 The Licensor does not either expressly or tacitly warrant that the Software does not infringe any third party intellectual property right relating to a patent, software or any other property right. Therefore, the Licensor disclaims any and all liability towards the Licensee arising out of any or all proceedings for infringement that may be instituted in respect of the use, modification and redistribution of the Software. Nevertheless, should such proceedings be instituted against the Licensee, the Licensor shall provide it with technical and legal assistance for its defense. Such technical and legal assistance shall be decided on a case-by-case basis between the relevant Licensor and the Licensee pursuant to a memorandum of understanding. The Licensor disclaims any and all liability as regards the Licensee's use of the name of the Software. No warranty is given as regards the existence of prior rights over the name of the Software or as regards the existence of a trademark.","title":" Article 9 - WARRANTY"},{"location":"license/#article-10-termination","text":"10.1 In the event of a breach by the Licensee of its obligations hereunder, the Licensor may automatically terminate this Agreement thirty (30) days after notice has been sent to the Licensee and has remained ineffective. 10.2 A Licensee whose Agreement is terminated shall no longer be authorized to use, modify or distribute the Software. However, any licenses that it may have granted prior to termination of the Agreement shall remain valid subject to their having been granted in compliance with the terms and conditions hereof.","title":" Article 10 - TERMINATION"},{"location":"license/#article-11-miscellaneous","text":"11.1 EXCUSABLE EVENTS Neither Party shall be liable for any or all delay, or failure to perform the Agreement, that may be attributable to an event of force majeure, an act of God or an outside cause, such as defective functioning or interruptions of the electricity or telecommunications networks, network paralysis following a virus attack, intervention by government authorities, natural disasters, water damage, earthquakes, fire, explosions, strikes and labor unrest, war, etc. 11.2 Any failure by either Party, on one or more occasions, to invoke one or more of the provisions hereof, shall under no circumstances be interpreted as being a waiver by the interested Party of its right to invoke said provision(s) subsequently. 11.3 The Agreement cancels and replaces any or all previous agreements, whether written or oral, between the Parties and having the same purpose, and constitutes the entirety of the agreement between said Parties concerning said purpose. No supplement or modification to the terms and conditions hereof shall be effective as between the Parties unless it is made in writing and signed by their duly authorized representatives. 11.4 In the event that one or more of the provisions hereof were to conflict with a current or future applicable act or legislative text, said act or legislative text shall prevail, and the Parties shall make the necessary amendments so as to comply with said act or legislative text. All other provisions shall remain effective. Similarly, invalidity of a provision of the Agreement, for any reason whatsoever, shall not cause the Agreement as a whole to be invalid. 11.5 LANGUAGE The Agreement is drafted in both French and English and both versions are deemed authentic.","title":" Article 11 - MISCELLANEOUS"},{"location":"license/#article-12-new-versions-of-the-agreement","text":"12.1 Any person is authorized to duplicate and distribute copies of this Agreement. 12.2 So as to ensure coherence, the wording of this Agreement is protected and may only be modified by the authors of the License, who reserve the right to periodically publish updates or new versions of the Agreement, each with a separate number. These subsequent versions may address new issues encountered by Free Software. 12.3 Any Software distributed under a given version of the Agreement may only be subsequently distributed under the same version of the Agreement or a subsequent version.","title":" Article 12 - NEW VERSIONS OF THE AGREEMENT"},{"location":"license/#article-13-governing-law-and-jurisdiction","text":"13.1 The Agreement is governed by French law. The Parties agree to endeavor to seek an amicable solution to any disagreements or disputes that may arise during the performance of the Agreement. 13.2 Failing an amicable solution within two (2) months as from their occurrence, and unless emergency proceedings are necessary, the disagreements or disputes shall be referred to the Paris Courts having jurisdiction, by the more diligent Party. Version 1.0 dated 2006-09-05.","title":" Article 13 - GOVERNING LAW AND JURISDICTION"},{"location":"usage/","text":"Usage \u00b6 Import dependencies. from cognitivefactory.interactive_clustering.utils.preprocessing import preprocess from cognitivefactory.interactive_clustering.utils.vectorization import vectorize from cognitivefactory.interactive_clustering.constraints.factory import managing_factory from cognitivefactory.interactive_clustering.clustering.factory import clustering_factory from cognitivefactory.interactive_clustering.sampling.factory import sampling_factory Initialization step (iteration 0 ) \u00b6 Get data. # Define dictionary of texts. dict_of_texts = { \"0\" : \"This is my first question.\" , \"1\" : \"This is my second item.\" , \"2\" : \"This is my third query.\" , \"3\" : \"This is my fourth issue.\" , # ... \"N\" : \"This is my last request.\" , } Preprocess data. # Preprocess data. dict_of_preprocess_texts = preprocess ( dict_of_texts = dict_of_texts , spacy_language_model = \"fr_core_news_sm\" , ) # Apply simple preprocessing. Spacy language model has to be installed. Other parameters are available. Vectorize data. # Vectorize data. dict_of_vectors = vectorize ( dict_of_texts = dict_of_preprocess_texts , vectorizer_type = \"tfidf\" , ) # Apply TF-IDF vectorization. Other parameters are available. Initialize constraints manager. # Create an instance of binary constraints manager. constraints_manager = managing_factory ( manager = \"binary\" , list_of_data_IDs = list ( dict_of_texts . keys ()), ) Apply first clustering without constraints. # Create an instance of constrained COP-kmeans clustering. clustering_model = clustering_factory ( algorithm = \"kmeans\" , random_seed = 1 , ) # Other clustering algorithms are available. # Run clustering. clustering_result = clustering_model . cluster ( constraints_manager = constraints_manager , nb_clusters = 2 , vectors = dict_of_vectors , ) Iteration step (iteration N ) \u00b6 Check if all possible constraints are annotated. # Check if all constraints are already annotated. is_finish = constraints_manager . check_completude_of_constraints () # Print result if is_finish : print ( \"All possible constraints are annotated. No more iteration can be run.\" ) # break Sampling constraints to annotate. # Create an instance of random sampler. sampler = sampling_factory ( algorithm = \"random\" , random_seed = None , ) # Other algorithms are available. # Sample constraints to annotated. selection = sampler . sample ( constraints_manager = constraints_manager , nb_to_select = 3 , #clustering_result=clustering_result, # Results from iteration `N-1`. #vectors=dict_of_vectors, ) Annotate constraints (manual operation). # TODO: Use a graphical interface for interactive clustering. # WIP: Project `interactive-clustering-gui`. list_of_annotation = [] # List of triplets with format `(data_ID1, data_ID2, annotation_type)` where `annotation_type` can be \"MUST_LINK\" or \"CANNOT_LINK\". Update constraints manager. for annotation in list_of_annotation : # Get the annotation data_ID1 , data_ID2 , constraint_type = annotation # Add constraints try : constraints_manager . add_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , constraint_type = constraint_type ) except ValueError as err : print ( err ) # An error can occur if parameters are incorrect or if annotation is incompatible with previous annotation. Determine the range of possible cluster number. # Get min and max range of clusters based on constraints. min_n , max_n = constraints_manager . get_min_and_max_number_of_clusters () # Choose the number of cluster. nb_clusters = int ( ( min_n + max_n ) / 2 ) # or manual selection. Run constrained clustering. # Create an instance of constrained COP-kmeans clustering. clustering_model = clustering_factory ( algorithm = \"kmeans\" , random_seed = 1 , ) # Other clustering algorithms are available. # Run clustering. clustering_result = clustering_model . cluster ( constraints_manager = constraints_manager , # Annotation since iteration `0`. nb_clusters = nb_clusters , vectors = dict_of_vectors , ) # Clustering results are corrected since the previous iteration. Analyze cluster (not implemented here). # TODO: Evaluate completness, homogeneity, v-measure, rand index (basic, adjusted), mutual information (basic, normalized, mutual), ... # TODO: Plot clustering.","title":"Usage"},{"location":"usage/#usage","text":"Import dependencies. from cognitivefactory.interactive_clustering.utils.preprocessing import preprocess from cognitivefactory.interactive_clustering.utils.vectorization import vectorize from cognitivefactory.interactive_clustering.constraints.factory import managing_factory from cognitivefactory.interactive_clustering.clustering.factory import clustering_factory from cognitivefactory.interactive_clustering.sampling.factory import sampling_factory","title":"Usage"},{"location":"usage/#initialization-step-iteration-0","text":"Get data. # Define dictionary of texts. dict_of_texts = { \"0\" : \"This is my first question.\" , \"1\" : \"This is my second item.\" , \"2\" : \"This is my third query.\" , \"3\" : \"This is my fourth issue.\" , # ... \"N\" : \"This is my last request.\" , } Preprocess data. # Preprocess data. dict_of_preprocess_texts = preprocess ( dict_of_texts = dict_of_texts , spacy_language_model = \"fr_core_news_sm\" , ) # Apply simple preprocessing. Spacy language model has to be installed. Other parameters are available. Vectorize data. # Vectorize data. dict_of_vectors = vectorize ( dict_of_texts = dict_of_preprocess_texts , vectorizer_type = \"tfidf\" , ) # Apply TF-IDF vectorization. Other parameters are available. Initialize constraints manager. # Create an instance of binary constraints manager. constraints_manager = managing_factory ( manager = \"binary\" , list_of_data_IDs = list ( dict_of_texts . keys ()), ) Apply first clustering without constraints. # Create an instance of constrained COP-kmeans clustering. clustering_model = clustering_factory ( algorithm = \"kmeans\" , random_seed = 1 , ) # Other clustering algorithms are available. # Run clustering. clustering_result = clustering_model . cluster ( constraints_manager = constraints_manager , nb_clusters = 2 , vectors = dict_of_vectors , )","title":"Initialization step (iteration 0)"},{"location":"usage/#iteration-step-iteration-n","text":"Check if all possible constraints are annotated. # Check if all constraints are already annotated. is_finish = constraints_manager . check_completude_of_constraints () # Print result if is_finish : print ( \"All possible constraints are annotated. No more iteration can be run.\" ) # break Sampling constraints to annotate. # Create an instance of random sampler. sampler = sampling_factory ( algorithm = \"random\" , random_seed = None , ) # Other algorithms are available. # Sample constraints to annotated. selection = sampler . sample ( constraints_manager = constraints_manager , nb_to_select = 3 , #clustering_result=clustering_result, # Results from iteration `N-1`. #vectors=dict_of_vectors, ) Annotate constraints (manual operation). # TODO: Use a graphical interface for interactive clustering. # WIP: Project `interactive-clustering-gui`. list_of_annotation = [] # List of triplets with format `(data_ID1, data_ID2, annotation_type)` where `annotation_type` can be \"MUST_LINK\" or \"CANNOT_LINK\". Update constraints manager. for annotation in list_of_annotation : # Get the annotation data_ID1 , data_ID2 , constraint_type = annotation # Add constraints try : constraints_manager . add_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , constraint_type = constraint_type ) except ValueError as err : print ( err ) # An error can occur if parameters are incorrect or if annotation is incompatible with previous annotation. Determine the range of possible cluster number. # Get min and max range of clusters based on constraints. min_n , max_n = constraints_manager . get_min_and_max_number_of_clusters () # Choose the number of cluster. nb_clusters = int ( ( min_n + max_n ) / 2 ) # or manual selection. Run constrained clustering. # Create an instance of constrained COP-kmeans clustering. clustering_model = clustering_factory ( algorithm = \"kmeans\" , random_seed = 1 , ) # Other clustering algorithms are available. # Run clustering. clustering_result = clustering_model . cluster ( constraints_manager = constraints_manager , # Annotation since iteration `0`. nb_clusters = nb_clusters , vectors = dict_of_vectors , ) # Clustering results are corrected since the previous iteration. Analyze cluster (not implemented here). # TODO: Evaluate completness, homogeneity, v-measure, rand index (basic, adjusted), mutual information (basic, normalized, mutual), ... # TODO: Plot clustering.","title":"Iteration step (iteration N)"},{"location":"reference/clustering/abstract/","text":"Name: cognitivefactory.interactive_clustering.clustering.abstract Description: The abstract class used to define constrained clustering algorithms. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL-C License v1.0 ( https://cecill.info/licences.fr.html ) AbstractConstrainedClustering ( ABC ) \u00b6 Abstract class that is used to define constrained clustering algorithms. The main inherited method is cluster . References Survey on Constrained Clustering : Lampert, T., T.-B.-H. Dao, B. Lafabregue, N. Serrette, G. Forestier, B. Cremilleux, C. Vrain, et P. Gancarski (2018). Constrained distance based clustering for time-series : a comparative and experimental study. Data Mining and Knowledge Discovery 32(6), 1663\u20131707. cluster ( self , constraints_manager , vectors , nb_clusters , verbose = False , ** kargs ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to cluster data. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs that will force clustering to respect some conditions during computation. required vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required nb_clusters int The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? required verbose bool Enable verbose output. Defaults to False . False **kargs dict Other parameters that can be used in the clustering. {} Exceptions: Type Description ValueError if vectors and constraints_manager are incompatible, or if some parameters are incorrectly set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\abstract.py @abstractmethod def cluster ( self , constraints_manager : AbstractConstraintsManager , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], nb_clusters : int , verbose : bool = False , ** kargs , ) -> Dict [ str , int ]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to cluster data. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation. vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). nb_clusters (int): The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? verbose (bool, optional): Enable verbose output. Defaults to `False`. **kargs (dict): Other parameters that can be used in the clustering. Raises: ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" rename_clusters_by_order ( clusters ) \u00b6 Rename cluster ID to be ordered by data IDs. Parameters: Name Type Description Default clusters Dict[str, int] The dictionary of clusters. required Returns: Type Description Dict[str, int] The sorted dictionary of clusters. Source code in interactive_clustering\\clustering\\abstract.py def rename_clusters_by_order ( clusters : Dict [ str , int ], ) -> Dict [ str , int ]: \"\"\" Rename cluster ID to be ordered by data IDs. Args: clusters (Dict[str, int]): The dictionary of clusters. Returns: Dict[str, int]: The sorted dictionary of clusters. \"\"\" # Get `list_of_data_IDs`. list_of_data_IDs = sorted ( clusters . keys ()) # Define a map to be able to rename cluster IDs. mapping_of_old_ID_to_new_ID : Dict [ int , int ] = {} new_ID : int = 0 for data_ID in list_of_data_IDs : # , cluster_ID in clusters.items(): if clusters [ data_ID ] not in mapping_of_old_ID_to_new_ID . keys (): mapping_of_old_ID_to_new_ID [ clusters [ data_ID ]] = new_ID new_ID += 1 # Rename cluster IDs. new_clusters = { data_ID_to_assign : mapping_of_old_ID_to_new_ID [ clusters [ data_ID_to_assign ]] for data_ID_to_assign in list_of_data_IDs } # Return the new ordered clusters return new_clusters","title":"clustering.abstract"},{"location":"reference/clustering/abstract/#cognitivefactory.interactive_clustering.clustering.abstract.AbstractConstrainedClustering","text":"Abstract class that is used to define constrained clustering algorithms. The main inherited method is cluster . References Survey on Constrained Clustering : Lampert, T., T.-B.-H. Dao, B. Lafabregue, N. Serrette, G. Forestier, B. Cremilleux, C. Vrain, et P. Gancarski (2018). Constrained distance based clustering for time-series : a comparative and experimental study. Data Mining and Knowledge Discovery 32(6), 1663\u20131707.","title":"AbstractConstrainedClustering"},{"location":"reference/clustering/abstract/#cognitivefactory.interactive_clustering.clustering.abstract.AbstractConstrainedClustering.cluster","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to cluster data. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs that will force clustering to respect some conditions during computation. required vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required nb_clusters int The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? required verbose bool Enable verbose output. Defaults to False . False **kargs dict Other parameters that can be used in the clustering. {} Exceptions: Type Description ValueError if vectors and constraints_manager are incompatible, or if some parameters are incorrectly set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\abstract.py @abstractmethod def cluster ( self , constraints_manager : AbstractConstraintsManager , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], nb_clusters : int , verbose : bool = False , ** kargs , ) -> Dict [ str , int ]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to cluster data. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation. vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). nb_clusters (int): The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? verbose (bool, optional): Enable verbose output. Defaults to `False`. **kargs (dict): Other parameters that can be used in the clustering. Raises: ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\"","title":"cluster()"},{"location":"reference/clustering/abstract/#cognitivefactory.interactive_clustering.clustering.abstract.rename_clusters_by_order","text":"Rename cluster ID to be ordered by data IDs. Parameters: Name Type Description Default clusters Dict[str, int] The dictionary of clusters. required Returns: Type Description Dict[str, int] The sorted dictionary of clusters. Source code in interactive_clustering\\clustering\\abstract.py def rename_clusters_by_order ( clusters : Dict [ str , int ], ) -> Dict [ str , int ]: \"\"\" Rename cluster ID to be ordered by data IDs. Args: clusters (Dict[str, int]): The dictionary of clusters. Returns: Dict[str, int]: The sorted dictionary of clusters. \"\"\" # Get `list_of_data_IDs`. list_of_data_IDs = sorted ( clusters . keys ()) # Define a map to be able to rename cluster IDs. mapping_of_old_ID_to_new_ID : Dict [ int , int ] = {} new_ID : int = 0 for data_ID in list_of_data_IDs : # , cluster_ID in clusters.items(): if clusters [ data_ID ] not in mapping_of_old_ID_to_new_ID . keys (): mapping_of_old_ID_to_new_ID [ clusters [ data_ID ]] = new_ID new_ID += 1 # Rename cluster IDs. new_clusters = { data_ID_to_assign : mapping_of_old_ID_to_new_ID [ clusters [ data_ID_to_assign ]] for data_ID_to_assign in list_of_data_IDs } # Return the new ordered clusters return new_clusters","title":"rename_clusters_by_order()"},{"location":"reference/clustering/factory/","text":"Name: cognitivefactory.interactive_clustering.clustering.factory Description: The factory method used to easily initialize a constrained clustering algorithm. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL-C License v1.0 ( https://cecill.info/licences.fr.html ) clustering_factory ( algorithm = 'kmeans' , ** kargs ) \u00b6 A factory to create a new instance of a constrained clustering model. Parameters: Name Type Description Default algorithm str The identification of model to instantiate. Can be \"hierarchical\" or \"kmeans\" or \"spectral\" . Defaults to \"kmeans\" . 'kmeans' **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if algorithm is not implemented. Returns: Type Description AbstractConstraintsClustering An instance of constrained clustering model. Examples: # Import. from cognitivefactory.interactive_clustering.clustering.factory import clustering_factory # Create an instance of kmeans. clustering_model = clustering_factory ( algorithm = \"kmeans\" , model = \"COP\" , random_seed = 42 , ) Source code in interactive_clustering\\clustering\\factory.py def clustering_factory ( algorithm : str = \"kmeans\" , ** kargs ) -> \"AbstractConstrainedClustering\" : \"\"\" A factory to create a new instance of a constrained clustering model. Args: algorithm (str): The identification of model to instantiate. Can be `\"hierarchical\"` or `\"kmeans\"` or `\"spectral\"`. Defaults to `\"kmeans\"`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if `algorithm` is not implemented. Returns: AbstractConstraintsClustering: An instance of constrained clustering model. Examples: ```python # Import. from cognitivefactory.interactive_clustering.clustering.factory import clustering_factory # Create an instance of kmeans. clustering_model = clustering_factory( algorithm=\"kmeans\", model=\"COP\", random_seed=42, ) ``` \"\"\" # Check that the requested algorithm is implemented. if algorithm not in { \"hierarchical\" , \"kmeans\" , \"spectral\" }: raise ValueError ( \"The `algorithm` '\" + str ( algorithm ) + \"' is not implemented.\" ) # Case of Hierachical Constrained Clustering. if algorithm == \"hierarchical\" : return HierarchicalConstrainedClustering ( ** kargs ) # Case of Spectral Constrained Clustering. if algorithm == \"spectral\" : return SpectralConstrainedClustering ( ** kargs ) # Case of KMeans Constrained Clustering. ## if algorithm==\"kmeans\": return KMeansConstrainedClustering ( ** kargs )","title":"clustering.factory"},{"location":"reference/clustering/factory/#cognitivefactory.interactive_clustering.clustering.factory.clustering_factory","text":"A factory to create a new instance of a constrained clustering model. Parameters: Name Type Description Default algorithm str The identification of model to instantiate. Can be \"hierarchical\" or \"kmeans\" or \"spectral\" . Defaults to \"kmeans\" . 'kmeans' **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if algorithm is not implemented. Returns: Type Description AbstractConstraintsClustering An instance of constrained clustering model. Examples: # Import. from cognitivefactory.interactive_clustering.clustering.factory import clustering_factory # Create an instance of kmeans. clustering_model = clustering_factory ( algorithm = \"kmeans\" , model = \"COP\" , random_seed = 42 , ) Source code in interactive_clustering\\clustering\\factory.py def clustering_factory ( algorithm : str = \"kmeans\" , ** kargs ) -> \"AbstractConstrainedClustering\" : \"\"\" A factory to create a new instance of a constrained clustering model. Args: algorithm (str): The identification of model to instantiate. Can be `\"hierarchical\"` or `\"kmeans\"` or `\"spectral\"`. Defaults to `\"kmeans\"`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if `algorithm` is not implemented. Returns: AbstractConstraintsClustering: An instance of constrained clustering model. Examples: ```python # Import. from cognitivefactory.interactive_clustering.clustering.factory import clustering_factory # Create an instance of kmeans. clustering_model = clustering_factory( algorithm=\"kmeans\", model=\"COP\", random_seed=42, ) ``` \"\"\" # Check that the requested algorithm is implemented. if algorithm not in { \"hierarchical\" , \"kmeans\" , \"spectral\" }: raise ValueError ( \"The `algorithm` '\" + str ( algorithm ) + \"' is not implemented.\" ) # Case of Hierachical Constrained Clustering. if algorithm == \"hierarchical\" : return HierarchicalConstrainedClustering ( ** kargs ) # Case of Spectral Constrained Clustering. if algorithm == \"spectral\" : return SpectralConstrainedClustering ( ** kargs ) # Case of KMeans Constrained Clustering. ## if algorithm==\"kmeans\": return KMeansConstrainedClustering ( ** kargs )","title":"clustering_factory()"},{"location":"reference/clustering/hierarchical/","text":"Name: cognitivefactory.interactive_clustering.clustering.hierarchical Description: Implementation of constrained hierarchical clustering algorithms. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL-C License v1.0 ( https://cecill.info/licences.fr.html ) Cluster \u00b6 This class represents a cluster as a node of the hierarchical clustering tree. __init__ ( self , vectors , cluster_ID , clustering_iteration , children = None , members = None ) special \u00b6 The constructor for Cluster class. Parameters: Name Type Description Default vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager (if constraints_manager is set). The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required cluster_ID int The cluster ID that is defined during HierarchicalConstrainedClustering.cluster running. required clustering_iteration int The cluster iteration that is defined during HierarchicalConstrainedClustering.cluster running. required children Optional[List[\"Cluster\"]] A list of clusters children for cluster initialization. Incompatible with members parameter. Defaults to None . None members Optional[List[str]] A list of data IDs for cluster initialization. Incompatible with children parameter. Defaults to None . None Exceptions: Type Description ValueError if children and members are both set or both unset. Source code in interactive_clustering\\clustering\\hierarchical.py def __init__ ( self , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], cluster_ID : int , clustering_iteration : int , children : Optional [ List [ \"Cluster\" ]] = None , members : Optional [ List [ str ]] = None , ) -> None : \"\"\" The constructor for Cluster class. Args: vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager` (if `constraints_manager` is set). The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). cluster_ID (int): The cluster ID that is defined during `HierarchicalConstrainedClustering.cluster` running. clustering_iteration (int): The cluster iteration that is defined during `HierarchicalConstrainedClustering.cluster` running. children (Optional[List[\"Cluster\"]], optional): A list of clusters children for cluster initialization. Incompatible with `members` parameter. Defaults to `None`. members (Optional[List[str]], optional): A list of data IDs for cluster initialization. Incompatible with `children` parameter. Defaults to `None`. Raises: ValueError: if `children` and `members` are both set or both unset. \"\"\" # Store links to `vectors`. self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors # Cluster ID and Clustering iteration. self . cluster_ID : int = cluster_ID self . clustering_iteration : int = clustering_iteration # Check children and members. if (( children is not None ) and ( members is not None )) or (( children is None ) and ( members is None )): raise ValueError ( \"Cluster initialization must be by `children` setting or by `members` setting, but not by both or none of them.\" ) # Add children (empty or not). self . children : List [ \"Cluster\" ] = children if ( children is not None ) else [] # Cluster inverse depth. self . cluster_inverse_depth : int = ( max ([ child . cluster_inverse_depth for child in self . children ]) + 1 if ( self . children ) else 0 ) # Add members (empty or not). self . members : List [ str ] = ( members if members is not None else [ data_ID for child in self . children for data_ID in child . members ] ) # Update centroids self . update_centroid () add_new_children ( self , new_children , new_clustering_iteration ) \u00b6 Add new children to the cluster. Parameters: Name Type Description Default new_children List[\"Cluster\"] The list of new clusters children to add. required new_clustering_iteration int The new cluster iteration that is defined during HierarchicalConstrainedClustering.clusterize running. required Source code in interactive_clustering\\clustering\\hierarchical.py def add_new_children ( self , new_children : List [ \"Cluster\" ], new_clustering_iteration : int , ) -> None : \"\"\" Add new children to the cluster. Args: new_children (List[\"Cluster\"]): The list of new clusters children to add. new_clustering_iteration (int): The new cluster iteration that is defined during HierarchicalConstrainedClustering.clusterize running. \"\"\" # Update clustering iteration. self . clustering_iteration = new_clustering_iteration # Update children. self . children += [ new_child for new_child in new_children if new_child not in self . children ] # Update cluster inverse depth. self . cluster_inverse_depth = max ([ child . cluster_inverse_depth for child in self . children ]) + 1 # Update members. self . members = [ data_ID for child in self . children for data_ID in child . members ] # Update centroids. self . update_centroid () get_cluster_size ( self ) \u00b6 Get cluster size. Returns: Type Description int The cluster size, i.e. the number of members in the cluster. Source code in interactive_clustering\\clustering\\hierarchical.py def get_cluster_size ( self ) -> int : \"\"\" Get cluster size. Returns: int: The cluster size, i.e. the number of members in the cluster. \"\"\" # Update centroids. return len ( self . members ) to_dict ( self ) \u00b6 Transform the Cluster object into a dictionary. It can be used before serialize this object in JSON. Returns: Type Description Dict[str, Any] A dictionary that represents the Cluster object. Source code in interactive_clustering\\clustering\\hierarchical.py def to_dict ( self ) -> Dict [ str , Any ]: \"\"\" Transform the Cluster object into a dictionary. It can be used before serialize this object in JSON. Returns: Dict[str, Any]: A dictionary that represents the Cluster object. \"\"\" # Define the result dictionary. results : Dict [ str , Any ] = {} # Add clustering information. results [ \"cluster_ID\" ] = self . cluster_ID results [ \"clustering_iteration\" ] = self . clustering_iteration # Add children information. results [ \"children\" ] = [ child . to_dict () for child in self . children ] results [ \"cluster_inverse_depth\" ] = self . cluster_inverse_depth # Add members information. results [ \"members\" ] = self . members return results update_centroid ( self ) \u00b6 Update centroid of the cluster. Source code in interactive_clustering\\clustering\\hierarchical.py def update_centroid ( self ) -> None : \"\"\" Update centroid of the cluster. \"\"\" # Update centroids. self . centroid : Union [ ndarray , csr_matrix ] = ( sum ([ self . vectors [ data_ID ] for data_ID in self . members ]) / self . get_cluster_size () ) HierarchicalConstrainedClustering ( AbstractConstrainedClustering ) \u00b6 This class implements the hierarchical constrained clustering. It inherits from AbstractConstrainedClustering . References Hierarchical Clustering: Murtagh, F. et P. Contreras (2012). Algorithms for hierarchical clustering : An overview. Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery 2, 86\u201397. Constrained Hierarchical Clustering: Davidson, I. et S. S. Ravi (2005). Agglomerative Hierarchical Clustering with Constraints : Theoretical and Empirical Results. Springer, Berlin, Heidelberg 3721, 12. Examples: # Import. from scipy.sparse import csr_matrix from cognitivefactory.interactive_clustering.clustering.hierarchical import HierarchicalConstrainedClustering # Create an instance of hierarchical clustering. clustering_model = HierarchicalConstrainedClustering ( linkage = \"ward\" , random_seed = 2 , ) # Define vectors. # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts. vectors = { \"0\" : csr_matrix ([ 1.00 , 0.00 , 0.00 ]), \"1\" : csr_matrix ([ 0.95 , 0.02 , 0.01 ]), \"2\" : csr_matrix ([ 0.98 , 0.00 , 0.00 ]), \"3\" : csr_matrix ([ 0.99 , 0.00 , 0.00 ]), \"4\" : csr_matrix ([ 0.01 , 0.99 , 0.07 ]), \"5\" : csr_matrix ([ 0.02 , 0.99 , 0.07 ]), \"6\" : csr_matrix ([ 0.01 , 0.99 , 0.02 ]), \"7\" : csr_matrix ([ 0.01 , 0.01 , 0.97 ]), \"8\" : csr_matrix ([ 0.00 , 0.01 , 0.99 ]), \"9\" : csr_matrix ([ 0.00 , 0.00 , 1.00 ]), } # Define constraints manager. constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = list ( vectors . keys ())) # Run clustering. dict_of_predicted_clusters = clustering_model ( constraints_manager = constraints_manager , vectors = vectors , nb_clusters = 3 , ) # Print results. print ( \"Expected results\" , \";\" , { \"0\" : 0 , \"1\" : 0 , \"2\" : 0 , \"3\" : 0 , \"4\" : 1 , \"5\" : 1 , \"6\" : 1 , \"7\" : 2 , \"8\" : 2 , \"9\" : 2 ,}) print ( \"Computed results\" , \":\" , dict_of_predicted_clusters ) __init__ ( self , linkage = 'ward' , random_seed = None , ** kargs ) special \u00b6 The constructor for Hierarchical Constrainted Clustering class. Parameters: Name Type Description Default linkage str The metric used to merge clusters. Several type are implemented : - \"ward\" : Merge the two clusters for which the merged cluster from these clusters have the lowest intra-class distance. - \"average\" : Merge the two clusters that have the closest barycenters. - \"complete\" : Merge the two clusters for which the maximum distance between two data of these clusters is the lowest. - \"single\" : Merge the two clusters for which the minimum distance between two data of these clusters is the lowest. Defaults to \"ward\" . 'ward' random_seed Optional[int] The random seed to use to redo the same clustering. Defaults to None . None **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if some parameters are incorrectly set. Source code in interactive_clustering\\clustering\\hierarchical.py def __init__ ( self , linkage : str = \"ward\" , random_seed : Optional [ int ] = None , ** kargs ) -> None : \"\"\" The constructor for Hierarchical Constrainted Clustering class. Args: linkage (str, optional): The metric used to merge clusters. Several type are implemented : - `\"ward\"`: Merge the two clusters for which the merged cluster from these clusters have the lowest intra-class distance. - `\"average\"`: Merge the two clusters that have the closest barycenters. - `\"complete\"`: Merge the two clusters for which the maximum distance between two data of these clusters is the lowest. - `\"single\"`: Merge the two clusters for which the minimum distance between two data of these clusters is the lowest. Defaults to `\"ward\"`. random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if some parameters are incorrectly set. \"\"\" # Store `self.linkage`. if linkage not in { \"ward\" , \"average\" , \"complete\" , \"single\" }: raise ValueError ( \"The `linkage` '\" + str ( linkage ) + \"' is not implemented.\" ) self . linkage : str = linkage # Store `self.random_seed` self . random_seed : Optional [ int ] = random_seed # Store `self.kargs` for hierarchical clustering. self . kargs = kargs # Initialize `self.clustering_root` and `self.dict_of_predicted_clusters`. self . clustering_root : Optional [ Cluster ] = None self . dict_of_predicted_clusters : Optional [ Dict [ str , int ]] = None cluster ( self , constraints_manager , vectors , nb_clusters , verbose = False , ** kargs ) \u00b6 The main method used to cluster data with the Hierarchical model. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs that will force clustering to respect some conditions during computation. required vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required nb_clusters int The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? required verbose bool Enable verbose output. Defaults to False . False **kargs dict Other parameters that can be used in the clustering. {} Exceptions: Type Description ValueError If some parameters are incorrectly set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\hierarchical.py def cluster ( self , constraints_manager : AbstractConstraintsManager , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], nb_clusters : int , verbose : bool = False , ** kargs , ) -> Dict [ str , int ]: \"\"\" The main method used to cluster data with the Hierarchical model. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation. vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). nb_clusters (int): The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? verbose (bool, optional): Enable verbose output. Defaults to `False`. **kargs (dict): Other parameters that can be used in the clustering. Raises: ValueError: If some parameters are incorrectly set. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" ### ### GET PARAMETERS ### # Store `self.constraints_manager` and `self.list_of_data_IDs`. if not isinstance ( constraints_manager , AbstractConstraintsManager ): raise ValueError ( \"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\" ) self . constraints_manager : AbstractConstraintsManager = constraints_manager self . list_of_data_IDs : List [ str ] = self . constraints_manager . get_list_of_managed_data_IDs () # Store `self.vectors`. if not isinstance ( vectors , dict ): raise ValueError ( \"The `vectors` parameter has to be a `dict` type.\" ) self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors # Store `self.nb_clusters`. if nb_clusters < 2 : raise ValueError ( \"The `nb_clusters` '\" + str ( nb_clusters ) + \"' must be greater than or equal to 2.\" ) self . nb_clusters : int = nb_clusters # Store `self.pairwise_distances_matrix`. self . pairwise_distances_matrix : Dict [ str , Dict [ str , float ]] = { data_ID1 : { data_ID2 : pairwise_distances ( X = self . vectors [ data_ID1 ], Y = self . vectors [ data_ID2 ], metric = \"euclidean\" ,)[ 0 ][ 0 ] . astype ( np . float64 ) for data_ID2 in self . list_of_data_IDs } for data_ID1 in self . list_of_data_IDs } # TODO need ? ### ### INITIALIZE HIERARCHICAL CONSTRAINED CLUSTERING ### # Verbose if verbose : # pragma: no cover # Verbose - Print progression status. TIME_start : datetime = datetime . now () print ( \" \" , \"CLUSTERING_ITERATION=\" + \"INITIALIZATION\" , \"(current_time = \" + str ( TIME_start - TIME_start ) . split ( \".\" )[ 0 ] + \")\" , ) # Initialize `self.clustering_root` and `self.dict_of_predicted_clusters`. self . clustering_root = None self . dict_of_predicted_clusters = None # Initialize iteration counter. self . clustering_iteration : int = 0 # Initialize `current_clusters` and `self.clusters_storage`. self . current_clusters : List [ int ] = [] self . clusters_storage : Dict [ int , Cluster ] = {} # Get the list of possibles lists of MUST_LINK data for initialization. list_of_possible_lists_of_MUST_LINK_data : List [ List [ str ]] = self . constraints_manager . get_connected_components () # Estimation of max number of iteration. max_clustering_iteration : int = len ( list_of_possible_lists_of_MUST_LINK_data ) - 1 # For each list of same data (MUST_LINK constraints). for MUST_LINK_data in list_of_possible_lists_of_MUST_LINK_data : # Create a initial cluster with data that MUST be LINKed. self . _add_new_cluster_by_setting_members ( members = MUST_LINK_data , ) # Initialize distance between clusters. self . clusters_distance : Dict [ int , Dict [ int , float ]] = {} for cluster_IDi in self . current_clusters : for cluster_IDj in self . current_clusters : if cluster_IDi < cluster_IDj : # Compute distance between cluster i and cluster j. distance : float = self . _compute_distance ( cluster_IDi = cluster_IDi , cluster_IDj = cluster_IDj ) # Store distance between cluster i and cluster j. self . _set_distance ( cluster_IDi = cluster_IDi , cluster_IDj = cluster_IDj , distance = distance ) # Initialize iterations at first iteration. self . clustering_iteration = 1 ### ### RUN ITERATIONS OF HIERARCHICAL CONSTRAINED CLUSTERING UNTIL CONVERGENCE ### # Iter until convergence of clustering. while len ( self . current_clusters ) > 1 : # Verbose if verbose : # pragma: no cover # Verbose - Print progression status. TIME_current : datetime = datetime . now () print ( \" \" , \"CLUSTERING_ITERATION=\" + str ( self . clustering_iteration ) . zfill ( 6 ) + \"/\" + str ( max_clustering_iteration ) . zfill ( 6 ), \"(current_time = \" + str ( TIME_current - TIME_start ) . split ( \".\" )[ 0 ] + \")\" , end = \" \\r \" , ) # Get clostest clusters to merge clostest_clusters : Optional [ Tuple [ int , int ]] = self . _get_the_two_clostest_clusters () # If no clusters to merge, then stop iterations. if clostest_clusters is None : break # Merge clusters the two closest clusters and add the merged cluster to the storage. # If merge one cluster \"node\" with a cluster \"leaf\" : add the cluster \"leaf\" to the children of the cluster \"node\". # If merge two clusters \"nodes\" or two clusters \"leaves\" : create a new cluster \"node\". merged_cluster_ID : int = self . _add_new_cluster_by_merging_clusters ( children = [ clostest_clusters [ 0 ], clostest_clusters [ 1 ], ] ) # Update distances for cluster_ID in self . current_clusters : if cluster_ID != merged_cluster_ID : # Compute distance between cluster and merged cluster. distance = self . _compute_distance ( cluster_IDi = cluster_ID , cluster_IDj = merged_cluster_ID ) # Store distance between cluster and merged cluster. self . _set_distance ( cluster_IDi = cluster_ID , cluster_IDj = merged_cluster_ID , distance = distance ) # Update self.clustering_iteration. self . clustering_iteration += 1 ### ### END HIERARCHICAL CONSTRAINED CLUSTERING ### # Verbose if verbose : # pragma: no cover # Verbose - Print progression status. TIME_current = datetime . now () # Case of clustering not completed. if len ( self . current_clusters ) > 1 : print ( \" \" , \"CLUSTERING_ITERATION=\" + str ( self . clustering_iteration ) . zfill ( 5 ), \"-\" , \"End : No more cluster to merge\" , \"(current_time = \" + str ( TIME_current - TIME_start ) . split ( \".\" )[ 0 ] + \")\" , ) else : print ( \" \" , \"CLUSTERING_ITERATION=\" + str ( self . clustering_iteration ) . zfill ( 5 ), \"-\" , \"End : Full clustering done\" , \"(current_time = \" + str ( TIME_current - TIME_start ) . split ( \".\" )[ 0 ] + \")\" , ) # If several clusters remains, then merge them in a cluster root. if len ( self . current_clusters ) > 1 : # Merge all remaining clusters. # If merge one cluster \"node\" with many cluster \"leaves\" : add clusters \"leaves\" to the children of the cluster \"node\". # If merge many clusters \"nodes\" and/or many clusters \"leaves\" : create a new cluster \"node\". self . _add_new_cluster_by_merging_clusters ( children = self . current_clusters . copy ()) # Get clustering root. root_ID : int = self . current_clusters [ 0 ] self . clustering_root = self . clusters_storage [ root_ID ] ### ### GET PREDICTED CLUSTERS ### # Compute predicted clusters. self . dict_of_predicted_clusters = self . compute_predicted_clusters ( nb_clusters = self . nb_clusters , ) return self . dict_of_predicted_clusters compute_predicted_clusters ( self , nb_clusters , by = 'size' ) \u00b6 Compute the predicted clusters based on clustering tree and estimation of number of clusters. Parameters: Name Type Description Default nb_clusters int The number of clusters to compute. required by str A string to identifies the criteria used to explore HierarchicalConstrainedClustering tree. Can be \"size\" or \"iteration\" . Defaults to \"size\" . 'size' Exceptions: Type Description ValueError if clustering_root was not set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\hierarchical.py def compute_predicted_clusters ( self , nb_clusters : int , by : str = \"size\" ) -> Dict [ str , int ]: \"\"\" Compute the predicted clusters based on clustering tree and estimation of number of clusters. Args: nb_clusters (int): The number of clusters to compute. by (str, optional): A string to identifies the criteria used to explore `HierarchicalConstrainedClustering` tree. Can be `\"size\"` or `\"iteration\"`. Defaults to `\"size\"`. Raises: ValueError: if `clustering_root` was not set. Returns: Dict[str,int] : A dictionary that contains the predicted cluster for each data ID. \"\"\" # Check that the clustering has been made. if self . clustering_root is None : raise ValueError ( \"The `clustering_root` is not set, probably because clustering was not run.\" ) ### ### EXPLORE CLUSTER TREE ### # Define the resulted list of children as the children of `HierarchicalConstrainedClustering` root. list_of_clusters : List [ Cluster ] = [ self . clustering_root ] # Explore `HierarchicalConstrainedClustering` children until dict_of_predicted_clusters has the right number of children. while len ( list_of_clusters ) < nb_clusters : if by == \"size\" : # Get the biggest cluster in current children from `HierarchicalConstrainedClustering` exploration. # i.e. it's the cluster that has the more data to split. cluster_to_split = max ( list_of_clusters , key = lambda c : len ( c . members )) else : # if by == \"iteration\": # Get the most recent cluster in current children from `HierarchicalConstrainedClustering` exploration. # i.e. it's the cluster that was last merged. cluster_to_split = max ( list_of_clusters , key = lambda c : c . clustering_iteration ) # If the chosen cluster is a leaf : break the `HierarchicalConstrainedClustering` exploration. if cluster_to_split . children == []: # noqa: WPS520 break # Otherwise: The chosen cluster is a node, so split it and get its children. else : # ... remove the cluster obtained ... list_of_clusters . remove ( cluster_to_split ) # ... and add all its children. for child in cluster_to_split . children : list_of_clusters . append ( child ) ### ### GET PREDICTED CLUSTERS ### # Initialize the dictionary of predicted clusters. predicted_clusters : Dict [ str , int ] = { data_ID : - 1 for data_ID in self . list_of_data_IDs } # For all cluster... for cluster in list_of_clusters : # ... and for all member in each cluster... for data_ID in cluster . members : # ... affect the predicted cluster (cluster ID) to the data. predicted_clusters [ data_ID ] = cluster . cluster_ID # Rename cluster IDs by order. predicted_clusters = rename_clusters_by_order ( clusters = predicted_clusters ) # Return predicted clusters return predicted_clusters","title":"clustering.hierarchical"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster","text":"This class represents a cluster as a node of the hierarchical clustering tree.","title":"Cluster"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.__init__","text":"The constructor for Cluster class. Parameters: Name Type Description Default vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager (if constraints_manager is set). The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required cluster_ID int The cluster ID that is defined during HierarchicalConstrainedClustering.cluster running. required clustering_iteration int The cluster iteration that is defined during HierarchicalConstrainedClustering.cluster running. required children Optional[List[\"Cluster\"]] A list of clusters children for cluster initialization. Incompatible with members parameter. Defaults to None . None members Optional[List[str]] A list of data IDs for cluster initialization. Incompatible with children parameter. Defaults to None . None Exceptions: Type Description ValueError if children and members are both set or both unset. Source code in interactive_clustering\\clustering\\hierarchical.py def __init__ ( self , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], cluster_ID : int , clustering_iteration : int , children : Optional [ List [ \"Cluster\" ]] = None , members : Optional [ List [ str ]] = None , ) -> None : \"\"\" The constructor for Cluster class. Args: vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager` (if `constraints_manager` is set). The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). cluster_ID (int): The cluster ID that is defined during `HierarchicalConstrainedClustering.cluster` running. clustering_iteration (int): The cluster iteration that is defined during `HierarchicalConstrainedClustering.cluster` running. children (Optional[List[\"Cluster\"]], optional): A list of clusters children for cluster initialization. Incompatible with `members` parameter. Defaults to `None`. members (Optional[List[str]], optional): A list of data IDs for cluster initialization. Incompatible with `children` parameter. Defaults to `None`. Raises: ValueError: if `children` and `members` are both set or both unset. \"\"\" # Store links to `vectors`. self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors # Cluster ID and Clustering iteration. self . cluster_ID : int = cluster_ID self . clustering_iteration : int = clustering_iteration # Check children and members. if (( children is not None ) and ( members is not None )) or (( children is None ) and ( members is None )): raise ValueError ( \"Cluster initialization must be by `children` setting or by `members` setting, but not by both or none of them.\" ) # Add children (empty or not). self . children : List [ \"Cluster\" ] = children if ( children is not None ) else [] # Cluster inverse depth. self . cluster_inverse_depth : int = ( max ([ child . cluster_inverse_depth for child in self . children ]) + 1 if ( self . children ) else 0 ) # Add members (empty or not). self . members : List [ str ] = ( members if members is not None else [ data_ID for child in self . children for data_ID in child . members ] ) # Update centroids self . update_centroid ()","title":"__init__()"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.add_new_children","text":"Add new children to the cluster. Parameters: Name Type Description Default new_children List[\"Cluster\"] The list of new clusters children to add. required new_clustering_iteration int The new cluster iteration that is defined during HierarchicalConstrainedClustering.clusterize running. required Source code in interactive_clustering\\clustering\\hierarchical.py def add_new_children ( self , new_children : List [ \"Cluster\" ], new_clustering_iteration : int , ) -> None : \"\"\" Add new children to the cluster. Args: new_children (List[\"Cluster\"]): The list of new clusters children to add. new_clustering_iteration (int): The new cluster iteration that is defined during HierarchicalConstrainedClustering.clusterize running. \"\"\" # Update clustering iteration. self . clustering_iteration = new_clustering_iteration # Update children. self . children += [ new_child for new_child in new_children if new_child not in self . children ] # Update cluster inverse depth. self . cluster_inverse_depth = max ([ child . cluster_inverse_depth for child in self . children ]) + 1 # Update members. self . members = [ data_ID for child in self . children for data_ID in child . members ] # Update centroids. self . update_centroid ()","title":"add_new_children()"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.get_cluster_size","text":"Get cluster size. Returns: Type Description int The cluster size, i.e. the number of members in the cluster. Source code in interactive_clustering\\clustering\\hierarchical.py def get_cluster_size ( self ) -> int : \"\"\" Get cluster size. Returns: int: The cluster size, i.e. the number of members in the cluster. \"\"\" # Update centroids. return len ( self . members )","title":"get_cluster_size()"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.to_dict","text":"Transform the Cluster object into a dictionary. It can be used before serialize this object in JSON. Returns: Type Description Dict[str, Any] A dictionary that represents the Cluster object. Source code in interactive_clustering\\clustering\\hierarchical.py def to_dict ( self ) -> Dict [ str , Any ]: \"\"\" Transform the Cluster object into a dictionary. It can be used before serialize this object in JSON. Returns: Dict[str, Any]: A dictionary that represents the Cluster object. \"\"\" # Define the result dictionary. results : Dict [ str , Any ] = {} # Add clustering information. results [ \"cluster_ID\" ] = self . cluster_ID results [ \"clustering_iteration\" ] = self . clustering_iteration # Add children information. results [ \"children\" ] = [ child . to_dict () for child in self . children ] results [ \"cluster_inverse_depth\" ] = self . cluster_inverse_depth # Add members information. results [ \"members\" ] = self . members return results","title":"to_dict()"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.Cluster.update_centroid","text":"Update centroid of the cluster. Source code in interactive_clustering\\clustering\\hierarchical.py def update_centroid ( self ) -> None : \"\"\" Update centroid of the cluster. \"\"\" # Update centroids. self . centroid : Union [ ndarray , csr_matrix ] = ( sum ([ self . vectors [ data_ID ] for data_ID in self . members ]) / self . get_cluster_size () )","title":"update_centroid()"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.HierarchicalConstrainedClustering","text":"This class implements the hierarchical constrained clustering. It inherits from AbstractConstrainedClustering . References Hierarchical Clustering: Murtagh, F. et P. Contreras (2012). Algorithms for hierarchical clustering : An overview. Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery 2, 86\u201397. Constrained Hierarchical Clustering: Davidson, I. et S. S. Ravi (2005). Agglomerative Hierarchical Clustering with Constraints : Theoretical and Empirical Results. Springer, Berlin, Heidelberg 3721, 12. Examples: # Import. from scipy.sparse import csr_matrix from cognitivefactory.interactive_clustering.clustering.hierarchical import HierarchicalConstrainedClustering # Create an instance of hierarchical clustering. clustering_model = HierarchicalConstrainedClustering ( linkage = \"ward\" , random_seed = 2 , ) # Define vectors. # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts. vectors = { \"0\" : csr_matrix ([ 1.00 , 0.00 , 0.00 ]), \"1\" : csr_matrix ([ 0.95 , 0.02 , 0.01 ]), \"2\" : csr_matrix ([ 0.98 , 0.00 , 0.00 ]), \"3\" : csr_matrix ([ 0.99 , 0.00 , 0.00 ]), \"4\" : csr_matrix ([ 0.01 , 0.99 , 0.07 ]), \"5\" : csr_matrix ([ 0.02 , 0.99 , 0.07 ]), \"6\" : csr_matrix ([ 0.01 , 0.99 , 0.02 ]), \"7\" : csr_matrix ([ 0.01 , 0.01 , 0.97 ]), \"8\" : csr_matrix ([ 0.00 , 0.01 , 0.99 ]), \"9\" : csr_matrix ([ 0.00 , 0.00 , 1.00 ]), } # Define constraints manager. constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = list ( vectors . keys ())) # Run clustering. dict_of_predicted_clusters = clustering_model ( constraints_manager = constraints_manager , vectors = vectors , nb_clusters = 3 , ) # Print results. print ( \"Expected results\" , \";\" , { \"0\" : 0 , \"1\" : 0 , \"2\" : 0 , \"3\" : 0 , \"4\" : 1 , \"5\" : 1 , \"6\" : 1 , \"7\" : 2 , \"8\" : 2 , \"9\" : 2 ,}) print ( \"Computed results\" , \":\" , dict_of_predicted_clusters )","title":"HierarchicalConstrainedClustering"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.HierarchicalConstrainedClustering.__init__","text":"The constructor for Hierarchical Constrainted Clustering class. Parameters: Name Type Description Default linkage str The metric used to merge clusters. Several type are implemented : - \"ward\" : Merge the two clusters for which the merged cluster from these clusters have the lowest intra-class distance. - \"average\" : Merge the two clusters that have the closest barycenters. - \"complete\" : Merge the two clusters for which the maximum distance between two data of these clusters is the lowest. - \"single\" : Merge the two clusters for which the minimum distance between two data of these clusters is the lowest. Defaults to \"ward\" . 'ward' random_seed Optional[int] The random seed to use to redo the same clustering. Defaults to None . None **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if some parameters are incorrectly set. Source code in interactive_clustering\\clustering\\hierarchical.py def __init__ ( self , linkage : str = \"ward\" , random_seed : Optional [ int ] = None , ** kargs ) -> None : \"\"\" The constructor for Hierarchical Constrainted Clustering class. Args: linkage (str, optional): The metric used to merge clusters. Several type are implemented : - `\"ward\"`: Merge the two clusters for which the merged cluster from these clusters have the lowest intra-class distance. - `\"average\"`: Merge the two clusters that have the closest barycenters. - `\"complete\"`: Merge the two clusters for which the maximum distance between two data of these clusters is the lowest. - `\"single\"`: Merge the two clusters for which the minimum distance between two data of these clusters is the lowest. Defaults to `\"ward\"`. random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if some parameters are incorrectly set. \"\"\" # Store `self.linkage`. if linkage not in { \"ward\" , \"average\" , \"complete\" , \"single\" }: raise ValueError ( \"The `linkage` '\" + str ( linkage ) + \"' is not implemented.\" ) self . linkage : str = linkage # Store `self.random_seed` self . random_seed : Optional [ int ] = random_seed # Store `self.kargs` for hierarchical clustering. self . kargs = kargs # Initialize `self.clustering_root` and `self.dict_of_predicted_clusters`. self . clustering_root : Optional [ Cluster ] = None self . dict_of_predicted_clusters : Optional [ Dict [ str , int ]] = None","title":"__init__()"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.HierarchicalConstrainedClustering.cluster","text":"The main method used to cluster data with the Hierarchical model. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs that will force clustering to respect some conditions during computation. required vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required nb_clusters int The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? required verbose bool Enable verbose output. Defaults to False . False **kargs dict Other parameters that can be used in the clustering. {} Exceptions: Type Description ValueError If some parameters are incorrectly set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\hierarchical.py def cluster ( self , constraints_manager : AbstractConstraintsManager , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], nb_clusters : int , verbose : bool = False , ** kargs , ) -> Dict [ str , int ]: \"\"\" The main method used to cluster data with the Hierarchical model. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation. vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). nb_clusters (int): The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? verbose (bool, optional): Enable verbose output. Defaults to `False`. **kargs (dict): Other parameters that can be used in the clustering. Raises: ValueError: If some parameters are incorrectly set. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" ### ### GET PARAMETERS ### # Store `self.constraints_manager` and `self.list_of_data_IDs`. if not isinstance ( constraints_manager , AbstractConstraintsManager ): raise ValueError ( \"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\" ) self . constraints_manager : AbstractConstraintsManager = constraints_manager self . list_of_data_IDs : List [ str ] = self . constraints_manager . get_list_of_managed_data_IDs () # Store `self.vectors`. if not isinstance ( vectors , dict ): raise ValueError ( \"The `vectors` parameter has to be a `dict` type.\" ) self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors # Store `self.nb_clusters`. if nb_clusters < 2 : raise ValueError ( \"The `nb_clusters` '\" + str ( nb_clusters ) + \"' must be greater than or equal to 2.\" ) self . nb_clusters : int = nb_clusters # Store `self.pairwise_distances_matrix`. self . pairwise_distances_matrix : Dict [ str , Dict [ str , float ]] = { data_ID1 : { data_ID2 : pairwise_distances ( X = self . vectors [ data_ID1 ], Y = self . vectors [ data_ID2 ], metric = \"euclidean\" ,)[ 0 ][ 0 ] . astype ( np . float64 ) for data_ID2 in self . list_of_data_IDs } for data_ID1 in self . list_of_data_IDs } # TODO need ? ### ### INITIALIZE HIERARCHICAL CONSTRAINED CLUSTERING ### # Verbose if verbose : # pragma: no cover # Verbose - Print progression status. TIME_start : datetime = datetime . now () print ( \" \" , \"CLUSTERING_ITERATION=\" + \"INITIALIZATION\" , \"(current_time = \" + str ( TIME_start - TIME_start ) . split ( \".\" )[ 0 ] + \")\" , ) # Initialize `self.clustering_root` and `self.dict_of_predicted_clusters`. self . clustering_root = None self . dict_of_predicted_clusters = None # Initialize iteration counter. self . clustering_iteration : int = 0 # Initialize `current_clusters` and `self.clusters_storage`. self . current_clusters : List [ int ] = [] self . clusters_storage : Dict [ int , Cluster ] = {} # Get the list of possibles lists of MUST_LINK data for initialization. list_of_possible_lists_of_MUST_LINK_data : List [ List [ str ]] = self . constraints_manager . get_connected_components () # Estimation of max number of iteration. max_clustering_iteration : int = len ( list_of_possible_lists_of_MUST_LINK_data ) - 1 # For each list of same data (MUST_LINK constraints). for MUST_LINK_data in list_of_possible_lists_of_MUST_LINK_data : # Create a initial cluster with data that MUST be LINKed. self . _add_new_cluster_by_setting_members ( members = MUST_LINK_data , ) # Initialize distance between clusters. self . clusters_distance : Dict [ int , Dict [ int , float ]] = {} for cluster_IDi in self . current_clusters : for cluster_IDj in self . current_clusters : if cluster_IDi < cluster_IDj : # Compute distance between cluster i and cluster j. distance : float = self . _compute_distance ( cluster_IDi = cluster_IDi , cluster_IDj = cluster_IDj ) # Store distance between cluster i and cluster j. self . _set_distance ( cluster_IDi = cluster_IDi , cluster_IDj = cluster_IDj , distance = distance ) # Initialize iterations at first iteration. self . clustering_iteration = 1 ### ### RUN ITERATIONS OF HIERARCHICAL CONSTRAINED CLUSTERING UNTIL CONVERGENCE ### # Iter until convergence of clustering. while len ( self . current_clusters ) > 1 : # Verbose if verbose : # pragma: no cover # Verbose - Print progression status. TIME_current : datetime = datetime . now () print ( \" \" , \"CLUSTERING_ITERATION=\" + str ( self . clustering_iteration ) . zfill ( 6 ) + \"/\" + str ( max_clustering_iteration ) . zfill ( 6 ), \"(current_time = \" + str ( TIME_current - TIME_start ) . split ( \".\" )[ 0 ] + \")\" , end = \" \\r \" , ) # Get clostest clusters to merge clostest_clusters : Optional [ Tuple [ int , int ]] = self . _get_the_two_clostest_clusters () # If no clusters to merge, then stop iterations. if clostest_clusters is None : break # Merge clusters the two closest clusters and add the merged cluster to the storage. # If merge one cluster \"node\" with a cluster \"leaf\" : add the cluster \"leaf\" to the children of the cluster \"node\". # If merge two clusters \"nodes\" or two clusters \"leaves\" : create a new cluster \"node\". merged_cluster_ID : int = self . _add_new_cluster_by_merging_clusters ( children = [ clostest_clusters [ 0 ], clostest_clusters [ 1 ], ] ) # Update distances for cluster_ID in self . current_clusters : if cluster_ID != merged_cluster_ID : # Compute distance between cluster and merged cluster. distance = self . _compute_distance ( cluster_IDi = cluster_ID , cluster_IDj = merged_cluster_ID ) # Store distance between cluster and merged cluster. self . _set_distance ( cluster_IDi = cluster_ID , cluster_IDj = merged_cluster_ID , distance = distance ) # Update self.clustering_iteration. self . clustering_iteration += 1 ### ### END HIERARCHICAL CONSTRAINED CLUSTERING ### # Verbose if verbose : # pragma: no cover # Verbose - Print progression status. TIME_current = datetime . now () # Case of clustering not completed. if len ( self . current_clusters ) > 1 : print ( \" \" , \"CLUSTERING_ITERATION=\" + str ( self . clustering_iteration ) . zfill ( 5 ), \"-\" , \"End : No more cluster to merge\" , \"(current_time = \" + str ( TIME_current - TIME_start ) . split ( \".\" )[ 0 ] + \")\" , ) else : print ( \" \" , \"CLUSTERING_ITERATION=\" + str ( self . clustering_iteration ) . zfill ( 5 ), \"-\" , \"End : Full clustering done\" , \"(current_time = \" + str ( TIME_current - TIME_start ) . split ( \".\" )[ 0 ] + \")\" , ) # If several clusters remains, then merge them in a cluster root. if len ( self . current_clusters ) > 1 : # Merge all remaining clusters. # If merge one cluster \"node\" with many cluster \"leaves\" : add clusters \"leaves\" to the children of the cluster \"node\". # If merge many clusters \"nodes\" and/or many clusters \"leaves\" : create a new cluster \"node\". self . _add_new_cluster_by_merging_clusters ( children = self . current_clusters . copy ()) # Get clustering root. root_ID : int = self . current_clusters [ 0 ] self . clustering_root = self . clusters_storage [ root_ID ] ### ### GET PREDICTED CLUSTERS ### # Compute predicted clusters. self . dict_of_predicted_clusters = self . compute_predicted_clusters ( nb_clusters = self . nb_clusters , ) return self . dict_of_predicted_clusters","title":"cluster()"},{"location":"reference/clustering/hierarchical/#cognitivefactory.interactive_clustering.clustering.hierarchical.HierarchicalConstrainedClustering.compute_predicted_clusters","text":"Compute the predicted clusters based on clustering tree and estimation of number of clusters. Parameters: Name Type Description Default nb_clusters int The number of clusters to compute. required by str A string to identifies the criteria used to explore HierarchicalConstrainedClustering tree. Can be \"size\" or \"iteration\" . Defaults to \"size\" . 'size' Exceptions: Type Description ValueError if clustering_root was not set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\hierarchical.py def compute_predicted_clusters ( self , nb_clusters : int , by : str = \"size\" ) -> Dict [ str , int ]: \"\"\" Compute the predicted clusters based on clustering tree and estimation of number of clusters. Args: nb_clusters (int): The number of clusters to compute. by (str, optional): A string to identifies the criteria used to explore `HierarchicalConstrainedClustering` tree. Can be `\"size\"` or `\"iteration\"`. Defaults to `\"size\"`. Raises: ValueError: if `clustering_root` was not set. Returns: Dict[str,int] : A dictionary that contains the predicted cluster for each data ID. \"\"\" # Check that the clustering has been made. if self . clustering_root is None : raise ValueError ( \"The `clustering_root` is not set, probably because clustering was not run.\" ) ### ### EXPLORE CLUSTER TREE ### # Define the resulted list of children as the children of `HierarchicalConstrainedClustering` root. list_of_clusters : List [ Cluster ] = [ self . clustering_root ] # Explore `HierarchicalConstrainedClustering` children until dict_of_predicted_clusters has the right number of children. while len ( list_of_clusters ) < nb_clusters : if by == \"size\" : # Get the biggest cluster in current children from `HierarchicalConstrainedClustering` exploration. # i.e. it's the cluster that has the more data to split. cluster_to_split = max ( list_of_clusters , key = lambda c : len ( c . members )) else : # if by == \"iteration\": # Get the most recent cluster in current children from `HierarchicalConstrainedClustering` exploration. # i.e. it's the cluster that was last merged. cluster_to_split = max ( list_of_clusters , key = lambda c : c . clustering_iteration ) # If the chosen cluster is a leaf : break the `HierarchicalConstrainedClustering` exploration. if cluster_to_split . children == []: # noqa: WPS520 break # Otherwise: The chosen cluster is a node, so split it and get its children. else : # ... remove the cluster obtained ... list_of_clusters . remove ( cluster_to_split ) # ... and add all its children. for child in cluster_to_split . children : list_of_clusters . append ( child ) ### ### GET PREDICTED CLUSTERS ### # Initialize the dictionary of predicted clusters. predicted_clusters : Dict [ str , int ] = { data_ID : - 1 for data_ID in self . list_of_data_IDs } # For all cluster... for cluster in list_of_clusters : # ... and for all member in each cluster... for data_ID in cluster . members : # ... affect the predicted cluster (cluster ID) to the data. predicted_clusters [ data_ID ] = cluster . cluster_ID # Rename cluster IDs by order. predicted_clusters = rename_clusters_by_order ( clusters = predicted_clusters ) # Return predicted clusters return predicted_clusters","title":"compute_predicted_clusters()"},{"location":"reference/clustering/kmeans/","text":"Name: cognitivefactory.interactive_clustering.clustering.kmeans Description: Implementation of constrained kmeans clustering algorithms. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL-C License v1.0 ( https://cecill.info/licences.fr.html ) KMeansConstrainedClustering ( AbstractConstrainedClustering ) \u00b6 This class implements the kmeans constrained clustering. It inherits from AbstractConstrainedClustering . References KMeans Clustering: MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297. Constrained 'COP' KMeans Clustering: Wagstaff, K., C. Cardie, S. Rogers, et S. Schroedl (2001). Constrained K-means Clustering with Background Knowledge. International Conference on Machine Learning Examples: # Import. from scipy.sparse import csr_matrix from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager from cognitivefactory.interactive_clustering.clustering.kmeans import KMeansConstrainedClustering # Create an instance of kmeans clustering. clustering_model = KMeansConstrainedClustering ( model = \"COP\" , random_seed = 2 , ) # Define vectors. # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts. vectors = { \"0\" : csr_matrix ([ 1.00 , 0.00 , 0.00 , 0.00 ]), \"1\" : csr_matrix ([ 0.95 , 0.02 , 0.02 , 0.01 ]), \"2\" : csr_matrix ([ 0.98 , 0.00 , 0.02 , 0.00 ]), \"3\" : csr_matrix ([ 0.99 , 0.00 , 0.01 , 0.00 ]), \"4\" : csr_matrix ([ 0.50 , 0.22 , 0.21 , 0.07 ]), \"5\" : csr_matrix ([ 0.50 , 0.21 , 0.22 , 0.07 ]), \"6\" : csr_matrix ([ 0.01 , 0.01 , 0.01 , 0.97 ]), \"7\" : csr_matrix ([ 0.00 , 0.01 , 0.00 , 0.99 ]), \"8\" : csr_matrix ([ 0.00 , 0.00 , 0.00 , 1.00 ]), } # Define constraints manager. constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = list ( vectors . keys ())) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"1\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"7\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"8\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"4\" , data_ID2 = \"5\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"4\" , constraint_type = \"CANNOT_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"2\" , data_ID2 = \"4\" , constraint_type = \"CANNOT_LINK\" ) # Run clustering. dict_of_predicted_clusters = clustering_model ( constraints_manager = constraints_manager , vectors = vectors , nb_clusters = 3 , ) # Print results. print ( \"Expected results\" , \";\" , { \"0\" : 0 , \"1\" : 0 , \"2\" : 1 , \"3\" : 1 , \"4\" : 2 , \"5\" : 2 , \"6\" : 0 , \"7\" : 0 , \"8\" : 0 ,}) print ( \"Computed results\" , \":\" , dict_of_predicted_clusters ) __init__ ( self , model = 'COP' , max_iteration = 150 , tolerance = 0.0001 , random_seed = None , ** kargs ) special \u00b6 The constructor for KMeans Constrainted Clustering class. Parameters: Name Type Description Default model str The kmeans clustering model to use. Available kmeans models are \"COP\" . Defaults to \"COP\" . 'COP' max_iteration int The maximum number of kmeans iteration for convergence. Defaults to 150 . 150 tolerance float The tolerance for convergence computation. Defaults to 1e-4 . 0.0001 random_seed Optional[int] The random seed to use to redo the same clustering. Defaults to None . None **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if some parameters are incorrectly set. Source code in interactive_clustering\\clustering\\kmeans.py def __init__ ( self , model : str = \"COP\" , max_iteration : int = 150 , tolerance : float = 1e-4 , random_seed : Optional [ int ] = None , ** kargs , ) -> None : \"\"\" The constructor for KMeans Constrainted Clustering class. Args: model (str, optional): The kmeans clustering model to use. Available kmeans models are `\"COP\"`. Defaults to `\"COP\"`. max_iteration (int, optional): The maximum number of kmeans iteration for convergence. Defaults to `150`. tolerance (float, optional): The tolerance for convergence computation. Defaults to `1e-4`. random_seed (Optional[int]): The random seed to use to redo the same clustering. Defaults to `None`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if some parameters are incorrectly set. \"\"\" # Store `self.`model`. if model != \"COP\" : # TODO use `not in {\"COP\"}`. raise ValueError ( \"The `model` '\" + str ( model ) + \"' is not implemented.\" ) self . model : str = model # Store 'self.max_iteration`. if max_iteration < 1 : raise ValueError ( \"The `max_iteration` must be greater than or equal to 1.\" ) self . max_iteration : int = max_iteration # Store `self.tolerance`. if tolerance < 0 : raise ValueError ( \"The `tolerance` must be greater than 0.0.\" ) self . tolerance : float = tolerance # Store `self.random_seed`. self . random_seed : Optional [ int ] = random_seed # Store `self.kargs` for kmeans clustering. self . kargs = kargs # Initialize `self.dict_of_predicted_clusters`. self . dict_of_predicted_clusters : Optional [ Dict [ str , int ]] = None cluster ( self , constraints_manager , vectors , nb_clusters , verbose = False , ** kargs ) \u00b6 The main method used to cluster data with the KMeans model. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs that will force clustering to respect some conditions during computation. required vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required nb_clusters int The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? required verbose bool Enable verbose output. Defaults to False . False **kargs dict Other parameters that can be used in the clustering. {} Exceptions: Type Description ValueError if vectors and constraints_manager are incompatible, or if some parameters are incorrectly set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\kmeans.py def cluster ( self , constraints_manager : AbstractConstraintsManager , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], nb_clusters : int , verbose : bool = False , ** kargs , ) -> Dict [ str , int ]: \"\"\" The main method used to cluster data with the KMeans model. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation. vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). nb_clusters (int): The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? verbose (bool, optional): Enable verbose output. Defaults to `False`. **kargs (dict): Other parameters that can be used in the clustering. Raises: ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" ### ### GET PARAMETERS ### # Store `self.constraints_manager` and `self.list_of_data_IDs`. if not isinstance ( constraints_manager , AbstractConstraintsManager ): raise ValueError ( \"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\" ) self . constraints_manager : AbstractConstraintsManager = constraints_manager self . list_of_data_IDs : List [ str ] = self . constraints_manager . get_list_of_managed_data_IDs () # Store `self.vectors`. if not isinstance ( vectors , dict ): raise ValueError ( \"The `vectors` parameter has to be a `dict` type.\" ) self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors # Store `self.nb_clusters`. if nb_clusters < 2 : raise ValueError ( \"The `nb_clusters` '\" + str ( nb_clusters ) + \"' must be greater than or equal to 2.\" ) self . nb_clusters : int = nb_clusters ### ### RUN KMEANS CONSTRAINED CLUSTERING ### # Initialize `self.dict_of_predicted_clusters`. self . dict_of_predicted_clusters = None # Case of `\"COP\"` KMeans clustering. ##### DEFAULTS : if self.model==\"COP\": self . dict_of_predicted_clusters = self . _clustering_kmeans_model_COP ( verbose = verbose ) ### ### RETURN PREDICTED CLUSTERS ### return self . dict_of_predicted_clusters compute_centroids ( self , clusters ) \u00b6 Compute the centroids of each cluster. Parameters: Name Type Description Default clusters Dict[str,int] Current clusters assignation. required Returns: Type Description Dict[int, Union[ndarray,csr_matrix]] A dictionary which represent each cluster by a centroid. Source code in interactive_clustering\\clustering\\kmeans.py def compute_centroids ( self , clusters : Dict [ str , int ], ) -> Dict [ int , Union [ ndarray , csr_matrix ]]: \"\"\" Compute the centroids of each cluster. Args: clusters (Dict[str,int]): Current clusters assignation. Returns: Dict[int, Union[ndarray,csr_matrix]]: A dictionary which represent each cluster by a centroid. \"\"\" # Initialize centroids. centroids : Dict [ int , Union [ ndarray , csr_matrix ]] = {} # For all possible cluster ID. for cluster_ID in set ( clusters . values ()): # Compute cluster members. members_of_cluster_ID = [ vector for data_ID , vector in self . vectors . items () if clusters [ data_ID ] == cluster_ID ] # Compute centroid. centroid_for_cluster_ID : Union [ ndarray , csr_matrix ] = sum ( members_of_cluster_ID ) / len ( members_of_cluster_ID ) # Add centroids. centroids [ cluster_ID ] = centroid_for_cluster_ID # Return centroids. return centroids initialize_centroids ( self ) \u00b6 Initialize the centroid of each cluster by a vector. The choice is based on a random selection among data to cluster. Returns: Type Description Dict[int, Union[ndarray,csr_matrix]] A dictionary which represent each cluster by a centroid. Source code in interactive_clustering\\clustering\\kmeans.py def initialize_centroids ( self , ) -> Dict [ int , Union [ ndarray , csr_matrix ]]: \"\"\" Initialize the centroid of each cluster by a vector. The choice is based on a random selection among data to cluster. Returns: Dict[int, Union[ndarray,csr_matrix]]: A dictionary which represent each cluster by a centroid. \"\"\" # Get the list of possible indices. indices : List [ str ] = self . list_of_data_IDs . copy () # Shuffle the list of possible indices. random . seed ( self . random_seed ) random . shuffle ( indices ) # Subset indices. indices = indices [: self . nb_clusters ] # Set initial centroids based on vectors. centroids : Dict [ int , Union [ ndarray , csr_matrix ]] = { cluster_ID : self . vectors [ indices [ cluster_ID ]] for cluster_ID in range ( self . nb_clusters ) } # Return centroids. return centroids","title":"clustering.kmeans"},{"location":"reference/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering","text":"This class implements the kmeans constrained clustering. It inherits from AbstractConstrainedClustering . References KMeans Clustering: MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability 1(14), 281\u2013297. Constrained 'COP' KMeans Clustering: Wagstaff, K., C. Cardie, S. Rogers, et S. Schroedl (2001). Constrained K-means Clustering with Background Knowledge. International Conference on Machine Learning Examples: # Import. from scipy.sparse import csr_matrix from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager from cognitivefactory.interactive_clustering.clustering.kmeans import KMeansConstrainedClustering # Create an instance of kmeans clustering. clustering_model = KMeansConstrainedClustering ( model = \"COP\" , random_seed = 2 , ) # Define vectors. # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts. vectors = { \"0\" : csr_matrix ([ 1.00 , 0.00 , 0.00 , 0.00 ]), \"1\" : csr_matrix ([ 0.95 , 0.02 , 0.02 , 0.01 ]), \"2\" : csr_matrix ([ 0.98 , 0.00 , 0.02 , 0.00 ]), \"3\" : csr_matrix ([ 0.99 , 0.00 , 0.01 , 0.00 ]), \"4\" : csr_matrix ([ 0.50 , 0.22 , 0.21 , 0.07 ]), \"5\" : csr_matrix ([ 0.50 , 0.21 , 0.22 , 0.07 ]), \"6\" : csr_matrix ([ 0.01 , 0.01 , 0.01 , 0.97 ]), \"7\" : csr_matrix ([ 0.00 , 0.01 , 0.00 , 0.99 ]), \"8\" : csr_matrix ([ 0.00 , 0.00 , 0.00 , 1.00 ]), } # Define constraints manager. constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = list ( vectors . keys ())) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"1\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"7\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"8\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"4\" , data_ID2 = \"5\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"4\" , constraint_type = \"CANNOT_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"2\" , data_ID2 = \"4\" , constraint_type = \"CANNOT_LINK\" ) # Run clustering. dict_of_predicted_clusters = clustering_model ( constraints_manager = constraints_manager , vectors = vectors , nb_clusters = 3 , ) # Print results. print ( \"Expected results\" , \";\" , { \"0\" : 0 , \"1\" : 0 , \"2\" : 1 , \"3\" : 1 , \"4\" : 2 , \"5\" : 2 , \"6\" : 0 , \"7\" : 0 , \"8\" : 0 ,}) print ( \"Computed results\" , \":\" , dict_of_predicted_clusters )","title":"KMeansConstrainedClustering"},{"location":"reference/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering.__init__","text":"The constructor for KMeans Constrainted Clustering class. Parameters: Name Type Description Default model str The kmeans clustering model to use. Available kmeans models are \"COP\" . Defaults to \"COP\" . 'COP' max_iteration int The maximum number of kmeans iteration for convergence. Defaults to 150 . 150 tolerance float The tolerance for convergence computation. Defaults to 1e-4 . 0.0001 random_seed Optional[int] The random seed to use to redo the same clustering. Defaults to None . None **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if some parameters are incorrectly set. Source code in interactive_clustering\\clustering\\kmeans.py def __init__ ( self , model : str = \"COP\" , max_iteration : int = 150 , tolerance : float = 1e-4 , random_seed : Optional [ int ] = None , ** kargs , ) -> None : \"\"\" The constructor for KMeans Constrainted Clustering class. Args: model (str, optional): The kmeans clustering model to use. Available kmeans models are `\"COP\"`. Defaults to `\"COP\"`. max_iteration (int, optional): The maximum number of kmeans iteration for convergence. Defaults to `150`. tolerance (float, optional): The tolerance for convergence computation. Defaults to `1e-4`. random_seed (Optional[int]): The random seed to use to redo the same clustering. Defaults to `None`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if some parameters are incorrectly set. \"\"\" # Store `self.`model`. if model != \"COP\" : # TODO use `not in {\"COP\"}`. raise ValueError ( \"The `model` '\" + str ( model ) + \"' is not implemented.\" ) self . model : str = model # Store 'self.max_iteration`. if max_iteration < 1 : raise ValueError ( \"The `max_iteration` must be greater than or equal to 1.\" ) self . max_iteration : int = max_iteration # Store `self.tolerance`. if tolerance < 0 : raise ValueError ( \"The `tolerance` must be greater than 0.0.\" ) self . tolerance : float = tolerance # Store `self.random_seed`. self . random_seed : Optional [ int ] = random_seed # Store `self.kargs` for kmeans clustering. self . kargs = kargs # Initialize `self.dict_of_predicted_clusters`. self . dict_of_predicted_clusters : Optional [ Dict [ str , int ]] = None","title":"__init__()"},{"location":"reference/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering.cluster","text":"The main method used to cluster data with the KMeans model. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs that will force clustering to respect some conditions during computation. required vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required nb_clusters int The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? required verbose bool Enable verbose output. Defaults to False . False **kargs dict Other parameters that can be used in the clustering. {} Exceptions: Type Description ValueError if vectors and constraints_manager are incompatible, or if some parameters are incorrectly set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\kmeans.py def cluster ( self , constraints_manager : AbstractConstraintsManager , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], nb_clusters : int , verbose : bool = False , ** kargs , ) -> Dict [ str , int ]: \"\"\" The main method used to cluster data with the KMeans model. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation. vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). nb_clusters (int): The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? verbose (bool, optional): Enable verbose output. Defaults to `False`. **kargs (dict): Other parameters that can be used in the clustering. Raises: ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" ### ### GET PARAMETERS ### # Store `self.constraints_manager` and `self.list_of_data_IDs`. if not isinstance ( constraints_manager , AbstractConstraintsManager ): raise ValueError ( \"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\" ) self . constraints_manager : AbstractConstraintsManager = constraints_manager self . list_of_data_IDs : List [ str ] = self . constraints_manager . get_list_of_managed_data_IDs () # Store `self.vectors`. if not isinstance ( vectors , dict ): raise ValueError ( \"The `vectors` parameter has to be a `dict` type.\" ) self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors # Store `self.nb_clusters`. if nb_clusters < 2 : raise ValueError ( \"The `nb_clusters` '\" + str ( nb_clusters ) + \"' must be greater than or equal to 2.\" ) self . nb_clusters : int = nb_clusters ### ### RUN KMEANS CONSTRAINED CLUSTERING ### # Initialize `self.dict_of_predicted_clusters`. self . dict_of_predicted_clusters = None # Case of `\"COP\"` KMeans clustering. ##### DEFAULTS : if self.model==\"COP\": self . dict_of_predicted_clusters = self . _clustering_kmeans_model_COP ( verbose = verbose ) ### ### RETURN PREDICTED CLUSTERS ### return self . dict_of_predicted_clusters","title":"cluster()"},{"location":"reference/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering.compute_centroids","text":"Compute the centroids of each cluster. Parameters: Name Type Description Default clusters Dict[str,int] Current clusters assignation. required Returns: Type Description Dict[int, Union[ndarray,csr_matrix]] A dictionary which represent each cluster by a centroid. Source code in interactive_clustering\\clustering\\kmeans.py def compute_centroids ( self , clusters : Dict [ str , int ], ) -> Dict [ int , Union [ ndarray , csr_matrix ]]: \"\"\" Compute the centroids of each cluster. Args: clusters (Dict[str,int]): Current clusters assignation. Returns: Dict[int, Union[ndarray,csr_matrix]]: A dictionary which represent each cluster by a centroid. \"\"\" # Initialize centroids. centroids : Dict [ int , Union [ ndarray , csr_matrix ]] = {} # For all possible cluster ID. for cluster_ID in set ( clusters . values ()): # Compute cluster members. members_of_cluster_ID = [ vector for data_ID , vector in self . vectors . items () if clusters [ data_ID ] == cluster_ID ] # Compute centroid. centroid_for_cluster_ID : Union [ ndarray , csr_matrix ] = sum ( members_of_cluster_ID ) / len ( members_of_cluster_ID ) # Add centroids. centroids [ cluster_ID ] = centroid_for_cluster_ID # Return centroids. return centroids","title":"compute_centroids()"},{"location":"reference/clustering/kmeans/#cognitivefactory.interactive_clustering.clustering.kmeans.KMeansConstrainedClustering.initialize_centroids","text":"Initialize the centroid of each cluster by a vector. The choice is based on a random selection among data to cluster. Returns: Type Description Dict[int, Union[ndarray,csr_matrix]] A dictionary which represent each cluster by a centroid. Source code in interactive_clustering\\clustering\\kmeans.py def initialize_centroids ( self , ) -> Dict [ int , Union [ ndarray , csr_matrix ]]: \"\"\" Initialize the centroid of each cluster by a vector. The choice is based on a random selection among data to cluster. Returns: Dict[int, Union[ndarray,csr_matrix]]: A dictionary which represent each cluster by a centroid. \"\"\" # Get the list of possible indices. indices : List [ str ] = self . list_of_data_IDs . copy () # Shuffle the list of possible indices. random . seed ( self . random_seed ) random . shuffle ( indices ) # Subset indices. indices = indices [: self . nb_clusters ] # Set initial centroids based on vectors. centroids : Dict [ int , Union [ ndarray , csr_matrix ]] = { cluster_ID : self . vectors [ indices [ cluster_ID ]] for cluster_ID in range ( self . nb_clusters ) } # Return centroids. return centroids","title":"initialize_centroids()"},{"location":"reference/clustering/spectral/","text":"Name: cognitivefactory.interactive_clustering.clustering.spectral Description: Implementation of constrained spectral clustering algorithms. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL-C License v1.0 ( https://cecill.info/licences.fr.html ) SpectralConstrainedClustering ( AbstractConstrainedClustering ) \u00b6 This class implements the spectral constrained clustering. It inherits from AbstractConstrainedClustering . References Spectral Clustering: Ng, A. Y., M. I. Jordan, et Y.Weiss (2002). On Spectral Clustering: Analysis and an algorithm. In T. G. Dietterich, S. Becker, et Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press. Constrained 'SPEC' Spectral Clustering: Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566. Examples: # Import. from scipy.sparse import csr_matrix from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager from cognitivefactory.interactive_clustering.clustering.spectral import SpectralConstrainedClustering # Create an instance of spectral clustering. clustering_model = SpectralConstrainedClustering ( model = \"SPEC\" , random_seed = 1 , ) # Define vectors. # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts. vectors = { \"0\" : csr_matrix ([ 1.00 , 0.00 , 0.00 , 0.00 ]), \"1\" : csr_matrix ([ 0.95 , 0.02 , 0.02 , 0.01 ]), \"2\" : csr_matrix ([ 0.98 , 0.00 , 0.02 , 0.00 ]), \"3\" : csr_matrix ([ 0.99 , 0.00 , 0.01 , 0.00 ]), \"4\" : csr_matrix ([ 0.60 , 0.17 , 0.16 , 0.07 ]), \"5\" : csr_matrix ([ 0.60 , 0.16 , 0.17 , 0.07 ]), \"6\" : csr_matrix ([ 0.01 , 0.01 , 0.01 , 0.97 ]), \"7\" : csr_matrix ([ 0.00 , 0.01 , 0.00 , 0.99 ]), \"8\" : csr_matrix ([ 0.00 , 0.00 , 0.00 , 1.00 ]), } # Define constraints manager. constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = list ( vectors . keys ())) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"1\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"2\" , data_ID2 = \"3\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"4\" , data_ID2 = \"5\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"7\" , data_ID2 = \"8\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"4\" , constraint_type = \"CANNOT_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"2\" , data_ID2 = \"4\" , constraint_type = \"CANNOT_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"4\" , data_ID2 = \"7\" , constraint_type = \"CANNOT_LINK\" ) # Run clustering. dict_of_predicted_clusters = clustering_model ( constraints_manager = constraints_manager , vectors = vectors , nb_clusters = 3 , ) # Print results. print ( \"Expected results\" , \";\" , { \"0\" : 0 , \"1\" : 0 , \"2\" : 0 , \"3\" : 0 , \"4\" : 1 , \"5\" : 1 , \"6\" : 2 , \"7\" : 2 , \"8\" : 2 ,}) print ( \"Computed results\" , \":\" , dict_of_predicted_clusters ) __init__ ( self , model = 'SPEC' , nb_components = None , random_seed = None , ** kargs ) special \u00b6 The constructor for Spectral Constrainted Clustering class. Parameters: Name Type Description Default model str The spectral clustering model to use. Available spectral models are \"SPEC\" and \"CCSR\" . Defaults to \"SPEC\" . 'SPEC' nb_components Optional[int] The number of eigenvectors to compute in the spectral clustering. If None , set the number of components to the number of clusters. Defaults to None . None random_seed Optional[int] The random seed to use to redo the same clustering. Defaults to None . None **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if some parameters are incorrectly set. Source code in interactive_clustering\\clustering\\spectral.py def __init__ ( self , model : str = \"SPEC\" , nb_components : Optional [ int ] = None , random_seed : Optional [ int ] = None , ** kargs ) -> None : \"\"\" The constructor for Spectral Constrainted Clustering class. Args: model (str, optional): The spectral clustering model to use. Available spectral models are `\"SPEC\"` and `\"CCSR\"`. Defaults to `\"SPEC\"`. nb_components (Optional[int], optional): The number of eigenvectors to compute in the spectral clustering. If `None`, set the number of components to the number of clusters. Defaults to `None`. random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if some parameters are incorrectly set. \"\"\" # Store `self.model`. if model != \"SPEC\" : # TODO use `not in {\"SPEC\"}`. # TODO `\"CCSR\"` to add after correction. raise ValueError ( \"The `model` '\" + str ( model ) + \"' is not implemented.\" ) self . model : str = model # Store `self.nb_components`. if ( nb_components is not None ) and ( nb_components < 2 ): raise ValueError ( \"The `nb_components` '\" + str ( nb_components ) + \"' must be `None` or greater than or equal to 2.\" ) self . nb_components : Optional [ int ] = nb_components # Store `self.random_seed`. self . random_seed : Optional [ int ] = random_seed # Store `self.kargs` for kmeans clustering. self . kargs = kargs # Initialize `self.dict_of_predicted_clusters`. self . dict_of_predicted_clusters : Optional [ Dict [ str , int ]] = None cluster ( self , constraints_manager , vectors , nb_clusters , verbose = False , ** kargs ) \u00b6 The main method used to cluster data with the Spectral model. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs that will force clustering to respect some conditions during computation. required vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required nb_clusters int The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? required verbose bool Enable verbose output. Defaults to False . False **kargs dict Other parameters that can be used in the clustering. {} Exceptions: Type Description ValueError if vectors and constraints_manager are incompatible, or if some parameters are incorrectly set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\spectral.py def cluster ( self , constraints_manager : AbstractConstraintsManager , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], nb_clusters : int , verbose : bool = False , ** kargs , ) -> Dict [ str , int ]: \"\"\" The main method used to cluster data with the Spectral model. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation. vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). nb_clusters (int): The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? verbose (bool, optional): Enable verbose output. Defaults to `False`. **kargs (dict): Other parameters that can be used in the clustering. Raises: ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" ### ### GET PARAMETERS ### # Store `self.constraints_manager` and `self.list_of_data_IDs`. if not isinstance ( constraints_manager , AbstractConstraintsManager ): raise ValueError ( \"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\" ) self . constraints_manager : AbstractConstraintsManager = constraints_manager self . list_of_data_IDs : List [ str ] = self . constraints_manager . get_list_of_managed_data_IDs () # Store `self.vectors`. if not isinstance ( vectors , dict ): raise ValueError ( \"The `vectors` parameter has to be a `dict` type.\" ) self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors # Store `self.nb_clusters`. if nb_clusters < 2 : raise ValueError ( \"The `nb_clusters` '\" + str ( nb_clusters ) + \"' must be greater than or equal to 2.\" ) self . nb_clusters : int = nb_clusters # Define `self.current_nb_components`. self . current_nb_components : int = ( self . nb_components if (( self . nb_components is not None ) and ( self . nb_clusters < self . nb_components )) else self . nb_clusters ) # Compute `self.pairwise_similarity_matrix`. self . pairwise_similarity_matrix = csr_matrix ( [ [ pairwise_kernels ( X = self . vectors [ data_ID1 ], Y = self . vectors [ data_ID2 ], metric = \"rbf\" )[ 0 ][ 0 ] . astype ( np . float64 ) for data_ID2 in self . list_of_data_IDs ] for data_ID1 in self . list_of_data_IDs ] ) ### ### RUN SPECTRAL CONSTRAINED CLUSTERING ### # Initialize `self.dict_of_predicted_clusters`. self . dict_of_predicted_clusters = None # Case of `\"CCSR\"` spectral clustering. # TODO Don't work. ##if self.model == \"CCSR\": ## self.dict_of_predicted_clusters = self.clustering_spectral_model_CCSR(verbose=verbose) # Case of `\"SPEC\"` spectral clustering. ##### DEFAULTS : if self.model==\"SPEC\": self . dict_of_predicted_clusters = self . clustering_spectral_model_SPEC ( verbose = verbose ) ### ### RETURN PREDICTED CLUSTERS ### return self . dict_of_predicted_clusters clustering_spectral_model_SPEC ( self , verbose = False ) \u00b6 Implementation of a simple Spectral clustering algorithm, based affinity matrix modifications. References : - Constrained 'SPEC' Spectral Clustering: Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566. Parameters: Name Type Description Default verbose bool Enable verbose output. Default is False . False Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\spectral.py def clustering_spectral_model_SPEC ( self , verbose : bool = False , ) -> Dict [ str , int ]: \"\"\" Implementation of a simple Spectral clustering algorithm, based affinity matrix modifications. References : - Constrained _'SPEC'_ Spectral Clustering: `Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566.` Args: verbose (bool, optional): Enable verbose output. Default is `False`. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" ### ### MODIFY CONSTRAINTS MATRIX WITH CONSTRAINTS ### # Modify the similarity over data IDs. for ID1 , data_ID1 in enumerate ( self . list_of_data_IDs ): for ID2 , data_ID2 in enumerate ( self . list_of_data_IDs ): # Symetry is already handled in next instructions. if ID1 > ID2 : continue # For each `\"MUST_LINK\"` constraint, set the similarity to 1.0. if ( self . constraints_manager . get_inferred_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , ) == \"MUST_LINK\" ): self . pairwise_similarity_matrix [ ID1 , ID2 ] = 1.0 self . pairwise_similarity_matrix [ ID2 , ID1 ] = 1.0 # For each `\"CANNOT_LINK\"` constraint, set the similarity to 0.0. elif ( self . constraints_manager . get_inferred_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , ) == \"CANNOT_LINK\" ): self . pairwise_similarity_matrix [ ID1 , ID2 ] = 0.0 self . pairwise_similarity_matrix [ ID2 , ID1 ] = 0.0 ### ### RUN SPECTRAL CONSTRAINED CLUSTERING ### | Define laplacian matrix ### | Compute eigen vectors ### | Cluster eigen vectors ### | Return labels based on eigen vectors clustering ### # Initialize spectral clustering model. self . clustering_model = SpectralClustering ( n_clusters = self . nb_clusters , # n_components=self.current_nb_components, #TODO Add if `scikit-learn>=0.24.1` affinity = \"precomputed\" , random_state = self . random_seed , ** self . kargs , ) # Run spectral clustering model. self . clustering_model . fit_predict ( X = self . pairwise_similarity_matrix ) # Get prediction of spectral clustering model. list_of_clusters : List [ int ] = self . clustering_model . labels_ . tolist () # Define the dictionary of predicted clusters. predicted_clusters : Dict [ str , int ] = { data_ID : list_of_clusters [ ID ] for ID , data_ID in enumerate ( self . list_of_data_IDs ) } # Rename cluster IDs by order. predicted_clusters = rename_clusters_by_order ( clusters = predicted_clusters ) # Return predicted clusters return predicted_clusters","title":"clustering.spectral"},{"location":"reference/clustering/spectral/#cognitivefactory.interactive_clustering.clustering.spectral.SpectralConstrainedClustering","text":"This class implements the spectral constrained clustering. It inherits from AbstractConstrainedClustering . References Spectral Clustering: Ng, A. Y., M. I. Jordan, et Y.Weiss (2002). On Spectral Clustering: Analysis and an algorithm. In T. G. Dietterich, S. Becker, et Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press. Constrained 'SPEC' Spectral Clustering: Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566. Examples: # Import. from scipy.sparse import csr_matrix from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager from cognitivefactory.interactive_clustering.clustering.spectral import SpectralConstrainedClustering # Create an instance of spectral clustering. clustering_model = SpectralConstrainedClustering ( model = \"SPEC\" , random_seed = 1 , ) # Define vectors. # NB : use cognitivefactory.interactive_clustering.utils to preprocess and vectorize texts. vectors = { \"0\" : csr_matrix ([ 1.00 , 0.00 , 0.00 , 0.00 ]), \"1\" : csr_matrix ([ 0.95 , 0.02 , 0.02 , 0.01 ]), \"2\" : csr_matrix ([ 0.98 , 0.00 , 0.02 , 0.00 ]), \"3\" : csr_matrix ([ 0.99 , 0.00 , 0.01 , 0.00 ]), \"4\" : csr_matrix ([ 0.60 , 0.17 , 0.16 , 0.07 ]), \"5\" : csr_matrix ([ 0.60 , 0.16 , 0.17 , 0.07 ]), \"6\" : csr_matrix ([ 0.01 , 0.01 , 0.01 , 0.97 ]), \"7\" : csr_matrix ([ 0.00 , 0.01 , 0.00 , 0.99 ]), \"8\" : csr_matrix ([ 0.00 , 0.00 , 0.00 , 1.00 ]), } # Define constraints manager. constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = list ( vectors . keys ())) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"1\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"2\" , data_ID2 = \"3\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"4\" , data_ID2 = \"5\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"7\" , data_ID2 = \"8\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"4\" , constraint_type = \"CANNOT_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"2\" , data_ID2 = \"4\" , constraint_type = \"CANNOT_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"4\" , data_ID2 = \"7\" , constraint_type = \"CANNOT_LINK\" ) # Run clustering. dict_of_predicted_clusters = clustering_model ( constraints_manager = constraints_manager , vectors = vectors , nb_clusters = 3 , ) # Print results. print ( \"Expected results\" , \";\" , { \"0\" : 0 , \"1\" : 0 , \"2\" : 0 , \"3\" : 0 , \"4\" : 1 , \"5\" : 1 , \"6\" : 2 , \"7\" : 2 , \"8\" : 2 ,}) print ( \"Computed results\" , \":\" , dict_of_predicted_clusters )","title":"SpectralConstrainedClustering"},{"location":"reference/clustering/spectral/#cognitivefactory.interactive_clustering.clustering.spectral.SpectralConstrainedClustering.__init__","text":"The constructor for Spectral Constrainted Clustering class. Parameters: Name Type Description Default model str The spectral clustering model to use. Available spectral models are \"SPEC\" and \"CCSR\" . Defaults to \"SPEC\" . 'SPEC' nb_components Optional[int] The number of eigenvectors to compute in the spectral clustering. If None , set the number of components to the number of clusters. Defaults to None . None random_seed Optional[int] The random seed to use to redo the same clustering. Defaults to None . None **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if some parameters are incorrectly set. Source code in interactive_clustering\\clustering\\spectral.py def __init__ ( self , model : str = \"SPEC\" , nb_components : Optional [ int ] = None , random_seed : Optional [ int ] = None , ** kargs ) -> None : \"\"\" The constructor for Spectral Constrainted Clustering class. Args: model (str, optional): The spectral clustering model to use. Available spectral models are `\"SPEC\"` and `\"CCSR\"`. Defaults to `\"SPEC\"`. nb_components (Optional[int], optional): The number of eigenvectors to compute in the spectral clustering. If `None`, set the number of components to the number of clusters. Defaults to `None`. random_seed (Optional[int], optional): The random seed to use to redo the same clustering. Defaults to `None`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if some parameters are incorrectly set. \"\"\" # Store `self.model`. if model != \"SPEC\" : # TODO use `not in {\"SPEC\"}`. # TODO `\"CCSR\"` to add after correction. raise ValueError ( \"The `model` '\" + str ( model ) + \"' is not implemented.\" ) self . model : str = model # Store `self.nb_components`. if ( nb_components is not None ) and ( nb_components < 2 ): raise ValueError ( \"The `nb_components` '\" + str ( nb_components ) + \"' must be `None` or greater than or equal to 2.\" ) self . nb_components : Optional [ int ] = nb_components # Store `self.random_seed`. self . random_seed : Optional [ int ] = random_seed # Store `self.kargs` for kmeans clustering. self . kargs = kargs # Initialize `self.dict_of_predicted_clusters`. self . dict_of_predicted_clusters : Optional [ Dict [ str , int ]] = None","title":"__init__()"},{"location":"reference/clustering/spectral/#cognitivefactory.interactive_clustering.clustering.spectral.SpectralConstrainedClustering.cluster","text":"The main method used to cluster data with the Spectral model. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs that will force clustering to respect some conditions during computation. required vectors Dict[str,Union[ndarray,csr_matrix]] The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). required nb_clusters int The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? required verbose bool Enable verbose output. Defaults to False . False **kargs dict Other parameters that can be used in the clustering. {} Exceptions: Type Description ValueError if vectors and constraints_manager are incompatible, or if some parameters are incorrectly set. Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\spectral.py def cluster ( self , constraints_manager : AbstractConstraintsManager , vectors : Dict [ str , Union [ ndarray , csr_matrix ]], nb_clusters : int , verbose : bool = False , ** kargs , ) -> Dict [ str , int ]: \"\"\" The main method used to cluster data with the Spectral model. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs that will force clustering to respect some conditions during computation. vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). nb_clusters (int): The number of clusters to compute. #TODO Set defaults to None with elbow method or other method ? verbose (bool, optional): Enable verbose output. Defaults to `False`. **kargs (dict): Other parameters that can be used in the clustering. Raises: ValueError: if `vectors` and `constraints_manager` are incompatible, or if some parameters are incorrectly set. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" ### ### GET PARAMETERS ### # Store `self.constraints_manager` and `self.list_of_data_IDs`. if not isinstance ( constraints_manager , AbstractConstraintsManager ): raise ValueError ( \"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\" ) self . constraints_manager : AbstractConstraintsManager = constraints_manager self . list_of_data_IDs : List [ str ] = self . constraints_manager . get_list_of_managed_data_IDs () # Store `self.vectors`. if not isinstance ( vectors , dict ): raise ValueError ( \"The `vectors` parameter has to be a `dict` type.\" ) self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors # Store `self.nb_clusters`. if nb_clusters < 2 : raise ValueError ( \"The `nb_clusters` '\" + str ( nb_clusters ) + \"' must be greater than or equal to 2.\" ) self . nb_clusters : int = nb_clusters # Define `self.current_nb_components`. self . current_nb_components : int = ( self . nb_components if (( self . nb_components is not None ) and ( self . nb_clusters < self . nb_components )) else self . nb_clusters ) # Compute `self.pairwise_similarity_matrix`. self . pairwise_similarity_matrix = csr_matrix ( [ [ pairwise_kernels ( X = self . vectors [ data_ID1 ], Y = self . vectors [ data_ID2 ], metric = \"rbf\" )[ 0 ][ 0 ] . astype ( np . float64 ) for data_ID2 in self . list_of_data_IDs ] for data_ID1 in self . list_of_data_IDs ] ) ### ### RUN SPECTRAL CONSTRAINED CLUSTERING ### # Initialize `self.dict_of_predicted_clusters`. self . dict_of_predicted_clusters = None # Case of `\"CCSR\"` spectral clustering. # TODO Don't work. ##if self.model == \"CCSR\": ## self.dict_of_predicted_clusters = self.clustering_spectral_model_CCSR(verbose=verbose) # Case of `\"SPEC\"` spectral clustering. ##### DEFAULTS : if self.model==\"SPEC\": self . dict_of_predicted_clusters = self . clustering_spectral_model_SPEC ( verbose = verbose ) ### ### RETURN PREDICTED CLUSTERS ### return self . dict_of_predicted_clusters","title":"cluster()"},{"location":"reference/clustering/spectral/#cognitivefactory.interactive_clustering.clustering.spectral.SpectralConstrainedClustering.clustering_spectral_model_SPEC","text":"Implementation of a simple Spectral clustering algorithm, based affinity matrix modifications. References : - Constrained 'SPEC' Spectral Clustering: Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566. Parameters: Name Type Description Default verbose bool Enable verbose output. Default is False . False Returns: Type Description Dict[str,int] A dictionary that contains the predicted cluster for each data ID. Source code in interactive_clustering\\clustering\\spectral.py def clustering_spectral_model_SPEC ( self , verbose : bool = False , ) -> Dict [ str , int ]: \"\"\" Implementation of a simple Spectral clustering algorithm, based affinity matrix modifications. References : - Constrained _'SPEC'_ Spectral Clustering: `Kamvar, S. D., D. Klein, et C. D. Manning (2003). Spectral Learning. Proceedings of the international joint conference on artificial intelligence, 561\u2013566.` Args: verbose (bool, optional): Enable verbose output. Default is `False`. Returns: Dict[str,int]: A dictionary that contains the predicted cluster for each data ID. \"\"\" ### ### MODIFY CONSTRAINTS MATRIX WITH CONSTRAINTS ### # Modify the similarity over data IDs. for ID1 , data_ID1 in enumerate ( self . list_of_data_IDs ): for ID2 , data_ID2 in enumerate ( self . list_of_data_IDs ): # Symetry is already handled in next instructions. if ID1 > ID2 : continue # For each `\"MUST_LINK\"` constraint, set the similarity to 1.0. if ( self . constraints_manager . get_inferred_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , ) == \"MUST_LINK\" ): self . pairwise_similarity_matrix [ ID1 , ID2 ] = 1.0 self . pairwise_similarity_matrix [ ID2 , ID1 ] = 1.0 # For each `\"CANNOT_LINK\"` constraint, set the similarity to 0.0. elif ( self . constraints_manager . get_inferred_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , ) == \"CANNOT_LINK\" ): self . pairwise_similarity_matrix [ ID1 , ID2 ] = 0.0 self . pairwise_similarity_matrix [ ID2 , ID1 ] = 0.0 ### ### RUN SPECTRAL CONSTRAINED CLUSTERING ### | Define laplacian matrix ### | Compute eigen vectors ### | Cluster eigen vectors ### | Return labels based on eigen vectors clustering ### # Initialize spectral clustering model. self . clustering_model = SpectralClustering ( n_clusters = self . nb_clusters , # n_components=self.current_nb_components, #TODO Add if `scikit-learn>=0.24.1` affinity = \"precomputed\" , random_state = self . random_seed , ** self . kargs , ) # Run spectral clustering model. self . clustering_model . fit_predict ( X = self . pairwise_similarity_matrix ) # Get prediction of spectral clustering model. list_of_clusters : List [ int ] = self . clustering_model . labels_ . tolist () # Define the dictionary of predicted clusters. predicted_clusters : Dict [ str , int ] = { data_ID : list_of_clusters [ ID ] for ID , data_ID in enumerate ( self . list_of_data_IDs ) } # Rename cluster IDs by order. predicted_clusters = rename_clusters_by_order ( clusters = predicted_clusters ) # Return predicted clusters return predicted_clusters","title":"clustering_spectral_model_SPEC()"},{"location":"reference/constraints/abstract/","text":"Name: cognitivefactory.interactive_clustering.constraints.abstract Description: The abstract class used to define constraints managing algorithms. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL-C License v1.0 ( https://cecill.info/licences.fr.html ) AbstractConstraintsManager ( ABC ) \u00b6 Abstract class that is used to define constraints manager. The main inherited methods are about data IDs management, constraints management and constraints exploration. References Constraints in clustering: Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110. add_constraint ( self , data_ID1 , data_ID2 , constraint_type , constraint_value = 1.0 ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to add a constraint between two data IDs. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint addition. required data_ID2 str The second data ID that is concerned for this constraint addition. required constraint_type str The type of the constraint to add. The type have to be \"MUST_LINK\" or \"CANNOT_LINK\" . required constraint_value float The value of the constraint to add. The value have to be in range [0.0, 1.0] . Defaults to 1.0. 1.0 Exceptions: Type Description ValueError if data_ID1 , data_ID2 , constraint_type are not managed, or if a conflict is detected with constraints inference. Returns: Type Description bool True if the addition is done, False is the constraint can't be added. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def add_constraint ( self , data_ID1 : str , data_ID2 : str , constraint_type : str , constraint_value : float = 1.0 , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to add a constraint between two data IDs. Args: data_ID1 (str): The first data ID that is concerned for this constraint addition. data_ID2 (str): The second data ID that is concerned for this constraint addition. constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. constraint_value (float, optional): The value of the constraint to add. The value have to be in range `[0.0, 1.0]`. Defaults to 1.0. Raises: ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed, or if a conflict is detected with constraints inference. Returns: bool: `True` if the addition is done, `False` is the constraint can't be added. \"\"\" add_data_ID ( self , data_ID ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to add a new data ID to manage. Parameters: Name Type Description Default data_ID str The data ID to manage. required Exceptions: Type Description ValueError if data_ID is already managed. Returns: Type Description bool True if the addition is done. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def add_data_ID ( self , data_ID : str , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to add a new data ID to manage. Args: data_ID (str): The data ID to manage. Raises: ValueError: if `data_ID` is already managed. Returns: bool: `True` if the addition is done. \"\"\" check_completude_of_constraints ( self , threshold = 1.0 ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if threshold is not managed. Returns: Type Description bool Return True if all constraints are known, False otherwise. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def check_completude_of_constraints ( self , threshold : float = 1.0 , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `threshold` is not managed. Returns: bool: Return `True` if all constraints are known, `False` otherwise. \"\"\" delete_constraint ( self , data_ID1 , data_ID2 ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to delete the constraint between two data IDs. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint deletion. required data_ID2 str The second data ID that is concerned for this constraint deletion. required Exceptions: Type Description ValueError if data_ID1 or data_ID2 are not managed. Returns: Type Description bool True if the deletion is done. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def delete_constraint ( self , data_ID1 : str , data_ID2 : str , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to delete the constraint between two data IDs. Args: data_ID1 (str): The first data ID that is concerned for this constraint deletion. data_ID2 (str): The second data ID that is concerned for this constraint deletion. Raises: ValueError: if `data_ID1` or `data_ID2` are not managed. Returns: bool: `True` if the deletion is done. \"\"\" delete_data_ID ( self , data_ID ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to delete a data ID to no longer manage. Parameters: Name Type Description Default data_ID str The data ID to no longer manage. required Exceptions: Type Description ValueError if data_ID is not managed. Returns: Type Description bool True if the deletion is done. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def delete_data_ID ( self , data_ID : str , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to delete a data ID to no longer manage. Args: data_ID (str): The data ID to no longer manage. Raises: ValueError: if `data_ID` is not managed. Returns: bool: `True` if the deletion is done. \"\"\" get_added_constraint ( self , data_ID1 , data_ID2 ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint. required data_ID2 str The second data ID that is concerned for this constraint. required Exceptions: Type Description ValueError if data_ID1 or data_ID2 are not managed. Returns: Type Description Optional[Tuple[str, float]] None if no constraint, (constraint_type, constraint_value) otherwise. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_added_constraint ( self , data_ID1 : str , data_ID2 : str , ) -> Optional [ Tuple [ str , float ]]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added. Args: data_ID1 (str): The first data ID that is concerned for this constraint. data_ID2 (str): The second data ID that is concerned for this constraint. Raises: ValueError: if `data_ID1` or `data_ID2` are not managed. Returns: Optional[Tuple[str, float]]: `None` if no constraint, `(constraint_type, constraint_value)` otherwise. \"\"\" get_connected_components ( self , threshold = 1.0 ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to get the possible lists of data IDs that are connected by a \"MUST_LINK\" constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if threshold is not managed. Returns: Type Description List[List[int]] The list of lists of data IDs that represent a component of the constraints transitivity graph. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_connected_components ( self , threshold : float = 1.0 , ) -> List [ List [ str ]]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get the possible lists of data IDs that are connected by a `\"MUST_LINK\"` constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `threshold` is not managed. Returns: List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph. \"\"\" get_inferred_constraint ( self , data_ID1 , data_ID2 , threshold = 1.0 ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint. required data_ID2 str The second data ID that is concerned for this constraint. required threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if data_ID1 , data_ID2 or threshold are not managed. Returns: Type Description Optional[str] The type of the inferred constraint. The type can be None , \"MUST_LINK\" or \"CANNOT_LINK\" . Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_inferred_constraint ( self , data_ID1 : str , data_ID2 : str , threshold : float = 1.0 , ) -> Optional [ str ]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: data_ID1 (str): The first data ID that is concerned for this constraint. data_ID2 (str): The second data ID that is concerned for this constraint. threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `data_ID1`, `data_ID2` or `threshold` are not managed. Returns: Optional[str]: The type of the inferred constraint. The type can be `None`, `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. \"\"\" get_list_of_involved_data_IDs_in_a_constraint_conflict ( self , data_ID1 , data_ID2 , constraint_type ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to get all data IDs involved in a constraints conflict. Parameters: Name Type Description Default data_ID1 str The first data ID involved in the constraint_conflit. required data_ID2 str The second data ID involved in the constraint_conflit. required constraint_type str The constraint that create a conflict. The constraints can be \"MUST_LINK\" or \"CANNOT_LINK\" . required Exceptions: Type Description ValueError if data_ID1 , data_ID2 , constraint_type are not managed. Returns: Type Description Optional[List[str]] The list of data IDs that are involved in the conflict. It matches data IDs from connected components of data_ID1 and data_ID2 . Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_list_of_involved_data_IDs_in_a_constraint_conflict ( self , data_ID1 : str , data_ID2 : str , constraint_type : str , ) -> Optional [ List [ str ]]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get all data IDs involved in a constraints conflict. Args: data_ID1 (str): The first data ID involved in the constraint_conflit. data_ID2 (str): The second data ID involved in the constraint_conflit. constraint_type (str): The constraint that create a conflict. The constraints can be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. Raises: ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed. Returns: Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of `data_ID1` and `data_ID2`. \"\"\" get_list_of_managed_data_IDs ( self ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to get the list of data IDs that are managed. Returns: Type Description List[str] The list of data IDs that are managed. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_list_of_managed_data_IDs ( self , ) -> List [ str ]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get the list of data IDs that are managed. Returns: List[str]: The list of data IDs that are managed. \"\"\" get_min_and_max_number_of_clusters ( self , threshold = 1.0 ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if threshold is not managed. Returns: Type Description Tuple[int,int] The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints). Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_min_and_max_number_of_clusters ( self , threshold : float = 1.0 , ) -> Tuple [ int , int ]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `threshold` is not managed. Returns: Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints). \"\"\"","title":"constraints.abstract"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager","text":"Abstract class that is used to define constraints manager. The main inherited methods are about data IDs management, constraints management and constraints exploration. References Constraints in clustering: Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110.","title":"AbstractConstraintsManager"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.add_constraint","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to add a constraint between two data IDs. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint addition. required data_ID2 str The second data ID that is concerned for this constraint addition. required constraint_type str The type of the constraint to add. The type have to be \"MUST_LINK\" or \"CANNOT_LINK\" . required constraint_value float The value of the constraint to add. The value have to be in range [0.0, 1.0] . Defaults to 1.0. 1.0 Exceptions: Type Description ValueError if data_ID1 , data_ID2 , constraint_type are not managed, or if a conflict is detected with constraints inference. Returns: Type Description bool True if the addition is done, False is the constraint can't be added. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def add_constraint ( self , data_ID1 : str , data_ID2 : str , constraint_type : str , constraint_value : float = 1.0 , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to add a constraint between two data IDs. Args: data_ID1 (str): The first data ID that is concerned for this constraint addition. data_ID2 (str): The second data ID that is concerned for this constraint addition. constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. constraint_value (float, optional): The value of the constraint to add. The value have to be in range `[0.0, 1.0]`. Defaults to 1.0. Raises: ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed, or if a conflict is detected with constraints inference. Returns: bool: `True` if the addition is done, `False` is the constraint can't be added. \"\"\"","title":"add_constraint()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.add_data_ID","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to add a new data ID to manage. Parameters: Name Type Description Default data_ID str The data ID to manage. required Exceptions: Type Description ValueError if data_ID is already managed. Returns: Type Description bool True if the addition is done. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def add_data_ID ( self , data_ID : str , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to add a new data ID to manage. Args: data_ID (str): The data ID to manage. Raises: ValueError: if `data_ID` is already managed. Returns: bool: `True` if the addition is done. \"\"\"","title":"add_data_ID()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.check_completude_of_constraints","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if threshold is not managed. Returns: Type Description bool Return True if all constraints are known, False otherwise. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def check_completude_of_constraints ( self , threshold : float = 1.0 , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `threshold` is not managed. Returns: bool: Return `True` if all constraints are known, `False` otherwise. \"\"\"","title":"check_completude_of_constraints()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.delete_constraint","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to delete the constraint between two data IDs. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint deletion. required data_ID2 str The second data ID that is concerned for this constraint deletion. required Exceptions: Type Description ValueError if data_ID1 or data_ID2 are not managed. Returns: Type Description bool True if the deletion is done. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def delete_constraint ( self , data_ID1 : str , data_ID2 : str , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to delete the constraint between two data IDs. Args: data_ID1 (str): The first data ID that is concerned for this constraint deletion. data_ID2 (str): The second data ID that is concerned for this constraint deletion. Raises: ValueError: if `data_ID1` or `data_ID2` are not managed. Returns: bool: `True` if the deletion is done. \"\"\"","title":"delete_constraint()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.delete_data_ID","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to delete a data ID to no longer manage. Parameters: Name Type Description Default data_ID str The data ID to no longer manage. required Exceptions: Type Description ValueError if data_ID is not managed. Returns: Type Description bool True if the deletion is done. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def delete_data_ID ( self , data_ID : str , ) -> bool : \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to delete a data ID to no longer manage. Args: data_ID (str): The data ID to no longer manage. Raises: ValueError: if `data_ID` is not managed. Returns: bool: `True` if the deletion is done. \"\"\"","title":"delete_data_ID()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_added_constraint","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint. required data_ID2 str The second data ID that is concerned for this constraint. required Exceptions: Type Description ValueError if data_ID1 or data_ID2 are not managed. Returns: Type Description Optional[Tuple[str, float]] None if no constraint, (constraint_type, constraint_value) otherwise. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_added_constraint ( self , data_ID1 : str , data_ID2 : str , ) -> Optional [ Tuple [ str , float ]]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added. Args: data_ID1 (str): The first data ID that is concerned for this constraint. data_ID2 (str): The second data ID that is concerned for this constraint. Raises: ValueError: if `data_ID1` or `data_ID2` are not managed. Returns: Optional[Tuple[str, float]]: `None` if no constraint, `(constraint_type, constraint_value)` otherwise. \"\"\"","title":"get_added_constraint()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_connected_components","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to get the possible lists of data IDs that are connected by a \"MUST_LINK\" constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if threshold is not managed. Returns: Type Description List[List[int]] The list of lists of data IDs that represent a component of the constraints transitivity graph. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_connected_components ( self , threshold : float = 1.0 , ) -> List [ List [ str ]]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get the possible lists of data IDs that are connected by a `\"MUST_LINK\"` constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `threshold` is not managed. Returns: List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph. \"\"\"","title":"get_connected_components()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_inferred_constraint","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint. required data_ID2 str The second data ID that is concerned for this constraint. required threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if data_ID1 , data_ID2 or threshold are not managed. Returns: Type Description Optional[str] The type of the inferred constraint. The type can be None , \"MUST_LINK\" or \"CANNOT_LINK\" . Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_inferred_constraint ( self , data_ID1 : str , data_ID2 : str , threshold : float = 1.0 , ) -> Optional [ str ]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: data_ID1 (str): The first data ID that is concerned for this constraint. data_ID2 (str): The second data ID that is concerned for this constraint. threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `data_ID1`, `data_ID2` or `threshold` are not managed. Returns: Optional[str]: The type of the inferred constraint. The type can be `None`, `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. \"\"\"","title":"get_inferred_constraint()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_list_of_involved_data_IDs_in_a_constraint_conflict","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to get all data IDs involved in a constraints conflict. Parameters: Name Type Description Default data_ID1 str The first data ID involved in the constraint_conflit. required data_ID2 str The second data ID involved in the constraint_conflit. required constraint_type str The constraint that create a conflict. The constraints can be \"MUST_LINK\" or \"CANNOT_LINK\" . required Exceptions: Type Description ValueError if data_ID1 , data_ID2 , constraint_type are not managed. Returns: Type Description Optional[List[str]] The list of data IDs that are involved in the conflict. It matches data IDs from connected components of data_ID1 and data_ID2 . Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_list_of_involved_data_IDs_in_a_constraint_conflict ( self , data_ID1 : str , data_ID2 : str , constraint_type : str , ) -> Optional [ List [ str ]]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get all data IDs involved in a constraints conflict. Args: data_ID1 (str): The first data ID involved in the constraint_conflit. data_ID2 (str): The second data ID involved in the constraint_conflit. constraint_type (str): The constraint that create a conflict. The constraints can be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. Raises: ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed. Returns: Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of `data_ID1` and `data_ID2`. \"\"\"","title":"get_list_of_involved_data_IDs_in_a_constraint_conflict()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_list_of_managed_data_IDs","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to get the list of data IDs that are managed. Returns: Type Description List[str] The list of data IDs that are managed. Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_list_of_managed_data_IDs ( self , ) -> List [ str ]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get the list of data IDs that are managed. Returns: List[str]: The list of data IDs that are managed. \"\"\"","title":"get_list_of_managed_data_IDs()"},{"location":"reference/constraints/abstract/#cognitivefactory.interactive_clustering.constraints.abstract.AbstractConstraintsManager.get_min_and_max_number_of_clusters","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if threshold is not managed. Returns: Type Description Tuple[int,int] The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints). Source code in interactive_clustering\\constraints\\abstract.py @abstractmethod def get_min_and_max_number_of_clusters ( self , threshold : float = 1.0 , ) -> Tuple [ int , int ]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `threshold` is not managed. Returns: Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints). \"\"\"","title":"get_min_and_max_number_of_clusters()"},{"location":"reference/constraints/binary/","text":"Name: cognitivefactory.interactive_clustering.constraints.binary Description: Implementation of binary constraints manager. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL-C License v1.0 ( https://cecill.info/licences.fr.html ) BinaryConstraintsManager ( AbstractConstraintsManager ) \u00b6 This class implements the binary constraints mangement. It inherits from AbstractConstraintsManager , and it takes into account the strong transitivity of constraints. References Binary constraints in clustering: Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110. Examples: # Import. from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager # Create an instance of binary constraints manager. constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = [ \"0\" , \"1\" , \"2\" , \"3\" , \"4\" ]) # Add new data ID. constraints_manager . add_data_ID ( data_ID = \"99\" ) # Get list of data IDs. constraints_manager . get_list_of_managed_data_IDs () # Delete an existing data ID. constraints_manager . delete_data_ID ( data_ID = \"99\" ) # Add constraints. constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"1\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"1\" , data_ID2 = \"2\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"2, data_ID2=\" 3 \", constraint_type=\" CANNOT_LINK \") # Get added constraint. constraints_manager . get_added_constraint ( data_ID1 = \"0\" , data_ID2 = \"1\" ) # expected (\"MUST_LINK\", 1.0) constraints_manager . get_added_constraint ( data_ID1 = \"0\" , data_ID2 = \"2\" ) # expected None # Get inferred constraint. constraints_manager . get_inferred_constraint ( data_ID1 = \"0\" , data_ID2 = \"2\" ) # expected \"MUST_LINK\" constraints_manager . get_inferred_constraint ( data_ID1 = \"0\" , data_ID2 = \"3\" ) # expected \"CANNOT_LINK\" constraints_manager . get_inferred_constraint ( data_ID1 = \"0\" , data_ID2 = \"4\" ) # expected None __init__ ( self , list_of_data_IDs , ** kargs ) special \u00b6 The constructor for Binary Constraints Manager class. This class use the strong transitivity to infer on constraints, so constraints values are not taken into account. Parameters: Name Type Description Default list_of_data_IDs List[str] The list of data IDs to manage. required **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if list_of_data_IDs has duplicates. Source code in interactive_clustering\\constraints\\binary.py def __init__ ( self , list_of_data_IDs : List [ str ], ** kargs ) -> None : \"\"\" The constructor for Binary Constraints Manager class. This class use the strong transitivity to infer on constraints, so constraints values are not taken into account. Args: list_of_data_IDs (List[str]): The list of data IDs to manage. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if `list_of_data_IDs` has duplicates. \"\"\" # Define `self._allowed_constraint_types`. self . _allowed_constraint_types : Set [ str ] = { \"MUST_LINK\" , \"CANNOT_LINK\" , } # Define `self._allowed_constraint_value_range`. self . _allowed_constraint_value_range : Dict [ str , float ] = { \"min\" : 1.0 , \"max\" : 1.0 , } # If `list_of_data_IDs` has duplicates, raise `ValueError`. if len ( list_of_data_IDs ) != len ( set ( list_of_data_IDs )): raise ValueError ( \"There is duplicates in `list_of_data_IDs`.\" ) # Store `self.kargs` for binary constraints managing. self . kargs = kargs # Initialize `self._constraints_dictionary`. self . _constraints_dictionary : Dict [ str , Dict [ str , Optional [ Tuple [ str , float ]]]] = { data_ID1 : { data_ID2 : ( ( \"MUST_LINK\" , 1.0 ) if ( data_ID1 == data_ID2 ) else None # Unknwon constraints if `data_ID1` != `data_ID2`. ) for data_ID2 in list_of_data_IDs } for data_ID1 in list_of_data_IDs } # Define `self._constraints_transitivity`. self . _generate_constraints_transitivity () add_constraint ( self , data_ID1 , data_ID2 , constraint_type , constraint_value = 1.0 ) \u00b6 The main method used to add a constraint between two data IDs. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint addition. required data_ID2 str The second data ID that is concerned for this constraint addition. required constraint_type str The type of the constraint to add. The type have to be \"MUST_LINK\" or \"CANNOT_LINK\" . required constraint_value float The value of the constraint to add. The value have to be in range [0.0, 1.0] . Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if data_ID1 , data_ID2 , constraint_type are not managed, or if a conflict is detected with constraints inference. Returns: Type Description bool True if the addition is done, False is the constraint can't be added. Source code in interactive_clustering\\constraints\\binary.py def add_constraint ( self , data_ID1 : str , data_ID2 : str , constraint_type : str , constraint_value : float = 1.0 , ) -> bool : \"\"\" The main method used to add a constraint between two data IDs. Args: data_ID1 (str): The first data ID that is concerned for this constraint addition. data_ID2 (str): The second data ID that is concerned for this constraint addition. constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. constraint_value (float, optional): The value of the constraint to add. The value have to be in range `[0.0, 1.0]`. Defaults to `1.0`. Raises: ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed, or if a conflict is detected with constraints inference. Returns: bool: `True` if the addition is done, `False` is the constraint can't be added. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # If the `constraint_type` is not in `self._allowed_constraint_types`, then raises a `ValueError`. if constraint_type not in self . _allowed_constraint_types : raise ValueError ( \"The `constraint_type` `'\" + str ( constraint_type ) + \"'` is not managed. Allowed constraints types are : `\" + str ( self . _allowed_constraint_types ) + \"`.\" ) # Get current added constraint between `data_ID1` and `data_ID2`. inferred_constraint : Optional [ str ] = self . get_inferred_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , ) # Case of conflict with constraints inference. if ( inferred_constraint is not None ) and ( inferred_constraint != constraint_type ): raise ValueError ( \"The `constraint_type` `'\" + str ( constraint_type ) + \"'` is incompatible with the inferred constraint `'\" + str ( inferred_constraint ) + \"'` between data IDs `'\" + data_ID1 + \"'` and `'\" + data_ID2 + \"'`.\" ) # Get current added constraint between `data_ID1` and `data_ID2`. added_constraint : Optional [ Tuple [ str , float ]] = self . get_added_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , ) # If the constraint has already be added, ... if added_constraint is not None : # ... do nothing. return True # `added_constraint[0] == constraint_type`. # Otherwise, the constraint has to be added. # Add the direct constraint between `data_ID1` and `data_ID2`. self . _constraints_dictionary [ data_ID1 ][ data_ID2 ] = ( constraint_type , 1.0 ) self . _constraints_dictionary [ data_ID2 ][ data_ID1 ] = ( constraint_type , 1.0 ) # Add the transitivity constraint between `data_ID1` and `data_ID2`. self . _add_constraint_transitivity ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , constraint_type = constraint_type , ) return True add_data_ID ( self , data_ID ) \u00b6 The main method used to add a new data ID to manage. Parameters: Name Type Description Default data_ID str The data ID to manage. required Exceptions: Type Description ValueError if data_ID is already managed. Returns: Type Description bool True if the addition is done. Source code in interactive_clustering\\constraints\\binary.py def add_data_ID ( self , data_ID : str , ) -> bool : \"\"\" The main method used to add a new data ID to manage. Args: data_ID (str): The data ID to manage. Raises: ValueError: if `data_ID` is already managed. Returns: bool: `True` if the addition is done. \"\"\" # If `data_ID` is in the data IDs that are currently managed, then raises a `ValueError`. if data_ID in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID` `'\" + str ( data_ID ) + \"'` is already managed.\" ) # Add `data_ID` to `self._constraints_dictionary.keys()`. self . _constraints_dictionary [ data_ID ] = {} # Define constraint for `data_ID` and all other data IDs. for other_data_ID in self . _constraints_dictionary . keys (): self . _constraints_dictionary [ data_ID ][ other_data_ID ] = ( ( \"MUST_LINK\" , 1.0 ) if ( data_ID == other_data_ID ) else None # Unknwon constraints if `data_ID` != `other_data_ID`. ) self . _constraints_dictionary [ other_data_ID ][ data_ID ] = ( ( \"MUST_LINK\" , 1.0 ) if ( data_ID == other_data_ID ) else None # Unknwon constraints if `data_ID1` != `other_data_ID`. ) # Regenerate `self._constraints_transitivity`. # `Equivalent to `self._generate_constraints_transitivity()` self . _constraints_transitivity [ data_ID ] = { \"MUST_LINK\" : { data_ID : None }, \"CANNOT_LINK\" : {}, } # Return `True`. return True check_completude_of_constraints ( self , threshold = 1.0 ) \u00b6 The main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the threshold parameters is used if constraints values are used in the constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to define the transitivity link. Defaults to 1.0 . 1.0 Returns: Type Description bool Return True if all constraints are known, False otherwise. Source code in interactive_clustering\\constraints\\binary.py def check_completude_of_constraints ( self , threshold : float = 1.0 , ) -> bool : \"\"\" The main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity. Args: threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`. Returns: bool: Return `True` if all constraints are known, `False` otherwise. \"\"\" # For each data ID... for data_ID in self . _constraints_transitivity . keys (): # ... if some data IDs are not linked by transitivity to this `data_ID` with a `\"MUST_LINK\"` or `\"CANNOT_LINK\"` constraints... if ( len ( self . _constraints_transitivity [ data_ID ][ \"MUST_LINK\" ] . keys ()) + len ( self . _constraints_transitivity [ data_ID ][ \"CANNOT_LINK\" ] . keys ()) ) != len ( self . _constraints_transitivity . keys ()): # ... then return `False`. return False # Otherwise, return `True`. return True delete_constraint ( self , data_ID1 , data_ID2 ) \u00b6 The main method used to delete a constraint between two data IDs. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint deletion. required data_ID2 str The second data ID that is concerned for this constraint deletion. required Exceptions: Type Description ValueError if data_ID1 or data_ID2 are not managed. Returns: Type Description bool True if the deletion is done, False if the constraint can't be deleted. Source code in interactive_clustering\\constraints\\binary.py def delete_constraint ( self , data_ID1 : str , data_ID2 : str , ) -> bool : \"\"\" The main method used to delete a constraint between two data IDs. Args: data_ID1 (str): The first data ID that is concerned for this constraint deletion. data_ID2 (str): The second data ID that is concerned for this constraint deletion. Raises: ValueError: if `data_ID1` or `data_ID2` are not managed. Returns: bool: `True` if the deletion is done, `False` if the constraint can't be deleted. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # Delete the constraint between `data_ID1` and `data_ID2`. self . _constraints_dictionary [ data_ID1 ][ data_ID2 ] = None self . _constraints_dictionary [ data_ID2 ][ data_ID1 ] = None # Regenerate `self._constraints_transitivity`. self . _generate_constraints_transitivity () # Return `True` return True delete_data_ID ( self , data_ID ) \u00b6 The main method used to delete a data ID to no longer manage. Parameters: Name Type Description Default data_ID str The data ID to no longer manage. required Exceptions: Type Description ValueError if data_ID is not managed. Returns: Type Description bool True if the deletion is done. Source code in interactive_clustering\\constraints\\binary.py def delete_data_ID ( self , data_ID : str , ) -> bool : \"\"\" The main method used to delete a data ID to no longer manage. Args: data_ID (str): The data ID to no longer manage. Raises: ValueError: if `data_ID` is not managed. Returns: bool: `True` if the deletion is done. \"\"\" # If `data_ID` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID` `'\" + str ( data_ID ) + \"'` is not managed.\" ) # Remove `data_ID` from `self._constraints_dictionary.keys()`. self . _constraints_dictionary . pop ( data_ID ) # Remove `data_ID` from all `self._constraints_dictionary[other_data_ID].keys()`. for other_data_ID in self . _constraints_dictionary . keys (): self . _constraints_dictionary [ other_data_ID ] . pop ( data_ID ) # Regenerate `self._constraints_transitivity` self . _generate_constraints_transitivity () # Return `True`. return True get_added_constraint ( self , data_ID1 , data_ID2 ) \u00b6 The main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint. required data_ID2 str The second data ID that is concerned for this constraint. required Exceptions: Type Description ValueError if data_ID1 or data_ID2 are not managed. Returns: Type Description Optional[Tuple[str, float]] None if no constraint, (constraint_type, constraint_value) otherwise. Source code in interactive_clustering\\constraints\\binary.py def get_added_constraint ( self , data_ID1 : str , data_ID2 : str , ) -> Optional [ Tuple [ str , float ]]: \"\"\" The main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added. Args: data_ID1 (str): The first data ID that is concerned for this constraint. data_ID2 (str): The second data ID that is concerned for this constraint. Raises: ValueError: if `data_ID1` or `data_ID2` are not managed. Returns: Optional[Tuple[str, float]]: `None` if no constraint, `(constraint_type, constraint_value)` otherwise. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # Retrun the current added constraint type and value. return self . _constraints_dictionary [ data_ID1 ][ data_ID2 ] get_connected_components ( self , threshold = 1.0 ) \u00b6 The main method used to get the possible lists of data IDs that are linked by a \"MUST_LINK\" constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the threshold parameters is used if constraints values are used in the constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to define the transitivity link. Defaults to 1.0 . 1.0 Returns: Type Description List[List[int]] The list of lists of data IDs that represent a component of the constraints transitivity graph. Source code in interactive_clustering\\constraints\\binary.py def get_connected_components ( self , threshold : float = 1.0 , ) -> List [ List [ str ]]: \"\"\" The main method used to get the possible lists of data IDs that are linked by a `\"MUST_LINK\"` constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity. Args: threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`. Returns: List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph. \"\"\" # Initialize the list of connected components. list_of_connected_components : List [ List [ str ]] = [] # For each data ID... for data_ID in self . _constraints_transitivity . keys (): # ... get the list of `\"MUST_LINK\"` data IDs linked by transitivity with `data_ID` ... connected_component_of_a_data_ID = list ( self . _constraints_transitivity [ data_ID ][ \"MUST_LINK\" ] . keys ()) # ... and if the connected component is not already get... if connected_component_of_a_data_ID not in list_of_connected_components : # ... then add it to the list of connected components. list_of_connected_components . append ( connected_component_of_a_data_ID ) # Return the list of connected components. return list_of_connected_components get_inferred_constraint ( self , data_ID1 , data_ID2 , threshold = 1.0 ) \u00b6 The main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint. required data_ID2 str The second data ID that is concerned for this constraint. required threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if data_ID1 , data_ID2 or threshold are not managed. Returns: Type Description Optional[str] The type of the inferred constraint. The type can be None , \"MUST_LINK\" or \"CANNOT_LINK\" . Source code in interactive_clustering\\constraints\\binary.py def get_inferred_constraint ( self , data_ID1 : str , data_ID2 : str , threshold : float = 1.0 , ) -> Optional [ str ]: \"\"\" The main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: data_ID1 (str): The first data ID that is concerned for this constraint. data_ID2 (str): The second data ID that is concerned for this constraint. threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `data_ID1`, `data_ID2` or `threshold` are not managed. Returns: Optional[str]: The type of the inferred constraint. The type can be `None`, `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_transitivity . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_transitivity . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # Case of `\"MUST_LINK\"`. if data_ID1 in self . _constraints_transitivity [ data_ID2 ][ \"MUST_LINK\" ] . keys (): return \"MUST_LINK\" # Case of `\"CANNOT_LINK\"`. if data_ID1 in self . _constraints_transitivity [ data_ID2 ][ \"CANNOT_LINK\" ] . keys (): return \"CANNOT_LINK\" # Case of `None`. return None get_list_of_involved_data_IDs_in_a_constraint_conflict ( self , data_ID1 , data_ID2 , constraint_type ) \u00b6 Get all data IDs involved in a constraints conflict. Parameters: Name Type Description Default data_ID1 str The first data ID involved in the constraint_conflit. required data_ID2 str The second data ID involved in the constraint_conflit. required constraint_type str The constraint that create a conflict. The constraints can be \"MUST_LINK\" or \"CANNOT_LINK\" . required Exceptions: Type Description ValueError if data_ID1 , data_ID2 , constraint_type are not managed. Returns: Type Description Optional[List[str]] The list of data IDs that are involved in the conflict. It matches data IDs from connected components of data_ID1 and data_ID2 . Source code in interactive_clustering\\constraints\\binary.py def get_list_of_involved_data_IDs_in_a_constraint_conflict ( self , data_ID1 : str , data_ID2 : str , constraint_type : str , ) -> Optional [ List [ str ]]: \"\"\" Get all data IDs involved in a constraints conflict. Args: data_ID1 (str): The first data ID involved in the constraint_conflit. data_ID2 (str): The second data ID involved in the constraint_conflit. constraint_type (str): The constraint that create a conflict. The constraints can be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. Raises: ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed. Returns: Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of `data_ID1` and `data_ID2`. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # If the `constraint_conflict` is not in `self._allowed_constraint_types`, then raises a `ValueError`. if constraint_type not in self . _allowed_constraint_types : raise ValueError ( \"The `constraint_type` `'\" + str ( constraint_type ) + \"'` is not managed. Allowed constraints types are : `\" + str ( self . _allowed_constraint_types ) + \"`.\" ) # Case of conflict (after trying to add a constraint different from the inferred constraint). if self . get_inferred_constraint ( data_ID1 , data_ID2 ) is not None and constraint_type != self . get_inferred_constraint ( data_ID1 , data_ID2 ): return [ data_ID for connected_component in self . get_connected_components () # Get involved components. for data_ID in connected_component # Get data IDs from these components. if data_ID1 in connected_component or data_ID2 in connected_component ] # Case of no conflict. return None get_list_of_managed_data_IDs ( self ) \u00b6 The main method used to get the list of data IDs that are managed. Returns: Type Description List[str] The list of data IDs that are managed. Source code in interactive_clustering\\constraints\\binary.py def get_list_of_managed_data_IDs ( self , ) -> List [ str ]: \"\"\" The main method used to get the list of data IDs that are managed. Returns: List[str]: The list of data IDs that are managed. \"\"\" # Return the possible keys of `self._constraints_dictionary`. return list ( self . _constraints_dictionary . keys ()) get_min_and_max_number_of_clusters ( self , threshold = 1.0 ) \u00b6 The main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. Minimum number of cluster is estimated by the coloration of the \"CANNOT_LINK\" constraints graph. Maximum number of cluster is defined by the number of \"MUST_LINK\" connected components. The transitivity is taken into account, and the threshold parameters is used if constraints values are used in the constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to define the transitivity link. Defaults to 1.0 . 1.0 Returns: Type Description Tuple[int,int] The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints). Source code in interactive_clustering\\constraints\\binary.py def get_min_and_max_number_of_clusters ( self , threshold : float = 1.0 , ) -> Tuple [ int , int ]: \"\"\" The main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. Minimum number of cluster is estimated by the coloration of the `\"CANNOT_LINK\"` constraints graph. Maximum number of cluster is defined by the number of `\"MUST_LINK\"` connected components. The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity. Args: threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`. Returns: Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints). \"\"\" # Get the `\"MUST_LINK\"` connected components. list_of_connected_components : List [ List [ str ]] = self . get_connected_components () ### ### 1. Estimation of minimum clusters number. ### # Get connected component ids. list_of_connected_component_ids : List [ str ] = [ component [ 0 ] for component in list_of_connected_components ] # Keep only components that have more that one `\"CANNOT_LINK\"` constraints. list_of_linked_connected_components_ids : List [ str ] = [ component_id for component_id in list_of_connected_component_ids if len ( self . _constraints_transitivity [ component_id ][ \"CANNOT_LINK\" ] . keys ()) > 1 # noqa: WPS507 ] # Get the `\"CANNOT_LINK\"` constraints. list_of_cannot_link_constraints : List [ Tuple [ int , int ]] = [ ( i1 , i2 ) for i1 , data_ID1 in enumerate ( list_of_linked_connected_components_ids ) for i2 , data_ID2 in enumerate ( list_of_linked_connected_components_ids ) if ( i1 < i2 ) and ( # To get the complement, get all possible link that are not a `\"CANNOT_LINK\"`. data_ID2 in self . _constraints_transitivity [ data_ID1 ][ \"CANNOT_LINK\" ] . keys () ) ] # Create a networkx graph. cannot_link_graph : nx . Graph = nx . Graph () cannot_link_graph . add_nodes_from ( list_of_connected_component_ids ) # Add components id as nodes in the graph. cannot_link_graph . add_edges_from ( list_of_cannot_link_constraints ) # Add cannot link constraints as edges in the graph. # Estimate the minimum clusters number by trying to colorate the `\"CANNOT_LINK\"` constraints graph. # The lower bound has to be greater than 2. estimation_of_minimum_clusters_number : int = max ( 2 , 1 + min ( max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"largest_first\" ) . values ()), max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"smallest_last\" ) . values ()), max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"random_sequential\" ) . values ()), max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"random_sequential\" ) . values ()), max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"random_sequential\" ) . values ()), ), ) ### ### 2. Computation of maximum clusters number. ### # Determine the maximum clusters number with the number of `\"MUST_LINK\"` connected components. maximum_clusters_number : int = len ( list_of_connected_components ) # Return minimum and maximum. return ( estimation_of_minimum_clusters_number , maximum_clusters_number )","title":"constraints.binary"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager","text":"This class implements the binary constraints mangement. It inherits from AbstractConstraintsManager , and it takes into account the strong transitivity of constraints. References Binary constraints in clustering: Wagstaff, K. et C. Cardie (2000). Clustering with Instance-level Constraints. Proceedings of the Seventeenth International Conference on Machine Learning, 1103\u20131110. Examples: # Import. from cognitivefactory.interactive_clustering.constraints.binary import BinaryConstraintsManager # Create an instance of binary constraints manager. constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = [ \"0\" , \"1\" , \"2\" , \"3\" , \"4\" ]) # Add new data ID. constraints_manager . add_data_ID ( data_ID = \"99\" ) # Get list of data IDs. constraints_manager . get_list_of_managed_data_IDs () # Delete an existing data ID. constraints_manager . delete_data_ID ( data_ID = \"99\" ) # Add constraints. constraints_manager . add_constraint ( data_ID1 = \"0\" , data_ID2 = \"1\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"1\" , data_ID2 = \"2\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"2, data_ID2=\" 3 \", constraint_type=\" CANNOT_LINK \") # Get added constraint. constraints_manager . get_added_constraint ( data_ID1 = \"0\" , data_ID2 = \"1\" ) # expected (\"MUST_LINK\", 1.0) constraints_manager . get_added_constraint ( data_ID1 = \"0\" , data_ID2 = \"2\" ) # expected None # Get inferred constraint. constraints_manager . get_inferred_constraint ( data_ID1 = \"0\" , data_ID2 = \"2\" ) # expected \"MUST_LINK\" constraints_manager . get_inferred_constraint ( data_ID1 = \"0\" , data_ID2 = \"3\" ) # expected \"CANNOT_LINK\" constraints_manager . get_inferred_constraint ( data_ID1 = \"0\" , data_ID2 = \"4\" ) # expected None","title":"BinaryConstraintsManager"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.__init__","text":"The constructor for Binary Constraints Manager class. This class use the strong transitivity to infer on constraints, so constraints values are not taken into account. Parameters: Name Type Description Default list_of_data_IDs List[str] The list of data IDs to manage. required **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if list_of_data_IDs has duplicates. Source code in interactive_clustering\\constraints\\binary.py def __init__ ( self , list_of_data_IDs : List [ str ], ** kargs ) -> None : \"\"\" The constructor for Binary Constraints Manager class. This class use the strong transitivity to infer on constraints, so constraints values are not taken into account. Args: list_of_data_IDs (List[str]): The list of data IDs to manage. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if `list_of_data_IDs` has duplicates. \"\"\" # Define `self._allowed_constraint_types`. self . _allowed_constraint_types : Set [ str ] = { \"MUST_LINK\" , \"CANNOT_LINK\" , } # Define `self._allowed_constraint_value_range`. self . _allowed_constraint_value_range : Dict [ str , float ] = { \"min\" : 1.0 , \"max\" : 1.0 , } # If `list_of_data_IDs` has duplicates, raise `ValueError`. if len ( list_of_data_IDs ) != len ( set ( list_of_data_IDs )): raise ValueError ( \"There is duplicates in `list_of_data_IDs`.\" ) # Store `self.kargs` for binary constraints managing. self . kargs = kargs # Initialize `self._constraints_dictionary`. self . _constraints_dictionary : Dict [ str , Dict [ str , Optional [ Tuple [ str , float ]]]] = { data_ID1 : { data_ID2 : ( ( \"MUST_LINK\" , 1.0 ) if ( data_ID1 == data_ID2 ) else None # Unknwon constraints if `data_ID1` != `data_ID2`. ) for data_ID2 in list_of_data_IDs } for data_ID1 in list_of_data_IDs } # Define `self._constraints_transitivity`. self . _generate_constraints_transitivity ()","title":"__init__()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.add_constraint","text":"The main method used to add a constraint between two data IDs. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint addition. required data_ID2 str The second data ID that is concerned for this constraint addition. required constraint_type str The type of the constraint to add. The type have to be \"MUST_LINK\" or \"CANNOT_LINK\" . required constraint_value float The value of the constraint to add. The value have to be in range [0.0, 1.0] . Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if data_ID1 , data_ID2 , constraint_type are not managed, or if a conflict is detected with constraints inference. Returns: Type Description bool True if the addition is done, False is the constraint can't be added. Source code in interactive_clustering\\constraints\\binary.py def add_constraint ( self , data_ID1 : str , data_ID2 : str , constraint_type : str , constraint_value : float = 1.0 , ) -> bool : \"\"\" The main method used to add a constraint between two data IDs. Args: data_ID1 (str): The first data ID that is concerned for this constraint addition. data_ID2 (str): The second data ID that is concerned for this constraint addition. constraint_type (str): The type of the constraint to add. The type have to be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. constraint_value (float, optional): The value of the constraint to add. The value have to be in range `[0.0, 1.0]`. Defaults to `1.0`. Raises: ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed, or if a conflict is detected with constraints inference. Returns: bool: `True` if the addition is done, `False` is the constraint can't be added. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # If the `constraint_type` is not in `self._allowed_constraint_types`, then raises a `ValueError`. if constraint_type not in self . _allowed_constraint_types : raise ValueError ( \"The `constraint_type` `'\" + str ( constraint_type ) + \"'` is not managed. Allowed constraints types are : `\" + str ( self . _allowed_constraint_types ) + \"`.\" ) # Get current added constraint between `data_ID1` and `data_ID2`. inferred_constraint : Optional [ str ] = self . get_inferred_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , ) # Case of conflict with constraints inference. if ( inferred_constraint is not None ) and ( inferred_constraint != constraint_type ): raise ValueError ( \"The `constraint_type` `'\" + str ( constraint_type ) + \"'` is incompatible with the inferred constraint `'\" + str ( inferred_constraint ) + \"'` between data IDs `'\" + data_ID1 + \"'` and `'\" + data_ID2 + \"'`.\" ) # Get current added constraint between `data_ID1` and `data_ID2`. added_constraint : Optional [ Tuple [ str , float ]] = self . get_added_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , ) # If the constraint has already be added, ... if added_constraint is not None : # ... do nothing. return True # `added_constraint[0] == constraint_type`. # Otherwise, the constraint has to be added. # Add the direct constraint between `data_ID1` and `data_ID2`. self . _constraints_dictionary [ data_ID1 ][ data_ID2 ] = ( constraint_type , 1.0 ) self . _constraints_dictionary [ data_ID2 ][ data_ID1 ] = ( constraint_type , 1.0 ) # Add the transitivity constraint between `data_ID1` and `data_ID2`. self . _add_constraint_transitivity ( data_ID1 = data_ID1 , data_ID2 = data_ID2 , constraint_type = constraint_type , ) return True","title":"add_constraint()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.add_data_ID","text":"The main method used to add a new data ID to manage. Parameters: Name Type Description Default data_ID str The data ID to manage. required Exceptions: Type Description ValueError if data_ID is already managed. Returns: Type Description bool True if the addition is done. Source code in interactive_clustering\\constraints\\binary.py def add_data_ID ( self , data_ID : str , ) -> bool : \"\"\" The main method used to add a new data ID to manage. Args: data_ID (str): The data ID to manage. Raises: ValueError: if `data_ID` is already managed. Returns: bool: `True` if the addition is done. \"\"\" # If `data_ID` is in the data IDs that are currently managed, then raises a `ValueError`. if data_ID in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID` `'\" + str ( data_ID ) + \"'` is already managed.\" ) # Add `data_ID` to `self._constraints_dictionary.keys()`. self . _constraints_dictionary [ data_ID ] = {} # Define constraint for `data_ID` and all other data IDs. for other_data_ID in self . _constraints_dictionary . keys (): self . _constraints_dictionary [ data_ID ][ other_data_ID ] = ( ( \"MUST_LINK\" , 1.0 ) if ( data_ID == other_data_ID ) else None # Unknwon constraints if `data_ID` != `other_data_ID`. ) self . _constraints_dictionary [ other_data_ID ][ data_ID ] = ( ( \"MUST_LINK\" , 1.0 ) if ( data_ID == other_data_ID ) else None # Unknwon constraints if `data_ID1` != `other_data_ID`. ) # Regenerate `self._constraints_transitivity`. # `Equivalent to `self._generate_constraints_transitivity()` self . _constraints_transitivity [ data_ID ] = { \"MUST_LINK\" : { data_ID : None }, \"CANNOT_LINK\" : {}, } # Return `True`. return True","title":"add_data_ID()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.check_completude_of_constraints","text":"The main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the threshold parameters is used if constraints values are used in the constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to define the transitivity link. Defaults to 1.0 . 1.0 Returns: Type Description bool Return True if all constraints are known, False otherwise. Source code in interactive_clustering\\constraints\\binary.py def check_completude_of_constraints ( self , threshold : float = 1.0 , ) -> bool : \"\"\" The main method used to check if all possible constraints are known (not necessarily annotated because of the transitivity). The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity. Args: threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`. Returns: bool: Return `True` if all constraints are known, `False` otherwise. \"\"\" # For each data ID... for data_ID in self . _constraints_transitivity . keys (): # ... if some data IDs are not linked by transitivity to this `data_ID` with a `\"MUST_LINK\"` or `\"CANNOT_LINK\"` constraints... if ( len ( self . _constraints_transitivity [ data_ID ][ \"MUST_LINK\" ] . keys ()) + len ( self . _constraints_transitivity [ data_ID ][ \"CANNOT_LINK\" ] . keys ()) ) != len ( self . _constraints_transitivity . keys ()): # ... then return `False`. return False # Otherwise, return `True`. return True","title":"check_completude_of_constraints()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.delete_constraint","text":"The main method used to delete a constraint between two data IDs. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint deletion. required data_ID2 str The second data ID that is concerned for this constraint deletion. required Exceptions: Type Description ValueError if data_ID1 or data_ID2 are not managed. Returns: Type Description bool True if the deletion is done, False if the constraint can't be deleted. Source code in interactive_clustering\\constraints\\binary.py def delete_constraint ( self , data_ID1 : str , data_ID2 : str , ) -> bool : \"\"\" The main method used to delete a constraint between two data IDs. Args: data_ID1 (str): The first data ID that is concerned for this constraint deletion. data_ID2 (str): The second data ID that is concerned for this constraint deletion. Raises: ValueError: if `data_ID1` or `data_ID2` are not managed. Returns: bool: `True` if the deletion is done, `False` if the constraint can't be deleted. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # Delete the constraint between `data_ID1` and `data_ID2`. self . _constraints_dictionary [ data_ID1 ][ data_ID2 ] = None self . _constraints_dictionary [ data_ID2 ][ data_ID1 ] = None # Regenerate `self._constraints_transitivity`. self . _generate_constraints_transitivity () # Return `True` return True","title":"delete_constraint()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.delete_data_ID","text":"The main method used to delete a data ID to no longer manage. Parameters: Name Type Description Default data_ID str The data ID to no longer manage. required Exceptions: Type Description ValueError if data_ID is not managed. Returns: Type Description bool True if the deletion is done. Source code in interactive_clustering\\constraints\\binary.py def delete_data_ID ( self , data_ID : str , ) -> bool : \"\"\" The main method used to delete a data ID to no longer manage. Args: data_ID (str): The data ID to no longer manage. Raises: ValueError: if `data_ID` is not managed. Returns: bool: `True` if the deletion is done. \"\"\" # If `data_ID` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID` `'\" + str ( data_ID ) + \"'` is not managed.\" ) # Remove `data_ID` from `self._constraints_dictionary.keys()`. self . _constraints_dictionary . pop ( data_ID ) # Remove `data_ID` from all `self._constraints_dictionary[other_data_ID].keys()`. for other_data_ID in self . _constraints_dictionary . keys (): self . _constraints_dictionary [ other_data_ID ] . pop ( data_ID ) # Regenerate `self._constraints_transitivity` self . _generate_constraints_transitivity () # Return `True`. return True","title":"delete_data_ID()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_added_constraint","text":"The main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint. required data_ID2 str The second data ID that is concerned for this constraint. required Exceptions: Type Description ValueError if data_ID1 or data_ID2 are not managed. Returns: Type Description Optional[Tuple[str, float]] None if no constraint, (constraint_type, constraint_value) otherwise. Source code in interactive_clustering\\constraints\\binary.py def get_added_constraint ( self , data_ID1 : str , data_ID2 : str , ) -> Optional [ Tuple [ str , float ]]: \"\"\" The main method used to get the constraint added between the two data IDs. Do not take into account the constraints transitivity, just look at constraints that are explicitly added. Args: data_ID1 (str): The first data ID that is concerned for this constraint. data_ID2 (str): The second data ID that is concerned for this constraint. Raises: ValueError: if `data_ID1` or `data_ID2` are not managed. Returns: Optional[Tuple[str, float]]: `None` if no constraint, `(constraint_type, constraint_value)` otherwise. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # Retrun the current added constraint type and value. return self . _constraints_dictionary [ data_ID1 ][ data_ID2 ]","title":"get_added_constraint()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_connected_components","text":"The main method used to get the possible lists of data IDs that are linked by a \"MUST_LINK\" constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the threshold parameters is used if constraints values are used in the constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to define the transitivity link. Defaults to 1.0 . 1.0 Returns: Type Description List[List[int]] The list of lists of data IDs that represent a component of the constraints transitivity graph. Source code in interactive_clustering\\constraints\\binary.py def get_connected_components ( self , threshold : float = 1.0 , ) -> List [ List [ str ]]: \"\"\" The main method used to get the possible lists of data IDs that are linked by a `\"MUST_LINK\"` constraints. Each list forms a component of the constraints transitivity graph, and it forms a partition of the managed data IDs. The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity. Args: threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`. Returns: List[List[int]]: The list of lists of data IDs that represent a component of the constraints transitivity graph. \"\"\" # Initialize the list of connected components. list_of_connected_components : List [ List [ str ]] = [] # For each data ID... for data_ID in self . _constraints_transitivity . keys (): # ... get the list of `\"MUST_LINK\"` data IDs linked by transitivity with `data_ID` ... connected_component_of_a_data_ID = list ( self . _constraints_transitivity [ data_ID ][ \"MUST_LINK\" ] . keys ()) # ... and if the connected component is not already get... if connected_component_of_a_data_ID not in list_of_connected_components : # ... then add it to the list of connected components. list_of_connected_components . append ( connected_component_of_a_data_ID ) # Return the list of connected components. return list_of_connected_components","title":"get_connected_components()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_inferred_constraint","text":"The main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the threshold parameter is used to evaluate the impact of constraints transitivity. Parameters: Name Type Description Default data_ID1 str The first data ID that is concerned for this constraint. required data_ID2 str The second data ID that is concerned for this constraint. required threshold float The threshold used to evaluate the impact of constraints transitivity link. Defaults to 1.0 . 1.0 Exceptions: Type Description ValueError if data_ID1 , data_ID2 or threshold are not managed. Returns: Type Description Optional[str] The type of the inferred constraint. The type can be None , \"MUST_LINK\" or \"CANNOT_LINK\" . Source code in interactive_clustering\\constraints\\binary.py def get_inferred_constraint ( self , data_ID1 : str , data_ID2 : str , threshold : float = 1.0 , ) -> Optional [ str ]: \"\"\" The main method used to check if the constraint inferred by transitivity between the two data IDs. The transitivity is taken into account, and the `threshold` parameter is used to evaluate the impact of constraints transitivity. Args: data_ID1 (str): The first data ID that is concerned for this constraint. data_ID2 (str): The second data ID that is concerned for this constraint. threshold (float, optional): The threshold used to evaluate the impact of constraints transitivity link. Defaults to `1.0`. Raises: ValueError: if `data_ID1`, `data_ID2` or `threshold` are not managed. Returns: Optional[str]: The type of the inferred constraint. The type can be `None`, `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_transitivity . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_transitivity . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # Case of `\"MUST_LINK\"`. if data_ID1 in self . _constraints_transitivity [ data_ID2 ][ \"MUST_LINK\" ] . keys (): return \"MUST_LINK\" # Case of `\"CANNOT_LINK\"`. if data_ID1 in self . _constraints_transitivity [ data_ID2 ][ \"CANNOT_LINK\" ] . keys (): return \"CANNOT_LINK\" # Case of `None`. return None","title":"get_inferred_constraint()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_list_of_involved_data_IDs_in_a_constraint_conflict","text":"Get all data IDs involved in a constraints conflict. Parameters: Name Type Description Default data_ID1 str The first data ID involved in the constraint_conflit. required data_ID2 str The second data ID involved in the constraint_conflit. required constraint_type str The constraint that create a conflict. The constraints can be \"MUST_LINK\" or \"CANNOT_LINK\" . required Exceptions: Type Description ValueError if data_ID1 , data_ID2 , constraint_type are not managed. Returns: Type Description Optional[List[str]] The list of data IDs that are involved in the conflict. It matches data IDs from connected components of data_ID1 and data_ID2 . Source code in interactive_clustering\\constraints\\binary.py def get_list_of_involved_data_IDs_in_a_constraint_conflict ( self , data_ID1 : str , data_ID2 : str , constraint_type : str , ) -> Optional [ List [ str ]]: \"\"\" Get all data IDs involved in a constraints conflict. Args: data_ID1 (str): The first data ID involved in the constraint_conflit. data_ID2 (str): The second data ID involved in the constraint_conflit. constraint_type (str): The constraint that create a conflict. The constraints can be `\"MUST_LINK\"` or `\"CANNOT_LINK\"`. Raises: ValueError: if `data_ID1`, `data_ID2`, `constraint_type` are not managed. Returns: Optional[List[str]]: The list of data IDs that are involved in the conflict. It matches data IDs from connected components of `data_ID1` and `data_ID2`. \"\"\" # If `data_ID1` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID1 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID1` `'\" + str ( data_ID1 ) + \"'` is not managed.\" ) # If `data_ID2` is not in the data IDs that are currently managed, then raises a `ValueError`. if data_ID2 not in self . _constraints_dictionary . keys (): raise ValueError ( \"The `data_ID2` `'\" + str ( data_ID2 ) + \"'` is not managed.\" ) # If the `constraint_conflict` is not in `self._allowed_constraint_types`, then raises a `ValueError`. if constraint_type not in self . _allowed_constraint_types : raise ValueError ( \"The `constraint_type` `'\" + str ( constraint_type ) + \"'` is not managed. Allowed constraints types are : `\" + str ( self . _allowed_constraint_types ) + \"`.\" ) # Case of conflict (after trying to add a constraint different from the inferred constraint). if self . get_inferred_constraint ( data_ID1 , data_ID2 ) is not None and constraint_type != self . get_inferred_constraint ( data_ID1 , data_ID2 ): return [ data_ID for connected_component in self . get_connected_components () # Get involved components. for data_ID in connected_component # Get data IDs from these components. if data_ID1 in connected_component or data_ID2 in connected_component ] # Case of no conflict. return None","title":"get_list_of_involved_data_IDs_in_a_constraint_conflict()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_list_of_managed_data_IDs","text":"The main method used to get the list of data IDs that are managed. Returns: Type Description List[str] The list of data IDs that are managed. Source code in interactive_clustering\\constraints\\binary.py def get_list_of_managed_data_IDs ( self , ) -> List [ str ]: \"\"\" The main method used to get the list of data IDs that are managed. Returns: List[str]: The list of data IDs that are managed. \"\"\" # Return the possible keys of `self._constraints_dictionary`. return list ( self . _constraints_dictionary . keys ())","title":"get_list_of_managed_data_IDs()"},{"location":"reference/constraints/binary/#cognitivefactory.interactive_clustering.constraints.binary.BinaryConstraintsManager.get_min_and_max_number_of_clusters","text":"The main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. Minimum number of cluster is estimated by the coloration of the \"CANNOT_LINK\" constraints graph. Maximum number of cluster is defined by the number of \"MUST_LINK\" connected components. The transitivity is taken into account, and the threshold parameters is used if constraints values are used in the constraints transitivity. Parameters: Name Type Description Default threshold float The threshold used to define the transitivity link. Defaults to 1.0 . 1.0 Returns: Type Description Tuple[int,int] The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints). Source code in interactive_clustering\\constraints\\binary.py def get_min_and_max_number_of_clusters ( self , threshold : float = 1.0 , ) -> Tuple [ int , int ]: \"\"\" The main method used to get determine, for a clustering model that would not violate any constraints, the range of the possible clusters number. Minimum number of cluster is estimated by the coloration of the `\"CANNOT_LINK\"` constraints graph. Maximum number of cluster is defined by the number of `\"MUST_LINK\"` connected components. The transitivity is taken into account, and the `threshold` parameters is used if constraints values are used in the constraints transitivity. Args: threshold (float, optional): The threshold used to define the transitivity link. Defaults to `1.0`. Returns: Tuple[int,int]: The minimum and the maximum possible clusters numbers (for a clustering model that would not violate any constraints). \"\"\" # Get the `\"MUST_LINK\"` connected components. list_of_connected_components : List [ List [ str ]] = self . get_connected_components () ### ### 1. Estimation of minimum clusters number. ### # Get connected component ids. list_of_connected_component_ids : List [ str ] = [ component [ 0 ] for component in list_of_connected_components ] # Keep only components that have more that one `\"CANNOT_LINK\"` constraints. list_of_linked_connected_components_ids : List [ str ] = [ component_id for component_id in list_of_connected_component_ids if len ( self . _constraints_transitivity [ component_id ][ \"CANNOT_LINK\" ] . keys ()) > 1 # noqa: WPS507 ] # Get the `\"CANNOT_LINK\"` constraints. list_of_cannot_link_constraints : List [ Tuple [ int , int ]] = [ ( i1 , i2 ) for i1 , data_ID1 in enumerate ( list_of_linked_connected_components_ids ) for i2 , data_ID2 in enumerate ( list_of_linked_connected_components_ids ) if ( i1 < i2 ) and ( # To get the complement, get all possible link that are not a `\"CANNOT_LINK\"`. data_ID2 in self . _constraints_transitivity [ data_ID1 ][ \"CANNOT_LINK\" ] . keys () ) ] # Create a networkx graph. cannot_link_graph : nx . Graph = nx . Graph () cannot_link_graph . add_nodes_from ( list_of_connected_component_ids ) # Add components id as nodes in the graph. cannot_link_graph . add_edges_from ( list_of_cannot_link_constraints ) # Add cannot link constraints as edges in the graph. # Estimate the minimum clusters number by trying to colorate the `\"CANNOT_LINK\"` constraints graph. # The lower bound has to be greater than 2. estimation_of_minimum_clusters_number : int = max ( 2 , 1 + min ( max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"largest_first\" ) . values ()), max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"smallest_last\" ) . values ()), max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"random_sequential\" ) . values ()), max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"random_sequential\" ) . values ()), max ( nx . coloring . greedy_color ( cannot_link_graph , strategy = \"random_sequential\" ) . values ()), ), ) ### ### 2. Computation of maximum clusters number. ### # Determine the maximum clusters number with the number of `\"MUST_LINK\"` connected components. maximum_clusters_number : int = len ( list_of_connected_components ) # Return minimum and maximum. return ( estimation_of_minimum_clusters_number , maximum_clusters_number )","title":"get_min_and_max_number_of_clusters()"},{"location":"reference/constraints/factory/","text":"Name: cognitivefactory.interactive_clustering.constraints.factory Description: The factory method used to easily initialize a constraints manager. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL-C License v1.0 ( https://cecill.info/licences.fr.html ) managing_factory ( list_of_data_IDs , manager = 'binary' , ** kargs ) \u00b6 A factory to create a new instance of a constraints manager. Parameters: Name Type Description Default list_of_data_IDs List[str] The list of data IDs to manage. required manager str The identification of constraints manager to instantiate. Can be \"binary\". Defaults to \"binary\" . 'binary' **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if manager is not implemented. Returns: Type Description AbstractConstraintsManager An instance of constraints manager. Examples: # Import. from cognitivefactory.interactive_clustering.constraints.factory import managing_factory # Create an instance of binary constraints manager. constraints_manager = managing_factory ( list_of_data_IDs = [ \"0\" , \"1\" , \"2\" , \"3\" , \"4\" ], manager = \"binary\" , ) Source code in interactive_clustering\\constraints\\factory.py def managing_factory ( list_of_data_IDs : List [ str ], manager : str = \"binary\" , ** kargs ) -> AbstractConstraintsManager : \"\"\" A factory to create a new instance of a constraints manager. Args: list_of_data_IDs (List[str]): The list of data IDs to manage. manager (str, optional): The identification of constraints manager to instantiate. Can be \"binary\". Defaults to `\"binary\"`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if `manager` is not implemented. Returns: AbstractConstraintsManager : An instance of constraints manager. Examples: ```python # Import. from cognitivefactory.interactive_clustering.constraints.factory import managing_factory # Create an instance of binary constraints manager. constraints_manager = managing_factory( list_of_data_IDs=[\"0\", \"1\", \"2\", \"3\", \"4\"], manager=\"binary\", ) ``` \"\"\" # Check that the requested algorithm is implemented. if manager != \"binary\" : # TODO use `not in {\"binary\"}`. raise ValueError ( \"The `manager` '\" + str ( manager ) + \"' is not implemented.\" ) # Case of Binary Constraints Manager ## if manager==\"binary\": return BinaryConstraintsManager ( list_of_data_IDs = list_of_data_IDs , ** kargs )","title":"constraints.factory"},{"location":"reference/constraints/factory/#cognitivefactory.interactive_clustering.constraints.factory.managing_factory","text":"A factory to create a new instance of a constraints manager. Parameters: Name Type Description Default list_of_data_IDs List[str] The list of data IDs to manage. required manager str The identification of constraints manager to instantiate. Can be \"binary\". Defaults to \"binary\" . 'binary' **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if manager is not implemented. Returns: Type Description AbstractConstraintsManager An instance of constraints manager. Examples: # Import. from cognitivefactory.interactive_clustering.constraints.factory import managing_factory # Create an instance of binary constraints manager. constraints_manager = managing_factory ( list_of_data_IDs = [ \"0\" , \"1\" , \"2\" , \"3\" , \"4\" ], manager = \"binary\" , ) Source code in interactive_clustering\\constraints\\factory.py def managing_factory ( list_of_data_IDs : List [ str ], manager : str = \"binary\" , ** kargs ) -> AbstractConstraintsManager : \"\"\" A factory to create a new instance of a constraints manager. Args: list_of_data_IDs (List[str]): The list of data IDs to manage. manager (str, optional): The identification of constraints manager to instantiate. Can be \"binary\". Defaults to `\"binary\"`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if `manager` is not implemented. Returns: AbstractConstraintsManager : An instance of constraints manager. Examples: ```python # Import. from cognitivefactory.interactive_clustering.constraints.factory import managing_factory # Create an instance of binary constraints manager. constraints_manager = managing_factory( list_of_data_IDs=[\"0\", \"1\", \"2\", \"3\", \"4\"], manager=\"binary\", ) ``` \"\"\" # Check that the requested algorithm is implemented. if manager != \"binary\" : # TODO use `not in {\"binary\"}`. raise ValueError ( \"The `manager` '\" + str ( manager ) + \"' is not implemented.\" ) # Case of Binary Constraints Manager ## if manager==\"binary\": return BinaryConstraintsManager ( list_of_data_IDs = list_of_data_IDs , ** kargs )","title":"managing_factory()"},{"location":"reference/sampling/abstract/","text":"Name: cognitivefactory.interactive_clustering.sampling.abstract Description: The abstract class used to define constraints sampling algorithms. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL ( https://cecill.info/licences.fr.html ) AbstractConstraintsSampling ( ABC ) \u00b6 Abstract class that is used to define constraints sampling algorithms. The main inherited method is sample . sample ( self , constraints_manager , nb_to_select , clustering_result = None , vectors = None , ** kargs ) \u00b6 (ABSTRACT METHOD) An abstract method that represents the main method used to sample couple of data IDs for constraints annotation. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs. required nb_to_select int The number of couple of data IDs to select. required clustering_result Optional[Dict[str,int]] A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If None , no clustering result are used during the sampling. Defaults to None . None vectors Optional[Dict[str,Union[ndarray,csr_matrix]]] vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). If None , no vectors are used during the sampling. Defaults to None None **kargs dict Other parameters that can be used in the sampling. {} Exceptions: Type Description ValueError if some parameters are incorrectly set or incompatible. Returns: Type Description List[Tuple[str,str]] A list of couple of data IDs. Source code in interactive_clustering\\sampling\\abstract.py @abstractmethod def sample ( self , constraints_manager : AbstractConstraintsManager , nb_to_select : int , clustering_result : Optional [ Dict [ str , int ]] = None , vectors : Optional [ Dict [ str , Union [ ndarray , csr_matrix ]]] = None , ** kargs , ) -> List [ Tuple [ str , str ]]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to sample couple of data IDs for constraints annotation. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs. nb_to_select (int): The number of couple of data IDs to select. clustering_result (Optional[Dict[str,int]], optional): A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If `None`, no clustering result are used during the sampling. Defaults to `None`. vectors (Optional[Dict[str,Union[ndarray,csr_matrix]]], optional): vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). If `None`, no vectors are used during the sampling. Defaults to `None` **kargs (dict): Other parameters that can be used in the sampling. Raises: ValueError: if some parameters are incorrectly set or incompatible. Returns: List[Tuple[str,str]]: A list of couple of data IDs. \"\"\"","title":"sampling.abstract"},{"location":"reference/sampling/abstract/#cognitivefactory.interactive_clustering.sampling.abstract.AbstractConstraintsSampling","text":"Abstract class that is used to define constraints sampling algorithms. The main inherited method is sample .","title":"AbstractConstraintsSampling"},{"location":"reference/sampling/abstract/#cognitivefactory.interactive_clustering.sampling.abstract.AbstractConstraintsSampling.sample","text":"(ABSTRACT METHOD) An abstract method that represents the main method used to sample couple of data IDs for constraints annotation. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs. required nb_to_select int The number of couple of data IDs to select. required clustering_result Optional[Dict[str,int]] A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If None , no clustering result are used during the sampling. Defaults to None . None vectors Optional[Dict[str,Union[ndarray,csr_matrix]]] vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). If None , no vectors are used during the sampling. Defaults to None None **kargs dict Other parameters that can be used in the sampling. {} Exceptions: Type Description ValueError if some parameters are incorrectly set or incompatible. Returns: Type Description List[Tuple[str,str]] A list of couple of data IDs. Source code in interactive_clustering\\sampling\\abstract.py @abstractmethod def sample ( self , constraints_manager : AbstractConstraintsManager , nb_to_select : int , clustering_result : Optional [ Dict [ str , int ]] = None , vectors : Optional [ Dict [ str , Union [ ndarray , csr_matrix ]]] = None , ** kargs , ) -> List [ Tuple [ str , str ]]: \"\"\" (ABSTRACT METHOD) An abstract method that represents the main method used to sample couple of data IDs for constraints annotation. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs. nb_to_select (int): The number of couple of data IDs to select. clustering_result (Optional[Dict[str,int]], optional): A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If `None`, no clustering result are used during the sampling. Defaults to `None`. vectors (Optional[Dict[str,Union[ndarray,csr_matrix]]], optional): vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). If `None`, no vectors are used during the sampling. Defaults to `None` **kargs (dict): Other parameters that can be used in the sampling. Raises: ValueError: if some parameters are incorrectly set or incompatible. Returns: List[Tuple[str,str]]: A list of couple of data IDs. \"\"\"","title":"sample()"},{"location":"reference/sampling/clusters_based/","text":"Name: cognitivefactory.interactive_clustering.sampling.clusters_based Description: Implementation of constraints sampling based on clusters information. Author: Erwan Schild Created: 04/10/2021 Licence: CeCILL ( https://cecill.info/licences.fr.html ) ClustersBasedConstraintsSampling ( AbstractConstraintsSampling ) \u00b6 This class implements the sampling of data IDs based on clusters information in order to annotate constraints. It inherits from AbstractConstraintsSampling . Examples: # Import. from r_wnlp.interactive_clustering.constraints.binary import BinaryConstraintsManager from r_wnlp.interactive_clustering.sampling.clusters_based import ClustersBasedConstraintsSampling # Create an instance of random sampling. sampler = ClustersBasedConstraintsSampling ( random_seed = 1 ) # Define list of data IDs. list_of_data_IDs = [ \"bonjour\" , \"salut\" , \"coucou\" , \"au revoir\" , \"a bient\u00f4t\" ,] # Define constraints manager (set it to None for no constraints). constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = list_of_data_IDs , ) constraints_manager . add_constraint ( data_ID1 = \"bonjour\" , data_ID2 = \"salut\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"au revoir\" , data_ID2 = \"a bient\u00f4t\" , constraint_type = \"MUST_LINK\" ) # Run sampling. selection = sampler . sample ( constraints_manager = constraints_manager , nb_to_select = 3 , ) # Print results. print ( \"Expected results\" , \";\" , [( \"au revoir\" , \"bonjour\" ), ( \"bonjour\" , \"coucou\" ), ( \"a bient\u00f4t\" , \"coucou\" ),]) print ( \"Computed results\" , \":\" , selection ) __init__ ( self , random_seed = None , clusters_restriction = None , distance_restriction = None , without_added_constraints = True , without_inferred_constraints = True , ** kargs ) special \u00b6 The constructor for Clusters Based Constraints Sampling class. Parameters: Name Type Description Default random_seed Optional[int] The random seed to use to redo the same sampling. Defaults to None . None clusters_restriction Optional[str] Restrict the sampling with a cluster constraints. Can impose data IDs to be in \"same_cluster\" or \"different_clusters\" . Defaults to None . # TODO: \"specific_clusters\" None distance_restriction Optional[str] Restrict the sampling with a distance constraints. Can impose data IDs to be \"closest_neighbors\" or \"\"farthest_neighbors\"\" . Defaults to None . None without_added_constraints bool Option to not sample the already added constraints. Defaults to True . True without_inferred_constraints bool Option to not sample the deduced constraints from already added one. Defaults to True . True **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if some parameters are incorrectly set. Source code in interactive_clustering\\sampling\\clusters_based.py def __init__ ( self , random_seed : Optional [ int ] = None , clusters_restriction : Optional [ str ] = None , distance_restriction : Optional [ str ] = None , without_added_constraints : bool = True , without_inferred_constraints : bool = True , ** kargs , ) -> None : \"\"\" The constructor for Clusters Based Constraints Sampling class. Args: random_seed (Optional[int]): The random seed to use to redo the same sampling. Defaults to `None`. clusters_restriction (Optional[str]): Restrict the sampling with a cluster constraints. Can impose data IDs to be in `\"same_cluster\"` or `\"different_clusters\"`. Defaults to `None`. # TODO: `\"specific_clusters\"` distance_restriction (Optional[str]): Restrict the sampling with a distance constraints. Can impose data IDs to be `\"closest_neighbors\"` or `\"\"farthest_neighbors\"\"`. Defaults to `None`. without_added_constraints (bool): Option to not sample the already added constraints. Defaults to `True`. without_inferred_constraints (bool): Option to not sample the deduced constraints from already added one. Defaults to `True`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if some parameters are incorrectly set. \"\"\" # Store `self.random_seed`. self . random_seed : Optional [ int ] = random_seed # Store clusters restriction. if clusters_restriction not in { None , \"same_cluster\" , \"different_clusters\" }: raise ValueError ( \"The `clusters_restriction` '\" + str ( clusters_restriction ) + \"' is not implemented.\" ) self . clusters_restriction : Optional [ str ] = clusters_restriction # Store distance restriction. if distance_restriction not in { None , \"closest_neighbors\" , \"farthest_neighbors\" }: raise ValueError ( \"The `distance_restriction` '\" + str ( distance_restriction ) + \"' is not implemented.\" ) self . distance_restriction : Optional [ str ] = distance_restriction # Store constraints restrictions. if not isinstance ( without_added_constraints , bool ): raise ValueError ( \"The `without_added_constraints` must be boolean\" ) self . without_added_constraints : bool = without_added_constraints if not isinstance ( without_inferred_constraints , bool ): raise ValueError ( \"The `without_inferred_constraints` must be boolean\" ) self . without_inferred_constraints : bool = without_inferred_constraints sample ( self , constraints_manager , nb_to_select , clustering_result = None , vectors = None , ** kargs ) \u00b6 The main method used to sample pairs of data IDs for constraints annotation. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs. required nb_to_select int The number of pairs of data IDs to sample. required clustering_result Optional[Dict[str,int]] A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If None , no clustering result are used during the sampling. Defaults to None . None vectors Optional[Dict[str,Union[ndarray,csr_matrix]]] vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). If None , no vectors are used during the sampling. Defaults to None None **kargs dict Other parameters that can be used in the sampling. {} Exceptions: Type Description ValueError if some parameters are incorrectly set or incompatible. Returns: Type Description List[Tuple[str,str]] A list of couple of data IDs. Source code in interactive_clustering\\sampling\\clusters_based.py def sample ( self , constraints_manager : AbstractConstraintsManager , nb_to_select : int , clustering_result : Optional [ Dict [ str , int ]] = None , vectors : Optional [ Dict [ str , Union [ ndarray , csr_matrix ]]] = None , ** kargs , ) -> List [ Tuple [ str , str ]]: \"\"\" The main method used to sample pairs of data IDs for constraints annotation. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs. nb_to_select (int): The number of pairs of data IDs to sample. clustering_result (Optional[Dict[str,int]], optional): A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If `None`, no clustering result are used during the sampling. Defaults to `None`. vectors (Optional[Dict[str,Union[ndarray,csr_matrix]]], optional): vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). If `None`, no vectors are used during the sampling. Defaults to `None` **kargs (dict): Other parameters that can be used in the sampling. Raises: ValueError: if some parameters are incorrectly set or incompatible. Returns: List[Tuple[str,str]]: A list of couple of data IDs. \"\"\" ### ### GET PARAMETERS ### # Check `constraints_manager`. if not isinstance ( constraints_manager , AbstractConstraintsManager ): raise ValueError ( \"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\" ) self . constraints_manager : AbstractConstraintsManager = constraints_manager # Check `nb_to_select`. if not isinstance ( nb_to_select , int ) or ( nb_to_select < 0 ): raise ValueError ( \"The `nb_to_select` '\" + str ( nb_to_select ) + \"' must be greater than or equal to 0.\" ) elif nb_to_select == 0 : return [] # If `self.cluster_restriction` is set, check `clustering_result` parameters. if self . clusters_restriction is not None : if not isinstance ( clustering_result , dict ): raise ValueError ( \"The `clustering_result` parameter has to be a `Dict[str, int]` type.\" ) self . clustering_result : Dict [ str , int ] = clustering_result # If `self.distance_restriction` is set, check `vectors` parameters. if self . distance_restriction is not None : if not isinstance ( vectors , dict ): raise ValueError ( \"The `vectors` parameter has to be a `Dict[str, Union[ndarray, csr_matrix]]` type.\" ) self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors ### ### DEFINE POSSIBLE PAIRS OF DATA IDS ### # Initialize possible pairs of data IDs list_of_possible_pairs_of_data_IDs : List [ Tuple [ str , str ]] = [] # Loop over pairs of data IDs. for data_ID1 in self . constraints_manager . get_list_of_managed_data_IDs (): for data_ID2 in self . constraints_manager . get_list_of_managed_data_IDs (): # Select ordered pairs. if data_ID1 >= data_ID2 : continue # Check clusters restriction. if ( self . clusters_restriction == \"same_cluster\" and self . clustering_result [ data_ID1 ] != self . clustering_result [ data_ID2 ] ) or ( self . clusters_restriction == \"different_clusters\" and self . clustering_result [ data_ID1 ] == self . clustering_result [ data_ID2 ] ): continue # Check known constraints. if ( self . without_added_constraints is True and self . constraints_manager . get_added_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 ) is not None ) or ( self . without_inferred_constraints is True and self . constraints_manager . get_inferred_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 ) is not None ): continue # Add the pair of data IDs. list_of_possible_pairs_of_data_IDs . append (( data_ID1 , data_ID2 )) ### ### SAMPLING ### # Precompute pairwise distances. if self . distance_restriction is not None : # Compute pairwise distances. matrix_of_pairwise_distances : ndarray = pairwise_distances ( X = vstack ( vector for vector in self . vectors . values ()), metric = \"euclidean\" , # TODO get different pairwise_distances config in **kargs ) # Format pairwise distances in a dictionary. self . dict_of_pairwise_distances : Dict [ str , Dict [ str , float ]] = { vector_ID1 : { vector_ID2 : float ( matrix_of_pairwise_distances [ i01 , i02 ]) for i02 , vector_ID2 in enumerate ( self . vectors . keys ()) } for i01 , vector_ID1 in enumerate ( self . vectors . keys ()) } # Set random seed. random . seed ( self . random_seed ) # Case of closest neightbors selection. if self . distance_restriction == \"closest_neighbors\" : return sorted ( list_of_possible_pairs_of_data_IDs , key = lambda combination : self . dict_of_pairwise_distances [ combination [ 0 ]][ combination [ 1 ]], )[: nb_to_select ] # Case of farthest neightbors selection. if self . distance_restriction == \"farthest_neighbors\" : return sorted ( list_of_possible_pairs_of_data_IDs , key = lambda combination : self . dict_of_pairwise_distances [ combination [ 0 ]][ combination [ 1 ]], reverse = True , )[: nb_to_select ] # (default) Case of random selection. return random . sample ( list_of_possible_pairs_of_data_IDs , k = min ( nb_to_select , len ( list_of_possible_pairs_of_data_IDs )) )","title":"sampling.clusters_based"},{"location":"reference/sampling/clusters_based/#cognitivefactory.interactive_clustering.sampling.clusters_based.ClustersBasedConstraintsSampling","text":"This class implements the sampling of data IDs based on clusters information in order to annotate constraints. It inherits from AbstractConstraintsSampling . Examples: # Import. from r_wnlp.interactive_clustering.constraints.binary import BinaryConstraintsManager from r_wnlp.interactive_clustering.sampling.clusters_based import ClustersBasedConstraintsSampling # Create an instance of random sampling. sampler = ClustersBasedConstraintsSampling ( random_seed = 1 ) # Define list of data IDs. list_of_data_IDs = [ \"bonjour\" , \"salut\" , \"coucou\" , \"au revoir\" , \"a bient\u00f4t\" ,] # Define constraints manager (set it to None for no constraints). constraints_manager = BinaryConstraintsManager ( list_of_data_IDs = list_of_data_IDs , ) constraints_manager . add_constraint ( data_ID1 = \"bonjour\" , data_ID2 = \"salut\" , constraint_type = \"MUST_LINK\" ) constraints_manager . add_constraint ( data_ID1 = \"au revoir\" , data_ID2 = \"a bient\u00f4t\" , constraint_type = \"MUST_LINK\" ) # Run sampling. selection = sampler . sample ( constraints_manager = constraints_manager , nb_to_select = 3 , ) # Print results. print ( \"Expected results\" , \";\" , [( \"au revoir\" , \"bonjour\" ), ( \"bonjour\" , \"coucou\" ), ( \"a bient\u00f4t\" , \"coucou\" ),]) print ( \"Computed results\" , \":\" , selection )","title":"ClustersBasedConstraintsSampling"},{"location":"reference/sampling/clusters_based/#cognitivefactory.interactive_clustering.sampling.clusters_based.ClustersBasedConstraintsSampling.__init__","text":"The constructor for Clusters Based Constraints Sampling class. Parameters: Name Type Description Default random_seed Optional[int] The random seed to use to redo the same sampling. Defaults to None . None clusters_restriction Optional[str] Restrict the sampling with a cluster constraints. Can impose data IDs to be in \"same_cluster\" or \"different_clusters\" . Defaults to None . # TODO: \"specific_clusters\" None distance_restriction Optional[str] Restrict the sampling with a distance constraints. Can impose data IDs to be \"closest_neighbors\" or \"\"farthest_neighbors\"\" . Defaults to None . None without_added_constraints bool Option to not sample the already added constraints. Defaults to True . True without_inferred_constraints bool Option to not sample the deduced constraints from already added one. Defaults to True . True **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if some parameters are incorrectly set. Source code in interactive_clustering\\sampling\\clusters_based.py def __init__ ( self , random_seed : Optional [ int ] = None , clusters_restriction : Optional [ str ] = None , distance_restriction : Optional [ str ] = None , without_added_constraints : bool = True , without_inferred_constraints : bool = True , ** kargs , ) -> None : \"\"\" The constructor for Clusters Based Constraints Sampling class. Args: random_seed (Optional[int]): The random seed to use to redo the same sampling. Defaults to `None`. clusters_restriction (Optional[str]): Restrict the sampling with a cluster constraints. Can impose data IDs to be in `\"same_cluster\"` or `\"different_clusters\"`. Defaults to `None`. # TODO: `\"specific_clusters\"` distance_restriction (Optional[str]): Restrict the sampling with a distance constraints. Can impose data IDs to be `\"closest_neighbors\"` or `\"\"farthest_neighbors\"\"`. Defaults to `None`. without_added_constraints (bool): Option to not sample the already added constraints. Defaults to `True`. without_inferred_constraints (bool): Option to not sample the deduced constraints from already added one. Defaults to `True`. **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if some parameters are incorrectly set. \"\"\" # Store `self.random_seed`. self . random_seed : Optional [ int ] = random_seed # Store clusters restriction. if clusters_restriction not in { None , \"same_cluster\" , \"different_clusters\" }: raise ValueError ( \"The `clusters_restriction` '\" + str ( clusters_restriction ) + \"' is not implemented.\" ) self . clusters_restriction : Optional [ str ] = clusters_restriction # Store distance restriction. if distance_restriction not in { None , \"closest_neighbors\" , \"farthest_neighbors\" }: raise ValueError ( \"The `distance_restriction` '\" + str ( distance_restriction ) + \"' is not implemented.\" ) self . distance_restriction : Optional [ str ] = distance_restriction # Store constraints restrictions. if not isinstance ( without_added_constraints , bool ): raise ValueError ( \"The `without_added_constraints` must be boolean\" ) self . without_added_constraints : bool = without_added_constraints if not isinstance ( without_inferred_constraints , bool ): raise ValueError ( \"The `without_inferred_constraints` must be boolean\" ) self . without_inferred_constraints : bool = without_inferred_constraints","title":"__init__()"},{"location":"reference/sampling/clusters_based/#cognitivefactory.interactive_clustering.sampling.clusters_based.ClustersBasedConstraintsSampling.sample","text":"The main method used to sample pairs of data IDs for constraints annotation. Parameters: Name Type Description Default constraints_manager AbstractConstraintsManager A constraints manager over data IDs. required nb_to_select int The number of pairs of data IDs to sample. required clustering_result Optional[Dict[str,int]] A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If None , no clustering result are used during the sampling. Defaults to None . None vectors Optional[Dict[str,Union[ndarray,csr_matrix]]] vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the constraints_manager . The value of the dictionary represent the vector of each data. Vectors can be dense ( numpy.ndarray ) or sparse ( scipy.sparse.csr_matrix ). If None , no vectors are used during the sampling. Defaults to None None **kargs dict Other parameters that can be used in the sampling. {} Exceptions: Type Description ValueError if some parameters are incorrectly set or incompatible. Returns: Type Description List[Tuple[str,str]] A list of couple of data IDs. Source code in interactive_clustering\\sampling\\clusters_based.py def sample ( self , constraints_manager : AbstractConstraintsManager , nb_to_select : int , clustering_result : Optional [ Dict [ str , int ]] = None , vectors : Optional [ Dict [ str , Union [ ndarray , csr_matrix ]]] = None , ** kargs , ) -> List [ Tuple [ str , str ]]: \"\"\" The main method used to sample pairs of data IDs for constraints annotation. Args: constraints_manager (AbstractConstraintsManager): A constraints manager over data IDs. nb_to_select (int): The number of pairs of data IDs to sample. clustering_result (Optional[Dict[str,int]], optional): A dictionary that represents the predicted cluster for each data ID. The keys of the dictionary represents the data IDs. If `None`, no clustering result are used during the sampling. Defaults to `None`. vectors (Optional[Dict[str,Union[ndarray,csr_matrix]]], optional): vectors (Dict[str,Union[ndarray,csr_matrix]]): The representation of data vectors. The keys of the dictionary represents the data IDs. This keys have to refer to the list of data IDs managed by the `constraints_manager`. The value of the dictionary represent the vector of each data. Vectors can be dense (`numpy.ndarray`) or sparse (`scipy.sparse.csr_matrix`). If `None`, no vectors are used during the sampling. Defaults to `None` **kargs (dict): Other parameters that can be used in the sampling. Raises: ValueError: if some parameters are incorrectly set or incompatible. Returns: List[Tuple[str,str]]: A list of couple of data IDs. \"\"\" ### ### GET PARAMETERS ### # Check `constraints_manager`. if not isinstance ( constraints_manager , AbstractConstraintsManager ): raise ValueError ( \"The `constraints_manager` parameter has to be a `AbstractConstraintsManager` type.\" ) self . constraints_manager : AbstractConstraintsManager = constraints_manager # Check `nb_to_select`. if not isinstance ( nb_to_select , int ) or ( nb_to_select < 0 ): raise ValueError ( \"The `nb_to_select` '\" + str ( nb_to_select ) + \"' must be greater than or equal to 0.\" ) elif nb_to_select == 0 : return [] # If `self.cluster_restriction` is set, check `clustering_result` parameters. if self . clusters_restriction is not None : if not isinstance ( clustering_result , dict ): raise ValueError ( \"The `clustering_result` parameter has to be a `Dict[str, int]` type.\" ) self . clustering_result : Dict [ str , int ] = clustering_result # If `self.distance_restriction` is set, check `vectors` parameters. if self . distance_restriction is not None : if not isinstance ( vectors , dict ): raise ValueError ( \"The `vectors` parameter has to be a `Dict[str, Union[ndarray, csr_matrix]]` type.\" ) self . vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = vectors ### ### DEFINE POSSIBLE PAIRS OF DATA IDS ### # Initialize possible pairs of data IDs list_of_possible_pairs_of_data_IDs : List [ Tuple [ str , str ]] = [] # Loop over pairs of data IDs. for data_ID1 in self . constraints_manager . get_list_of_managed_data_IDs (): for data_ID2 in self . constraints_manager . get_list_of_managed_data_IDs (): # Select ordered pairs. if data_ID1 >= data_ID2 : continue # Check clusters restriction. if ( self . clusters_restriction == \"same_cluster\" and self . clustering_result [ data_ID1 ] != self . clustering_result [ data_ID2 ] ) or ( self . clusters_restriction == \"different_clusters\" and self . clustering_result [ data_ID1 ] == self . clustering_result [ data_ID2 ] ): continue # Check known constraints. if ( self . without_added_constraints is True and self . constraints_manager . get_added_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 ) is not None ) or ( self . without_inferred_constraints is True and self . constraints_manager . get_inferred_constraint ( data_ID1 = data_ID1 , data_ID2 = data_ID2 ) is not None ): continue # Add the pair of data IDs. list_of_possible_pairs_of_data_IDs . append (( data_ID1 , data_ID2 )) ### ### SAMPLING ### # Precompute pairwise distances. if self . distance_restriction is not None : # Compute pairwise distances. matrix_of_pairwise_distances : ndarray = pairwise_distances ( X = vstack ( vector for vector in self . vectors . values ()), metric = \"euclidean\" , # TODO get different pairwise_distances config in **kargs ) # Format pairwise distances in a dictionary. self . dict_of_pairwise_distances : Dict [ str , Dict [ str , float ]] = { vector_ID1 : { vector_ID2 : float ( matrix_of_pairwise_distances [ i01 , i02 ]) for i02 , vector_ID2 in enumerate ( self . vectors . keys ()) } for i01 , vector_ID1 in enumerate ( self . vectors . keys ()) } # Set random seed. random . seed ( self . random_seed ) # Case of closest neightbors selection. if self . distance_restriction == \"closest_neighbors\" : return sorted ( list_of_possible_pairs_of_data_IDs , key = lambda combination : self . dict_of_pairwise_distances [ combination [ 0 ]][ combination [ 1 ]], )[: nb_to_select ] # Case of farthest neightbors selection. if self . distance_restriction == \"farthest_neighbors\" : return sorted ( list_of_possible_pairs_of_data_IDs , key = lambda combination : self . dict_of_pairwise_distances [ combination [ 0 ]][ combination [ 1 ]], reverse = True , )[: nb_to_select ] # (default) Case of random selection. return random . sample ( list_of_possible_pairs_of_data_IDs , k = min ( nb_to_select , len ( list_of_possible_pairs_of_data_IDs )) )","title":"sample()"},{"location":"reference/sampling/factory/","text":"Name: cognitivefactory.interactive_clustering.sampling.factory Description: The factory method used to easily initialize a constraints sampling algorithm. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL ( https://cecill.info/licences.fr.html ) sampling_factory ( algorithm , ** kargs ) \u00b6 A factory to create a new instance of a constraints sampling model. Parameters: Name Type Description Default algorithm str The identification of model to instantiate. Can be \"random\" or \"random_in_same_cluster\" or \"farthest_in_same_cluster\" or \"closest_in_different_clusters\" . Defaults to \"random\" required **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if algorithm is not implemented. Returns: Type Description AbstractConstraintsSampling An instance of constraints sampling model. Examples: # Import. from cognitivefactory.interactive_clustering.sampling.factory import sampling_factory # Create an instance of random sampler. sampler = sampling_factory ( algorithm = \"random\" , ) Source code in interactive_clustering\\sampling\\factory.py def sampling_factory ( algorithm : str , ** kargs ) -> \"AbstractConstraintsSampling\" : \"\"\" A factory to create a new instance of a constraints sampling model. Args: algorithm (str): The identification of model to instantiate. Can be `\"random\"` or `\"random_in_same_cluster\"` or `\"farthest_in_same_cluster\"` or `\"closest_in_different_clusters\"`. Defaults to `\"random\"` **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if `algorithm` is not implemented. Returns: AbstractConstraintsSampling: An instance of constraints sampling model. Examples: ```python # Import. from cognitivefactory.interactive_clustering.sampling.factory import sampling_factory # Create an instance of random sampler. sampler = sampling_factory( algorithm=\"random\", ) ``` \"\"\" # Check that the requested algorithm is implemented. if algorithm not in { \"random\" , \"random_in_same_cluster\" , \"farthest_in_same_cluster\" , \"closest_in_different_clusters\" , }: raise ValueError ( \"The `algorithm` '\" + str ( algorithm ) + \"' is not implemented.\" ) # Case of Random In Same Cluster Constraints Sampling. if algorithm == \"random_in_same_cluster\" : return ClustersBasedConstraintsSampling ( clusters_restriction = \"same_cluster\" , ** kargs ) # Case of Farthest In Same Cluster Constraints Sampling. if algorithm == \"farthest_in_same_cluster\" : return ClustersBasedConstraintsSampling ( distance_restriction = \"farthest_neighbors\" , clusters_restriction = \"same_cluster\" , ** kargs ) # Case of Closest In Different Clusters Constraints Sampling. if algorithm == \"closest_in_different_clusters\" : return ClustersBasedConstraintsSampling ( distance_restriction = \"closest_neighbors\" , clusters_restriction = \"different_clusters\" , ** kargs ) # Case of Random Constraints Sampling. ##if algorithm == \"random\": return ClustersBasedConstraintsSampling ( ** kargs )","title":"sampling.factory"},{"location":"reference/sampling/factory/#cognitivefactory.interactive_clustering.sampling.factory.sampling_factory","text":"A factory to create a new instance of a constraints sampling model. Parameters: Name Type Description Default algorithm str The identification of model to instantiate. Can be \"random\" or \"random_in_same_cluster\" or \"farthest_in_same_cluster\" or \"closest_in_different_clusters\" . Defaults to \"random\" required **kargs dict Other parameters that can be used in the instantiation. {} Exceptions: Type Description ValueError if algorithm is not implemented. Returns: Type Description AbstractConstraintsSampling An instance of constraints sampling model. Examples: # Import. from cognitivefactory.interactive_clustering.sampling.factory import sampling_factory # Create an instance of random sampler. sampler = sampling_factory ( algorithm = \"random\" , ) Source code in interactive_clustering\\sampling\\factory.py def sampling_factory ( algorithm : str , ** kargs ) -> \"AbstractConstraintsSampling\" : \"\"\" A factory to create a new instance of a constraints sampling model. Args: algorithm (str): The identification of model to instantiate. Can be `\"random\"` or `\"random_in_same_cluster\"` or `\"farthest_in_same_cluster\"` or `\"closest_in_different_clusters\"`. Defaults to `\"random\"` **kargs (dict): Other parameters that can be used in the instantiation. Raises: ValueError: if `algorithm` is not implemented. Returns: AbstractConstraintsSampling: An instance of constraints sampling model. Examples: ```python # Import. from cognitivefactory.interactive_clustering.sampling.factory import sampling_factory # Create an instance of random sampler. sampler = sampling_factory( algorithm=\"random\", ) ``` \"\"\" # Check that the requested algorithm is implemented. if algorithm not in { \"random\" , \"random_in_same_cluster\" , \"farthest_in_same_cluster\" , \"closest_in_different_clusters\" , }: raise ValueError ( \"The `algorithm` '\" + str ( algorithm ) + \"' is not implemented.\" ) # Case of Random In Same Cluster Constraints Sampling. if algorithm == \"random_in_same_cluster\" : return ClustersBasedConstraintsSampling ( clusters_restriction = \"same_cluster\" , ** kargs ) # Case of Farthest In Same Cluster Constraints Sampling. if algorithm == \"farthest_in_same_cluster\" : return ClustersBasedConstraintsSampling ( distance_restriction = \"farthest_neighbors\" , clusters_restriction = \"same_cluster\" , ** kargs ) # Case of Closest In Different Clusters Constraints Sampling. if algorithm == \"closest_in_different_clusters\" : return ClustersBasedConstraintsSampling ( distance_restriction = \"closest_neighbors\" , clusters_restriction = \"different_clusters\" , ** kargs ) # Case of Random Constraints Sampling. ##if algorithm == \"random\": return ClustersBasedConstraintsSampling ( ** kargs )","title":"sampling_factory()"},{"location":"reference/utils/frequency/","text":"Name: cognitivefactory.interactive_clustering.utils.frequency Description: Utilities methods for frequency analysis. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL ( https://cecill.info/licences.fr.html ) compute_clusters_frequency ( clustering_result ) \u00b6 Get the frequency of each cluster present in a clustering result. Parameters: Name Type Description Default clustering_result Dict[str,int] The dictionary that contains the predicted cluster for each data ID. required Returns: Type Description Dict[int,float] Frequency fo each predicted intent. Source code in interactive_clustering\\utils\\frequency.py def compute_clusters_frequency ( clustering_result : Dict [ str , int ]) -> Dict [ int , float ]: \"\"\" Get the frequency of each cluster present in a clustering result. Args: clustering_result (Dict[str,int]): The dictionary that contains the predicted cluster for each data ID. Returns: Dict[int,float] : Frequency fo each predicted intent. \"\"\" # Get the total number of data IDs. nb_of_data_IDs = len ( clustering_result . keys ()) # Default case : No data, so no cluster. if nb_of_data_IDs == 0 : return {} # Get possible clusters IDs. list_of_possible_cluster_IDs : List [ int ] = sorted ( { clustering_result [ data_ID ] for data_ID in clustering_result . keys ()} ) # Compute frequency of clusters in `clustering_result`. frequence_of_clusters : Dict [ int , float ] = { cluster_ID : len ([ data_ID for data_ID in clustering_result if clustering_result [ data_ID ] == cluster_ID ]) / nb_of_data_IDs for cluster_ID in list_of_possible_cluster_IDs } # Return the frequence of clusters. return frequence_of_clusters","title":"utils.frequency"},{"location":"reference/utils/frequency/#cognitivefactory.interactive_clustering.utils.frequency.compute_clusters_frequency","text":"Get the frequency of each cluster present in a clustering result. Parameters: Name Type Description Default clustering_result Dict[str,int] The dictionary that contains the predicted cluster for each data ID. required Returns: Type Description Dict[int,float] Frequency fo each predicted intent. Source code in interactive_clustering\\utils\\frequency.py def compute_clusters_frequency ( clustering_result : Dict [ str , int ]) -> Dict [ int , float ]: \"\"\" Get the frequency of each cluster present in a clustering result. Args: clustering_result (Dict[str,int]): The dictionary that contains the predicted cluster for each data ID. Returns: Dict[int,float] : Frequency fo each predicted intent. \"\"\" # Get the total number of data IDs. nb_of_data_IDs = len ( clustering_result . keys ()) # Default case : No data, so no cluster. if nb_of_data_IDs == 0 : return {} # Get possible clusters IDs. list_of_possible_cluster_IDs : List [ int ] = sorted ( { clustering_result [ data_ID ] for data_ID in clustering_result . keys ()} ) # Compute frequency of clusters in `clustering_result`. frequence_of_clusters : Dict [ int , float ] = { cluster_ID : len ([ data_ID for data_ID in clustering_result if clustering_result [ data_ID ] == cluster_ID ]) / nb_of_data_IDs for cluster_ID in list_of_possible_cluster_IDs } # Return the frequence of clusters. return frequence_of_clusters","title":"compute_clusters_frequency()"},{"location":"reference/utils/preprocessing/","text":"Name: cognitivefactory.interactive_clustering.utils.preprocessing Description: Utilities methods to apply NLP preprocessing. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL ( https://cecill.info/licences.fr.html ) preprocess ( dict_of_texts , apply_stopwords_deletion = False , apply_parsing_filter = False , apply_lemmatization = False , spacy_language_model = 'fr_core_news_sm' ) \u00b6 A method used to preprocess texts. It applies simple preprocessing (lowercasing, punctuations deletion, accents replacement, whitespace deletion). Some options are available to delete stopwords, apply lemmatization, and delete tokens according to their depth in the denpendency tree. References spaCy : Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. spaCy language models: https://spacy.io/usage/models NLTK : Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc. NLTK 'SnowballStemmer' : https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball Parameters: Name Type Description Default dict_of_texts Dict[str,str] A dictionary that contains the texts to preprocess. required apply_stopwords_deletion bool The option to delete stopwords. Defaults to False . False apply_parsing_filter bool The option to filter tokens based on dependency parsing results. If set, it only keeps \"ROOT\" tokens and their direct children. Defaults to False . False apply_lemmatization bool The option to lemmatize tokens. Defaults to False . False spacy_language_model str The spaCy language model to use if vectorizer is spacy. The model has to be installed. Defaults to \"fr_core_news_sm\" . 'fr_core_news_sm' Exceptions: Type Description ValueError Raises error if the spacy_language_model is not installed. Returns: Type Description Dict[str,str] A dictionary that contains the preprocessed texts. Examples: # Import. from cognitivefactory.interactive_clustering.utils.preprocessing import preprocess # Define data. dict_of_texts = { \"0\" : \"Comment signaler une perte de carte de paiement ?\" , \"1\" : \"Quelle est la proc\u00e9dure pour chercher une carte de cr\u00e9dit aval\u00e9e ?\" , \"2\" : \"Ma carte Visa a un plafond de paiment trop bas, puis-je l'augmenter ?\" , } # Apply preprocessing. dict_of_preprocessed_texts = preprocess ( dict_of_texts = dict_of_texts apply_stopwords_deletion = True , apply_parsing_filter = False , apply_lemmatization = False , spacy_language_model = \"fr_core_news_sm\" , ) # Print results. print ( \"Expected results\" , \";\" , { \"0\" : \"signaler perte carte paiement\" , \"1\" : \"procedure chercher carte credit avalee\" , \"2\" : \"carte visa plafond paiment l augmenter\" ,}) print ( \"Computed results\" , \":\" , dict_of_preprocessed_texts ) Source code in interactive_clustering\\utils\\preprocessing.py def preprocess ( dict_of_texts : Dict [ str , str ], apply_stopwords_deletion : bool = False , apply_parsing_filter : bool = False , apply_lemmatization : bool = False , spacy_language_model : str = \"fr_core_news_sm\" , ) -> Dict [ str , str ]: \"\"\" A method used to preprocess texts. It applies simple preprocessing (lowercasing, punctuations deletion, accents replacement, whitespace deletion). Some options are available to delete stopwords, apply lemmatization, and delete tokens according to their depth in the denpendency tree. References: - _spaCy_: `Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.` - _spaCy_ language models: `https://spacy.io/usage/models` - _NLTK_: `Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc.` - _NLTK_ _'SnowballStemmer'_: `https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball` Args: dict_of_texts (Dict[str,str]): A dictionary that contains the texts to preprocess. apply_stopwords_deletion (bool, optional): The option to delete stopwords. Defaults to `False`. apply_parsing_filter (bool, optional): The option to filter tokens based on dependency parsing results. If set, it only keeps `\"ROOT\"` tokens and their direct children. Defaults to `False`. apply_lemmatization (bool, optional): The option to lemmatize tokens. Defaults to `False`. spacy_language_model (str, optional): The spaCy language model to use if vectorizer is spacy. The model has to be installed. Defaults to `\"fr_core_news_sm\"`. Raises: ValueError: Raises error if the `spacy_language_model` is not installed. Returns: Dict[str,str]: A dictionary that contains the preprocessed texts. Examples: ```python # Import. from cognitivefactory.interactive_clustering.utils.preprocessing import preprocess # Define data. dict_of_texts={ \"0\": \"Comment signaler une perte de carte de paiement ?\", \"1\": \"Quelle est la proc\u00e9dure pour chercher une carte de cr\u00e9dit aval\u00e9e ?\", \"2\": \"Ma carte Visa a un plafond de paiment trop bas, puis-je l'augmenter ?\", } # Apply preprocessing. dict_of_preprocessed_texts = preprocess( dict_of_texts=dict_of_texts apply_stopwords_deletion=True, apply_parsing_filter=False, apply_lemmatization=False, spacy_language_model=\"fr_core_news_sm\", ) # Print results. print(\"Expected results\", \";\", {\"0\": \"signaler perte carte paiement\", \"1\": \"procedure chercher carte credit avalee\", \"2\": \"carte visa plafond paiment l augmenter\",}) print(\"Computed results\", \":\", dict_of_preprocessed_texts) ``` \"\"\" # Initialize dictionary of preprocessed texts. dict_of_preprocessed_texts : Dict [ str , str ] = {} # Initialize punctuation translator. punctuation_translator = str . maketrans ( { punct : \" \" for punct in ( \".\" , \",\" , \";\" , \":\" , \"!\" , \"\u00a1\" , \"?\" , \"\u00bf\" , \"\u2026\" , \"\u2022\" , \"(\" , \")\" , \"{\" , \"}\" , \"[\" , \"]\" , \"\u00ab\" , \"\u00bb\" , \"^\" , \"`\" , \"'\" , '\"' , \" \\\\ \" , \"/\" , \"|\" , \"-\" , \"_\" , \"#\" , \"&\" , \"~\" , \"@\" , ) } ) # Load vectorizer (spacy language model). try : spacy_nlp = spacy . load ( name = spacy_language_model , disable = [ # \"tagger\", # Needed for lemmatization. # \"parser\", # Needed for filtering on dependency parsing. \"ner\" , # Not needed ], ) except OSError as err : # `spacy_language_model` is not installed. raise ValueError ( \"The `spacy_language_model` '\" + str ( spacy_language_model ) + \"' is not installed.\" ) from err # Initialize stemmer. ####stemmer = SnowballStemmer(language=\"french\") # For each text... for key , text in dict_of_texts . items (): # Force string type. preprocessed_text : str = str ( text ) # Apply lowercasing. preprocessed_text = text . lower () # Apply punctuation deletion (before tokenization). preprocessed_text = preprocessed_text . translate ( punctuation_translator ) # Apply tokenization and spaCy pipeline. tokens = [ token for token in spacy_nlp ( preprocessed_text ) if ( # Spaces are not allowed. not token . is_space ) and ( # Punctuation are not allowed. not token . is_punct and not token . is_quote ) and ( # If set, stopwords are not allowed. ( not apply_stopwords_deletion ) or ( not token . is_stop ) ) and ( # If set, stopwords are not allowed. ( not apply_parsing_filter ) or ( len ( list ( token . ancestors )) <= 1 ) ) ] # Apply retokenization with lemmatization. if apply_lemmatization : preprocessed_text = \" \" . join ([ token . lemma_ . strip () for token in tokens ]) # Apply retokenization without lemmatization. else : preprocessed_text = \" \" . join ([ token . text . strip () for token in tokens ]) # Apply accents deletion (after lemmatization). preprocessed_text = \"\" . join ( [ char for char in unicodedata . normalize ( \"NFKD\" , preprocessed_text ) if not unicodedata . combining ( char )] ) # Store preprocessed text. dict_of_preprocessed_texts [ key ] = preprocessed_text return dict_of_preprocessed_texts","title":"utils.preprocessing"},{"location":"reference/utils/preprocessing/#cognitivefactory.interactive_clustering.utils.preprocessing.preprocess","text":"A method used to preprocess texts. It applies simple preprocessing (lowercasing, punctuations deletion, accents replacement, whitespace deletion). Some options are available to delete stopwords, apply lemmatization, and delete tokens according to their depth in the denpendency tree. References spaCy : Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. spaCy language models: https://spacy.io/usage/models NLTK : Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc. NLTK 'SnowballStemmer' : https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball Parameters: Name Type Description Default dict_of_texts Dict[str,str] A dictionary that contains the texts to preprocess. required apply_stopwords_deletion bool The option to delete stopwords. Defaults to False . False apply_parsing_filter bool The option to filter tokens based on dependency parsing results. If set, it only keeps \"ROOT\" tokens and their direct children. Defaults to False . False apply_lemmatization bool The option to lemmatize tokens. Defaults to False . False spacy_language_model str The spaCy language model to use if vectorizer is spacy. The model has to be installed. Defaults to \"fr_core_news_sm\" . 'fr_core_news_sm' Exceptions: Type Description ValueError Raises error if the spacy_language_model is not installed. Returns: Type Description Dict[str,str] A dictionary that contains the preprocessed texts. Examples: # Import. from cognitivefactory.interactive_clustering.utils.preprocessing import preprocess # Define data. dict_of_texts = { \"0\" : \"Comment signaler une perte de carte de paiement ?\" , \"1\" : \"Quelle est la proc\u00e9dure pour chercher une carte de cr\u00e9dit aval\u00e9e ?\" , \"2\" : \"Ma carte Visa a un plafond de paiment trop bas, puis-je l'augmenter ?\" , } # Apply preprocessing. dict_of_preprocessed_texts = preprocess ( dict_of_texts = dict_of_texts apply_stopwords_deletion = True , apply_parsing_filter = False , apply_lemmatization = False , spacy_language_model = \"fr_core_news_sm\" , ) # Print results. print ( \"Expected results\" , \";\" , { \"0\" : \"signaler perte carte paiement\" , \"1\" : \"procedure chercher carte credit avalee\" , \"2\" : \"carte visa plafond paiment l augmenter\" ,}) print ( \"Computed results\" , \":\" , dict_of_preprocessed_texts ) Source code in interactive_clustering\\utils\\preprocessing.py def preprocess ( dict_of_texts : Dict [ str , str ], apply_stopwords_deletion : bool = False , apply_parsing_filter : bool = False , apply_lemmatization : bool = False , spacy_language_model : str = \"fr_core_news_sm\" , ) -> Dict [ str , str ]: \"\"\" A method used to preprocess texts. It applies simple preprocessing (lowercasing, punctuations deletion, accents replacement, whitespace deletion). Some options are available to delete stopwords, apply lemmatization, and delete tokens according to their depth in the denpendency tree. References: - _spaCy_: `Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.` - _spaCy_ language models: `https://spacy.io/usage/models` - _NLTK_: `Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O\u2019Reilly Media Inc.` - _NLTK_ _'SnowballStemmer'_: `https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball` Args: dict_of_texts (Dict[str,str]): A dictionary that contains the texts to preprocess. apply_stopwords_deletion (bool, optional): The option to delete stopwords. Defaults to `False`. apply_parsing_filter (bool, optional): The option to filter tokens based on dependency parsing results. If set, it only keeps `\"ROOT\"` tokens and their direct children. Defaults to `False`. apply_lemmatization (bool, optional): The option to lemmatize tokens. Defaults to `False`. spacy_language_model (str, optional): The spaCy language model to use if vectorizer is spacy. The model has to be installed. Defaults to `\"fr_core_news_sm\"`. Raises: ValueError: Raises error if the `spacy_language_model` is not installed. Returns: Dict[str,str]: A dictionary that contains the preprocessed texts. Examples: ```python # Import. from cognitivefactory.interactive_clustering.utils.preprocessing import preprocess # Define data. dict_of_texts={ \"0\": \"Comment signaler une perte de carte de paiement ?\", \"1\": \"Quelle est la proc\u00e9dure pour chercher une carte de cr\u00e9dit aval\u00e9e ?\", \"2\": \"Ma carte Visa a un plafond de paiment trop bas, puis-je l'augmenter ?\", } # Apply preprocessing. dict_of_preprocessed_texts = preprocess( dict_of_texts=dict_of_texts apply_stopwords_deletion=True, apply_parsing_filter=False, apply_lemmatization=False, spacy_language_model=\"fr_core_news_sm\", ) # Print results. print(\"Expected results\", \";\", {\"0\": \"signaler perte carte paiement\", \"1\": \"procedure chercher carte credit avalee\", \"2\": \"carte visa plafond paiment l augmenter\",}) print(\"Computed results\", \":\", dict_of_preprocessed_texts) ``` \"\"\" # Initialize dictionary of preprocessed texts. dict_of_preprocessed_texts : Dict [ str , str ] = {} # Initialize punctuation translator. punctuation_translator = str . maketrans ( { punct : \" \" for punct in ( \".\" , \",\" , \";\" , \":\" , \"!\" , \"\u00a1\" , \"?\" , \"\u00bf\" , \"\u2026\" , \"\u2022\" , \"(\" , \")\" , \"{\" , \"}\" , \"[\" , \"]\" , \"\u00ab\" , \"\u00bb\" , \"^\" , \"`\" , \"'\" , '\"' , \" \\\\ \" , \"/\" , \"|\" , \"-\" , \"_\" , \"#\" , \"&\" , \"~\" , \"@\" , ) } ) # Load vectorizer (spacy language model). try : spacy_nlp = spacy . load ( name = spacy_language_model , disable = [ # \"tagger\", # Needed for lemmatization. # \"parser\", # Needed for filtering on dependency parsing. \"ner\" , # Not needed ], ) except OSError as err : # `spacy_language_model` is not installed. raise ValueError ( \"The `spacy_language_model` '\" + str ( spacy_language_model ) + \"' is not installed.\" ) from err # Initialize stemmer. ####stemmer = SnowballStemmer(language=\"french\") # For each text... for key , text in dict_of_texts . items (): # Force string type. preprocessed_text : str = str ( text ) # Apply lowercasing. preprocessed_text = text . lower () # Apply punctuation deletion (before tokenization). preprocessed_text = preprocessed_text . translate ( punctuation_translator ) # Apply tokenization and spaCy pipeline. tokens = [ token for token in spacy_nlp ( preprocessed_text ) if ( # Spaces are not allowed. not token . is_space ) and ( # Punctuation are not allowed. not token . is_punct and not token . is_quote ) and ( # If set, stopwords are not allowed. ( not apply_stopwords_deletion ) or ( not token . is_stop ) ) and ( # If set, stopwords are not allowed. ( not apply_parsing_filter ) or ( len ( list ( token . ancestors )) <= 1 ) ) ] # Apply retokenization with lemmatization. if apply_lemmatization : preprocessed_text = \" \" . join ([ token . lemma_ . strip () for token in tokens ]) # Apply retokenization without lemmatization. else : preprocessed_text = \" \" . join ([ token . text . strip () for token in tokens ]) # Apply accents deletion (after lemmatization). preprocessed_text = \"\" . join ( [ char for char in unicodedata . normalize ( \"NFKD\" , preprocessed_text ) if not unicodedata . combining ( char )] ) # Store preprocessed text. dict_of_preprocessed_texts [ key ] = preprocessed_text return dict_of_preprocessed_texts","title":"preprocess()"},{"location":"reference/utils/vectorization/","text":"Name: cognitivefactory.interactive_clustering.utils.vectorization Description: Utilities methods to apply NLP vectorization. Author: Erwan Schild Created: 17/03/2021 Licence: CeCILL ( https://cecill.info/licences.fr.html ) vectorize ( dict_of_texts , vectorizer_type = 'tfidf' , spacy_language_model = 'fr_core_news_sm' ) \u00b6 A method used to vectorize texts. Severals vectorizer are available : TFIDF, spaCy language model. References Scikit-learn : Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830. Scikit-learn 'TfidfVectorizer' : https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html spaCy : Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. spaCy language models: https://spacy.io/usage/models Parameters: Name Type Description Default dict_of_texts Dict[str,str] A dictionary that contains the texts to vectorize. required vectorizer_type str The vectorizer type to use. The type can be \"tfidf\" or \"spacy\" . Defaults to \"tfidf\" . 'tfidf' spacy_language_model str The spaCy language model to use if vectorizer is spacy. Defaults to \"fr_core_news_sm\" . 'fr_core_news_sm' Exceptions: Type Description ValueError Raises error if vectorizer_type is not implemented or if the spacy_language_model is not installed. Returns: Type Description Dict[str, Union[ndarray,csr_matrix]] A dictionary that contains the computed vectors. Examples: # Import. from cognitivefactory.interactive_clustering.utils.vectorization import vectorize # Define data. dict_of_texts = { \"0\" : \"comment signaler une perte de carte de paiement\" , \"1\" : \"quelle est la procedure pour chercher une carte de credit avalee\" , \"2\" : \"ma carte visa a un plafond de paiment trop bas puis je l augmenter\" , } # Apply vectorization. dict_of_vectors = vectorize ( dict_of_texts = dict_of_texts vectorizer_type = \"spacy\" , spacy_language_model = \"fr_core_news_sm\" , ) # Print results. print ( \"Computed results\" , \":\" , dict_of_vectors ) Source code in interactive_clustering\\utils\\vectorization.py def vectorize ( dict_of_texts : Dict [ str , str ], vectorizer_type : str = \"tfidf\" , spacy_language_model : str = \"fr_core_news_sm\" , ) -> Dict [ str , Union [ ndarray , csr_matrix ]]: \"\"\" A method used to vectorize texts. Severals vectorizer are available : TFIDF, spaCy language model. References: - _Scikit-learn_: `Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830.` - _Scikit-learn_ _'TfidfVectorizer'_: `https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html` - _spaCy_: `Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.` - _spaCy_ language models: `https://spacy.io/usage/models` Args: dict_of_texts (Dict[str,str]): A dictionary that contains the texts to vectorize. vectorizer_type (str, optional): The vectorizer type to use. The type can be `\"tfidf\"` or `\"spacy\"`. Defaults to `\"tfidf\"`. spacy_language_model (str, optional): The spaCy language model to use if vectorizer is spacy. Defaults to `\"fr_core_news_sm\"`. Raises: ValueError: Raises error if `vectorizer_type` is not implemented or if the `spacy_language_model` is not installed. Returns: Dict[str, Union[ndarray,csr_matrix]]: A dictionary that contains the computed vectors. Examples: ```python # Import. from cognitivefactory.interactive_clustering.utils.vectorization import vectorize # Define data. dict_of_texts={ \"0\": \"comment signaler une perte de carte de paiement\", \"1\": \"quelle est la procedure pour chercher une carte de credit avalee\", \"2\": \"ma carte visa a un plafond de paiment trop bas puis je l augmenter\", } # Apply vectorization. dict_of_vectors = vectorize( dict_of_texts=dict_of_texts vectorizer_type=\"spacy\", spacy_language_model=\"fr_core_news_sm\", ) # Print results. print(\"Computed results\", \":\", dict_of_vectors) ``` \"\"\" # Initialize dictionary of vectors. dict_of_vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = {} ### ### Case of TFIDF vectorization. ### if vectorizer_type == \"tfidf\" : # Initialize vectorizer. vectorizer = TfidfVectorizer ( analyzer = \"word\" , ngram_range = ( 1 , 3 ), min_df = 2 , ) # Apply vectorization. tfidf_vectorization : csr_matrix = vectorizer . fit_transform ( [ str ( dict_of_texts [ data_ID ]) for data_ID in dict_of_texts . keys ()] ) # Format dictionary of vectors to return. dict_of_vectors = { data_ID : tfidf_vectorization [ i ] . astype ( np . float64 ) for i , data_ID in enumerate ( dict_of_texts . keys ()) } # Return the dictionary of vectors. return dict_of_vectors ### ### Case of SPACY vectorization. ### if vectorizer_type == \"spacy\" : # Load vectorizer (spaCy language model). try : spacy_nlp = spacy . load ( name = spacy_language_model , disable = [ \"tagger\" , # Not needed \"parser\" , # Not needed \"ner\" , # Not needed ], ) except OSError as err : # `spacy_language_model` is not installed. raise ValueError ( \"The `spacy_language_model` '\" + str ( spacy_language_model ) + \"' is not installed.\" ) from err # Apply vectorization. dict_of_vectors = { data_ID : spacy_nlp ( str ( text )) . vector . astype ( np . float64 ) for data_ID , text in dict_of_texts . items () } # Return the dictionary of vectors. return dict_of_vectors ### ### Other case : Raise a `ValueError`. ### raise ValueError ( \"The `vectorizer_type` '\" + str ( vectorizer_type ) + \"' is not implemented.\" )","title":"utils.vectorization"},{"location":"reference/utils/vectorization/#cognitivefactory.interactive_clustering.utils.vectorization.vectorize","text":"A method used to vectorize texts. Severals vectorizer are available : TFIDF, spaCy language model. References Scikit-learn : Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830. Scikit-learn 'TfidfVectorizer' : https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html spaCy : Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. spaCy language models: https://spacy.io/usage/models Parameters: Name Type Description Default dict_of_texts Dict[str,str] A dictionary that contains the texts to vectorize. required vectorizer_type str The vectorizer type to use. The type can be \"tfidf\" or \"spacy\" . Defaults to \"tfidf\" . 'tfidf' spacy_language_model str The spaCy language model to use if vectorizer is spacy. Defaults to \"fr_core_news_sm\" . 'fr_core_news_sm' Exceptions: Type Description ValueError Raises error if vectorizer_type is not implemented or if the spacy_language_model is not installed. Returns: Type Description Dict[str, Union[ndarray,csr_matrix]] A dictionary that contains the computed vectors. Examples: # Import. from cognitivefactory.interactive_clustering.utils.vectorization import vectorize # Define data. dict_of_texts = { \"0\" : \"comment signaler une perte de carte de paiement\" , \"1\" : \"quelle est la procedure pour chercher une carte de credit avalee\" , \"2\" : \"ma carte visa a un plafond de paiment trop bas puis je l augmenter\" , } # Apply vectorization. dict_of_vectors = vectorize ( dict_of_texts = dict_of_texts vectorizer_type = \"spacy\" , spacy_language_model = \"fr_core_news_sm\" , ) # Print results. print ( \"Computed results\" , \":\" , dict_of_vectors ) Source code in interactive_clustering\\utils\\vectorization.py def vectorize ( dict_of_texts : Dict [ str , str ], vectorizer_type : str = \"tfidf\" , spacy_language_model : str = \"fr_core_news_sm\" , ) -> Dict [ str , Union [ ndarray , csr_matrix ]]: \"\"\" A method used to vectorize texts. Severals vectorizer are available : TFIDF, spaCy language model. References: - _Scikit-learn_: `Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, et E. Duchesnay (2011). Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research 12, 2825\u20132830.` - _Scikit-learn_ _'TfidfVectorizer'_: `https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html` - _spaCy_: `Honnibal, M. et I. Montani (2017). spaCy 2 : Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.` - _spaCy_ language models: `https://spacy.io/usage/models` Args: dict_of_texts (Dict[str,str]): A dictionary that contains the texts to vectorize. vectorizer_type (str, optional): The vectorizer type to use. The type can be `\"tfidf\"` or `\"spacy\"`. Defaults to `\"tfidf\"`. spacy_language_model (str, optional): The spaCy language model to use if vectorizer is spacy. Defaults to `\"fr_core_news_sm\"`. Raises: ValueError: Raises error if `vectorizer_type` is not implemented or if the `spacy_language_model` is not installed. Returns: Dict[str, Union[ndarray,csr_matrix]]: A dictionary that contains the computed vectors. Examples: ```python # Import. from cognitivefactory.interactive_clustering.utils.vectorization import vectorize # Define data. dict_of_texts={ \"0\": \"comment signaler une perte de carte de paiement\", \"1\": \"quelle est la procedure pour chercher une carte de credit avalee\", \"2\": \"ma carte visa a un plafond de paiment trop bas puis je l augmenter\", } # Apply vectorization. dict_of_vectors = vectorize( dict_of_texts=dict_of_texts vectorizer_type=\"spacy\", spacy_language_model=\"fr_core_news_sm\", ) # Print results. print(\"Computed results\", \":\", dict_of_vectors) ``` \"\"\" # Initialize dictionary of vectors. dict_of_vectors : Dict [ str , Union [ ndarray , csr_matrix ]] = {} ### ### Case of TFIDF vectorization. ### if vectorizer_type == \"tfidf\" : # Initialize vectorizer. vectorizer = TfidfVectorizer ( analyzer = \"word\" , ngram_range = ( 1 , 3 ), min_df = 2 , ) # Apply vectorization. tfidf_vectorization : csr_matrix = vectorizer . fit_transform ( [ str ( dict_of_texts [ data_ID ]) for data_ID in dict_of_texts . keys ()] ) # Format dictionary of vectors to return. dict_of_vectors = { data_ID : tfidf_vectorization [ i ] . astype ( np . float64 ) for i , data_ID in enumerate ( dict_of_texts . keys ()) } # Return the dictionary of vectors. return dict_of_vectors ### ### Case of SPACY vectorization. ### if vectorizer_type == \"spacy\" : # Load vectorizer (spaCy language model). try : spacy_nlp = spacy . load ( name = spacy_language_model , disable = [ \"tagger\" , # Not needed \"parser\" , # Not needed \"ner\" , # Not needed ], ) except OSError as err : # `spacy_language_model` is not installed. raise ValueError ( \"The `spacy_language_model` '\" + str ( spacy_language_model ) + \"' is not installed.\" ) from err # Apply vectorization. dict_of_vectors = { data_ID : spacy_nlp ( str ( text )) . vector . astype ( np . float64 ) for data_ID , text in dict_of_texts . items () } # Return the dictionary of vectors. return dict_of_vectors ### ### Other case : Raise a `ValueError`. ### raise ValueError ( \"The `vectorizer_type` '\" + str ( vectorizer_type ) + \"' is not implemented.\" )","title":"vectorize()"},{"location":"coverage/","text":".md-content { max-width: none !important; } article h1, article > a { display: none; } var coviframe = document.getElementById(\"coviframe\"); function resizeIframe() { coviframe.style.height = coviframe.contentWindow.document.documentElement.offsetHeight + 'px'; } coviframe.contentWindow.document.body.onclick = function() { coviframe.contentWindow.location.reload(); }","title":"Coverage report"}]}